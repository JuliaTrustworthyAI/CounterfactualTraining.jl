# Introduction

Today's prominence of artificial intelligence (AI) has largely been driven by advances in **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern AIs are tasked with learning these representations from scratch, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such AIs with ever greater degrees of freedom to achieve that task, which has often led them to outperform traditionally more parsimonious models. Unfortunately, in doing so they also learn increasingly complex and highly sensitive representations that we can no longer easily interpret.

This trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @goodfellow2014explaining showed that artificial neural networks (ANN) are sensitive to adversarial examples (AE): counterfactuals of model inputs that yield vastly different model predictions despite being semantically indifferent from their factual counterparts. Despite partially effective mitigation strategies such as **adversarial training**, truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. 

Part of the problem is that high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case]. Based purely on predictive performance, these solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two related challenges: firstly, it makes these models inherently opaque, since humans cannot simply interpret what type of explanation the complex learned representations correspond to; secondly, even if we could resolve the first challenge, it is not obvious how to mitigate models from learning representations that correspond to meaningless and undesirable explanations. 

The first challenge has attracted an abundance of research on **explainable AI** (XAI) which aims to develop tools to derive explanations from complex model representations. This can mitigate a scenario in which we deploy opaque models and blindly rely on their predictions. On countless occasions, this scenario has already occurred in practice and caused real harm to people who were affected adversely and often unfairly by automated decision-making systems involving opaque models [@oneil2016weapons]. Effective XAI tools can aide us in monitoring models and providing affected individuals with recourse [@wachter2017counterfactual].

To our surprise, the second challenge has not yet attracted any consolidated research effort. Specifically, there has been no concerted effort towards improving model **explainability**, which we define here as the degree to which learned representations correspond to explanations that are deemed desirable by humans. Instead, the choice has typically been to improve the capacity of XAI tools to identify the subset explanations that are both desirable and valid for any given model, independent of whether the learned representations are also compatible with undesirable explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that explainability can arise as byproduct of regularization techniques aimed at other objectives such as robustness, generalization and generative capacity [@schut2021generating, @augustin2020adversarial, @altmeyer2024faithful]. 

Building on these findings, we introduce a **counterfactual training**: a novel approach geared explicitly towards aligning model representations with desirable explanations. 

From this perspective, adversarial training induces models to "unlearn" representations that are susceptible to the semantically most meaningless explanations---adversarial examples. 

