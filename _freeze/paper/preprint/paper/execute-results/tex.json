{
  "hash": "a8223ddf51e2a9e9ebe8fff23a588627",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  arxiv-pdf:\n    keep-tex: true  \n    linenumbers: true\n    doublespacing: false\n    runninghead: \"Counterfactual Training (A Preprint)\"\n    authorcols: true\n---\n\n\n# Abstract\n\nCounterfactual Explanations have emerged as a popular tool to explain predictions made by opaque machine learning models: they explain how factual inputs need to change in order for some fitted model to produce some desired output. Much existing research has focused on identifying explanations that are not only valid but also deemed plausible and desirable with respect to the underlying data and stakeholder requirements. Recent work has shown that under this premise, the task of learning plausible explanations is effectively reassigned from the model itself to the (post-hoc) counterfactual explainer. Building on that work, we propose a novel model objective that leverages counterfactuals during the training phase (ad-hoc) in order to minimize the divergence between learned representations and plausible explanations. Through extensive experiments, we demonstrate that our proposed methodology facilitates training models that inherently deliver plausible explanations while maintaining high predictive performance. \n\n\n\n# Introduction\n\nToday's prominence of artificial intelligence (AI) has largely been driven by advances in **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern machine learning (ML) models are tasked with learning these representations from scratch, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such models with ever greater degrees of freedom to achieve that task, which has often led them to outperform traditionally more parsimonious models. Unfortunately, in doing so they also learn increasingly complex and highly sensitive representations that we can no longer easily interpret.\n\nThis trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are sensitive to adversarial examples: counterfactuals of model inputs that yield vastly different model predictions despite being \"imperceptible\" in that they are semantically indifferent from their factual counterparts. Despite partially effective mitigation strategies such as **adversarial training** [@goodfellow2014explaining], truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. \n\nPart of the problem is that high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case]^[For clarity: we follow standard ML convention in using \"degrees of freedom\" to refer to the number of parameters estimated from data.]. Based purely on predictive performance, these solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two related challenges: firstly, it makes these models inherently opaque, since humans cannot simply interpret what type of explanation the complex learned representations correspond to; secondly, even if we could resolve the first challenge, it is not obvious how to mitigate models from learning representations that correspond to meaningless and implausible explanations. \n\nThe first challenge has attracted an abundance of research on **explainable AI** (XAI) which aims to develop tools to derive explanations from complex model representations. This can mitigate a scenario in which we deploy opaque models and blindly rely on their predictions. On countless occasions, this scenario has already occurred in practice and caused real harm to people who were affected adversely and often unfairly by automated decision-making systems (ADMS) involving opaque models [@oneil2016weapons]. Effective XAI tools can aide us in monitoring models and providing recourse to individuals to turn adverse outcomes (e.g. \"loan application rejected\") into positive ones (\"application accepted\"). @wachter2017counterfactual propose **counterfactual explanations** as an effective approach to achieve this: they explain how factual inputs need to change in order for some fitted model to produce some desired output, typically involving minimal perturbations.\n\nTo our surprise, the second challenge has not yet attracted any consolidated research effort. Specifically, there has been no concerted effort towards improving model **explainability**, which we define here as the degree to which learned representations correspond to explanations that are interpretable and deemed **plausible** by humans (see @def-explainability). Instead, the choice has typically been to improve the capacity of XAI tools to identify the subset explanations that are both plausible and valid for any given model, independent of whether the learned representations are also compatible with implausible explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that explainability can arise as byproduct of regularization techniques aimed at other objectives such as robustness, generalization and generative capacity [@schut2021generating, @augustin2020adversarial, @altmeyer2024faithful]. \n\nBuilding on these findings, we introduce **counterfactual training**: a novel regularization technique geared explicitly towards aligning model representations with plausible explanations. Our contributions are as follows:\n\n- We discuss existing related work on improving models and consolidate it through the lens of counterfactual explanations (@sec-lit).\n- We present our proposed methodological framework that leverages faithful counterfactual explanations during the training phase of models to achieve the explainability objective (@sec-method).\n- Through extensive experiments we demonstrate the counterfactual training improve model explainability while maintaining high predictive performance. We run ablation studies and grid searches to understand how the underlying model components and hyperparameters affect outcomes. (@sec-experiments). \n\nDespite limitations of our approach discussed in @sec-discussion, we conclude that counterfactual training provides a practical framework for researchers and practitioners interested in making opaque models more trustworthy [@sec-conclusion]. We also believe that this work serves as an opportunity for XAI researchers to reevaluate the premise of improving XAI tools without improving models. \n\n\n\n\n# Related Literature {#sec-lit}\n\nTo the best of our knowledge, our proposed framework for counterfactual training represents the first attempt to use counterfactual explanations during training to improve model explainability. In high-level terms, we define model explainability as the extent to which valid explanations derived for an opaque model are also deemed plausible with respect to the underlying data and stakeholder requirements. To make this more concrete, we follow @augustin2020adversarial in tieing the concept of explainability to the quality of counterfactual explanations that we can generate for a given model. The authors show that counterfactual explanations---understood here as minimal input perturbations that yield some desired model prediction---are generally more meaningful if the underlying model is more robust to adversarial examples. We can make intuitive sense of this finding when looking at adversarial training (AT) through the lens of representation learning with high degrees of freedom: by inducing models to \"unlearn\" representations that are susceptible to worst-case counterfactuals (i.e. adversarial examples), AT effectively removes some implausible explanations from the solution space.\n\n## Adversarial Examples are Counterfactual Explanations\n\nThis interpretation of the link between explainability through counterfactuals on one side, and robustness to adversarial examples on the other, is backed by empirical evidence. @sauer2021counterfactual demonstrate that using counterfactual images during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, demonstrate the augmenting the training data of image classifiers can improve generalization. @teney2020learning propose an approach using counterfactuals in training that does not rely on data augmentation: they argue that counterfactual pairs typically already exist in training datasets. Specifically, their approach relies on, firstly, identifying similar input samples with different annotations and, secondly, ensuring that the gradient of the classifier aligns with the vector between pairs of counterfactual inputs using the cosine distance as a loss function. \n\nIn the natural language processing (NLP) domain, counterfactuals have similarly been used to improve models through data augmentation: @wu2021polyjuice, propose *Polyjuice*, a general-purpose counterfactual generator for language models. They demonstrate empirically that augmenting training data through *Polyjuice* counterfactuals improves robustness in a number of NLP tasks. @balashankar2023improving also use *Polyjuice* to augment NLP datasets through diverse counterfactuals and show that classifier robustness improves up to 20%. Finally, @luu2023counterfactual introduce Counterfactual Adversarial Training (CAT), which also aims at improving generalization and robustness of language models. Specifically, they propose to proceed as follows: firstly, they identify training samples that are subject to high predictive uncertainty; secondly, they generate counterfactual explanations for those samples; and, finally, they fine-tune the given language model on the augmented dataset that includes the generated counterfactuals. \n\nThere have also been several attempts at formalizing the relationship between counterfactual explanations (CE) and adversarial examples (AE). Pointing to clear similarities in how CE and AE are generated, @freiesleben2022intriguing makes the case for jointly studying the opaqueness and robustness problem in representation learning. Formally, AE can be seen as the subset of CE, for which misclassification is achieved [@freiesleben2022intriguing]. Similarly, @pawelczyk2022exploring show that CE and AE are equivalent under certain conditions and derive theoretical upper bounds on the distances between them. \n\nTwo recent works are closely related to ours in that they use counterfactuals during training with the explicit goal of affecting certain properties of post-hoc counterfactual explanations. Firstly, @ross2021learning propose a way to train models that are guaranteed to provide recourse for individuals to move from an adverse outcome to some positive target class with high probability. The approach proposed by @ross2021learning builds on adversarial training, where in this context susceptibility to targeted adversarial examples for the positive class is explicitly induced. The proposed method allows for imposing a set of actionability constraints ex-ante: for example, users can specify that certain features (e.g. *age*, *gender*, ...) are immutable. Secondly, @guo2023counternet are the first to propose an end-to-end training pipeline that includes counterfactual explanations as part of the training procedure. In particular, they propose a specific network architecture that includes a predictor and CE generator network, where the parameters of the CE generator network are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor network. In contrast to @guo2023counternet, we impose no restrictions on the neural network architecture at all. \n\n## Beyond Robustness \n\nImproving the adversarial robustness of models is not the only path towards aligning representations with plausible explanations. In a work closely related to this one, @altmeyer2024faithful show that explainability can be improved through model averaging and refined model objectives. The authors propose a way to generate counterfactuals that are maximally **faithful** to the model in that they are consistent with what the model has learned about the underlying data. Formally, they rely on tools from energy-based modelling to minimize the divergence between the distribution of counterfactuals and the conditional posterior over inputs learned by the model. Their proposed counterfactual explainer, *ECCCo*, yields plausible explanations if and only if the underlying model has learned representations that align with them. They find that both deep ensembles [@lakshminarayanan2016simple] and joint energy-based models (JEMs) [@grathwohl2020your] tend to do well in this regard. \n\nOnce again it helps to look at these findings through the lens of representation learning with high degrees of freedom. Deep ensembles are approximate Bayesian model averages, which are most called for when models are underspecified by the available data [@wilson2020case]. Averaging across solutions mitigates the aforementioned risk of relying on a single locally optimal representations that corresponds to semantically meaningless explanations for the data. Previous work by @schut2021generating similarly found that generating plausible (\"interpretable\") counterfactual explanations is almost trivial for deep ensembles that have also undergone adversarial training. The case for JEMs is even clearer: they involve a hybrid objective that induces both high predictive performance and generative capacity [@grathwohl2020your]. This is closely related to the idea of aligning models with plausible explanations and has inspired our proposed counterfactual training objective, as we explain in @sec-method.\n\n\n\n# Counterfactual Training {#sec-method}\n\nCounterfactual training combines ideas from adversarial training, energy-based modelling and counterfactuals explanations with the explicit objective of aligning representations with plausible explanations that comply with user requirements. In the context of CE, plausibility has broadly been defined as the degree to which counterfactuals comply with the underlying data generating process [@poyiadzi2020face;@guidotti2022counterfactual;@altmeyer2024faithful]. Plausibility is a necessary but insufficient condition for using CE to provide algorithmic recourse (AR) to individuals affected by opaque models in practice. This is because for recourse recommendations to be **actionable**, they need to not only result in plausible counterfactuals but also be attainable. A plausible CE for a rejected 20-year-old loan applicant, for example, might reveal that their application would have been accepted, if only they were 20 years older. Ignoring all other features, this complies with the definition of plausibility if 40-year-old individuals are in fact more credit-worthy on average than young adults. But of course this CE does not qualify for providing actionable recourse to the applicant since *age* is not a mutable feature. For our intents and purposes, counterfactual training aims at improving model explainability by aligning models with counterfactuals that meet both desiderata, plausibility and actionability. Formally, we define explainability as follows:\n\n::: {#def-explainability}\n\n## Model Explainability\n\nLet $\\mathbf{M}_\\theta: \\mathcal{X} \\mapsto \\mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\\mathcal{X}$ to representations $\\phi(\\mathbf{x};\\theta)$ and finally to the $K$-dimensional output space $\\mathcal{Y}$. Assume that for any given input-output pair $\\{\\mathbf{x},\\mathbf{y}\\}_i$ there exists a counterfactual $\\mathbf{x}^{\\prime} = \\mathbf{x} + \\Delta: \\mathbf{M}_\\theta(\\mathbf{x}^{\\prime}) = \\mathbf{y}^{+} \\neq \\mathbf{y} = \\mathbf{M}_\\theta(\\mathbf{x})$ where $\\arg\\max_y{\\mathbf{y}^{+}}=y^+$ and $y^+$ denotes the index of the target class. \n\nWe say that $\\mathbf{M}_\\theta$ is **explainable** to the extent that faithfully generated counterfactuals are plausible (i.e. consistent with the data) and actionable. Formally, we define these properties as follows:\n\n1. (Plausibility) $\\int^{A} p(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$ where $A$ is some small region around $\\mathbf{x}^{\\prime}$.\n2. (Actionability) Permutations $\\Delta$ are subject to actionability constraints.\n\nWe consider counterfactuals as faithful to the extent that they are consistent with what the model has learned about the input data. Let $p_\\theta(\\mathbf{x}|\\mathbf{y}^{+})$ denote the conditional posterior over inputs, then formally:\n\n3. (Faithfulness) $\\int^{A} p_\\theta(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$ where $A$ is defined as above.\n\n:::\n\nThe definitions of faithfulness and plausibility in @def-explainability are the same as in @altmeyer2024faithful, with adapted notation. Actionability constraints in @def-explainability vary and depend on the context in which $\\mathbf{M}_\\theta$ is deployed. In this work, we focus on domain and mutability constraints for individual features $x_d$ for $d=1,...,D$. We limit ourselves to classification tasks for reasons discussed in @sec-discussion.\n\n## Our Proposed Objective\n\nLet $\\mathbf{x}_t^\\prime$ for $t=0,...,T$ denote a counterfactual explanation generated through gradient descent over $T$ iterations as initially proposed by @wachter2017counterfactual. For our purposes, we let $T$ vary and consider the counterfactual search as converged as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$^[For detailed background information on gradient-based counterfactual search and convergence see @sec-app-ce.].\n\nTo train models with high explainability as defined in @def-explainability, we propose to leverage counterfactuals in the following objective,\n\n$$\n\\min_\\theta \\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}),\\mathbf{y}) + \\lambda_{\\text{div}} \\text{div}(\\mathbf{x},\\mathbf{x}_T^\\prime,y;\\theta) + \\lambda_{\\text{adv}} \\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y})\n$$ {#eq-obj}\n\nwhere $\\text{yloss}(\\cdot)$ is any conventional classification loss that induces discriminative performance (e.g. cross-entropy). The two additional components in @eq-obj are explained in more detail below. For now, they can be sufficiently described as inducing explainability directly and indirectly by penalizing: 1) the contrastive divergence, $\\text{div}(\\cdot)$, between counterfactuals $\\mathbf{x}_T^\\prime$ and observed samples $x$ and, 2) the adversarial loss, $\\text{advloss}(.)$, with respect to nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$. The tradeoff between the different components can be governed by adjusting the strengths of the penalties $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$.\n\n### Directly Inducing Explainability through Contrastive Divergence\n\n@grathwohl2020your observe that any classifier can be re-interpreted as a joint energy-based model (JEM) that learns to discriminate output classes conditional on inputs and generate inputs. They show that JEMs can be trained to perform well at both tasks by directly maximizing the joint log-likelihood factorized as $\\log p_\\theta(\\mathbf{x},\\mathbf{y})=\\log p_\\theta(\\mathbf{y}|\\mathbf{x}) + \\log p_\\theta(\\mathbf{x})$. The first factor can be optimized using conventional cross-entropy as in @eq-obj. To optimize $\\log p_\\theta(\\mathbf{x})$ @grathwohl2020your minimize the contrastive divergence between samples drawn from $p_\\theta(\\mathbf{x})$ and training observations, i.e. samples from $p(\\mathbf{x})$. \n\nA key empirical finding in @altmeyer2024faithful was that JEMs tend to do well with respect to the plausibility objective in @def-explainability. If we consider samples drawn from $p_\\theta(\\mathbf{x})$ as counterfactuals, this is an expected finding, because the JEM objective effectively minimizes the divergence between the conditional posterior and $p(\\mathbf{x}|\\mathbf{y}^{+})$. To generate samples, @grathwohl2020your rely on Stochastic Gradient Langevin Dynamics (SGLD) using an uninformative prior for initialization. This is where we depart from their methodology: instead of generating samples through SGLD, we propose using counterfactual explainers to generate counterfactuals for observed training samples. Specifically, we have\n\n$$\n\\text{div}(\\mathbf{x},\\mathbf{x}_T^\\prime,y;\\theta) = \\mathcal{E}_\\theta(\\mathbf{x},y) - \\mathcal{E}_\\theta(\\mathbf{x}_T^\\prime,y)\n$$ {#eq-div}\n\nwhere $\\mathcal{E}_\\theta(\\cdot)$ denotes the energy function. In particular, we set $\\mathcal{E}_\\theta(\\mathbf{x},\\mathbf{y})=-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]$ where $y^+$ denotes the index of the target class. We generate samples $\\mathbf{x}_T^\\prime$ by first randomly sampling the target class $y^+ \\sim p(y)$ and then generating a counterfactual explanation for that target over $T$ iterations using a gradient-based counterfactual generator. This is similar to how conditional sampling is used to draw from $p_\\theta(\\mathbf{x})$ in @grathwohl2020your. \n\nIntuitively, the gradient of @eq-div decreases the energy of observed training samples (positive samples) while at same time increasing the energy of counterfactuals (negative samples) [@du2019implicit]. As the generated counterfactuals get more plausible (@def-explainability) over the cause of training, these two opposing effects gradually balance each out [@lippe2024uvadlc]. \n\nThe departure from SGLD allows us to tap into the vast repertoire of explainers that have been proposed in the literature to meet different desiderata. Typically, these methods facilitate the imposition of domain and mutability constraints, for example. In principle, any existing approach for generating counterfactual explanations is viable, so long as it does not violate the faithfulness condition. Like JEMs [@murphy2022probabilistic], counterfactual training can be considered as a form of contrastive representation learning. \n\n### Indirectly Inducing Explainability through Adversarial Robustness\n\nBased on our analysis in @sec-lit, counterfactuals $\\mathbf{x}^\\prime$ can be repurposed as additional training samples [@luu2023counterfactual;@balashankar2023improving] or adversarial examples [@freiesleben2022intriguing;@pawelczyk2022exploring]. This leaves some flexibility with respect to the exact choice for $\\text{advloss}(\\cdot)$ in @eq-obj. An intuitive functional form to use, though likely not the only reasonable choice, is inspired by adversarial training:\n\n$$\n\\begin{aligned}\n\\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y};\\varepsilon)&=\\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t_\\varepsilon}^\\prime),\\mathbf{y}) \\\\\nt_\\varepsilon &= \\max_t \\{t: ||\\Delta_t||_\\infty < \\varepsilon\\}\n\\end{aligned}\n$$ {#eq-adv}\n\nUnder this choice, we consider nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$ as adversarial examples as long as the magnitude of the perturbation to any individual feature is at most $\\varepsilon$. This is closely aligned with @szegedy2013intriguing, who define an adversarial attack as an \"imperceptible non-random perturbation\". Thus, we choose to work with a different distinction between CE and AE than @freiesleben2022intriguing, who considers misclassification as the key distinguishing feature of AE. One of the key observations in this work is that we can leverage counterfactual explanations during training and get adversarial examples, essentially for free. \n\n## Encoding Actionability Constraints {#sec-constraints}\n\nMany existing counterfactual explainers support domain and mutability constraints out-of-the-box. In fact, both types of constraints can be implemented for any counterfactual explainer that relies on gradient descent in the feature space for optimization [@altmeyer2023explaining]. In this context, domain constraints can be imposed by simply projecting counterfactuals back to the specified domain, if the previous gradient step resulted in updated feature values that were out-of-domain. Mutability constraints can similarly be enforced by setting partial derivatives to zero to ensure that features are only mutated in the allowed direction, if at all. \n\nSince actionability constraints are binding at test time, we should also impose them when generating $\\mathbf{x}^\\prime$ during each training iteration to align model representations with user requirements. Through their effect on $\\mathbf{x}^\\prime$, both types of constraints influence model outcomes through @eq-div. Here it is crucial that we avoid penalizing implausibility that arises due to mutability constraints. For any mutability-constrained feature $d$ this can be achieved by enforcing $\\mathbf{x}[d] - \\mathbf{x}^\\prime[d]:=0$ whenever perturbing $\\mathbf{x}^\\prime[d]$ in the direction of $\\mathbf{x}[d]$ would violate mutability constraints. Specifically, we set $\\mathbf{x}[d] := \\mathbf{x}^\\prime[d]$ if \n\n1. Feature $d$ is strictly immutable in practice.\n2. We have $\\mathbf{x}[d]>\\mathbf{x}^\\prime[d]$ but feature $d$ can only be decreased in practice.\n3. We have $\\mathbf{x}[d]<\\mathbf{x}^\\prime[d]$ but feature $d$ can only be increased in practice.\n\nFrom a Bayesian perspective, setting $\\mathbf{x}[d] := \\mathbf{x}^\\prime[d]$ can be understood as assuming a point mass prior for $p(\\mathbf{x})$ with respect to feature $d$. Intuitively, we think of this simply in terms ignoring implausibility costs with respect to immutable features, which effectively forces the model to instead seek plausibility with respect to the remaining features. This in turn results in lower overall sensitivity to immutable features, which we demonstrate empirically for different classifiers in @sec-experiments. Under certain conditions, this results holds theoretically[For the proof, see the supplementary appendix.]:\n\n::: {#prp-mtblty}\n\n## Protecting Immutable Features\n\nLet $f_\\theta(\\mathbf{x})=\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}))=\\mathcal{S}(\\Theta\\mathbf{x})$ denote a linear classifier with softmax activation $\\mathcal{S}$ (i.e. *multinomial logistic regression*) where $y\\in\\{1,...,K\\}=\\mathcal{K}$ and $\\mathbf{x} \\in \\mathbb{R}^D$. If we assume multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty (@eq-div) will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is mutable and discriminative.\n\n:::\n\nIt is worth highlighting that @prp-mtblty assumes independence of features. This raises a valid concern about the effect of protecting immutable features in the presence of proxy features that remain unprotected. We discuss this limitation in @sec-discussion.\n\n## Illustration\n\nTo better convey the intuition underlying our proposed method, we illustrate different model outcomes in @exm-grad.\n\n::: {#exm-grad}\n\n## Prediction of Consumer Credit Default\n\nSuppose we are interested in predicting the likelihood that loan applicants default on their credit. We have access to historical data on previous loan takers comprised of a binary outcome variable ($y\\in\\{1=\\text{default},2=\\text{no default}\\}$) two input features: 1) the subjects' *age*, which we define as immutable, and 2) the subjects' existing level of *debt*, which we define as mutable. \n\nWe have simulated this scenario using synthetic data with independent features and Gaussian class-conditional densities in @fig-poc. The four panels in @fig-poc show the outcomes for different training procedures using the same model architecture each time (a linear classifier). In each case, we show the linear decision boundary (green) and the training data colored according to their ground-truth label: orange points belong to the target class, $y^+=2$, blue points belong to the non-target class, $y^-=1$. Stars indicate counterfactuals in the target class generated at test time using generic gradient descent until convergence.\n\nIn panel (a), we have trained our model conventionally, and we do not impose mutability constraints at test time. The generated counterfactuals are all valid, but not plausible: they are clearly distinguishable from the ground-truth data. In panel (b), we have trained our model with counterfactual training, once again not imposing mutability constraints at test time. We observe that the counterfactuals are clearly plausible, therefore meeting the first objective of @def-explainability.\n\nIn panel (c), we have used conventional training again, this time imposing the mutability constraint on *age* at test time. Counterfactuals are valid but involve some substantial reductions in *debt* for some individuals, in particular very young applicants. By comparison, counterfactual paths are shorter on average in panel (d), where we have used counterfactual training and protected immutable features as described in @sec-constraints. In particular, we observe that due to the classifier's lower sensitivity to *age*, recourse recommendations with respect to *debt* are much more homogenous, in that they do not disproportionately punish younger individuals. The counterfactuals are also plausible with respect to the mutable feature. Thus, we consider the model in panel (d) as the most explainable according to @def-explainability.\n\n:::\n\n![Visual illustration of how counterfactual training improves explainability. See @exm-grad for details.](/paper/figures/poc.svg){#fig-poc}\n\n\n\n# Experiments {#sec-experiments}\n\nIn this section, we present experiments that we have conducted in order to answer the following research questions:\n\n::: {#cor-plaus}\n\n## Plausibility\n\nDoes our proposed counterfactual training objective (@eq-obj) induce models to learn plausible explanations?\n\n:::\n\n::: {#cor-action}\n\n## Actionability\n\nDoes our proposed counterfactual training objective (@eq-obj) yield more favorable algorithmic recourse outcomes in the presence of actionability constraints?\n\n:::\n\nBeyond this, we are also interested in understanding how robust our answers to @cor-plaus and @cor-action are:\n\n::: {#cor-hyper}\n\n## Hyperparameters \n\nWhat are the effects of different hyperparameter choices with respect to @eq-obj?\n\n:::\n\n\n## Experimental Setup\n\n## Experimental Results\n\n\n\n# Discussion {#sec-discussion}\n\n1. Limited to classification models.\n2. Proxy attributes of immutable features.\n3. Increased training time.\n4. Training instabilities\n5. Fairness and caveats (aware it's not a classical approach in this context, but there is a clear link).\n\n\n\n# Conclusion {#sec-conclusion}\n\n\n\n\n\n# References {-}\n\n::: {#refs}\n:::\n\n\n\n{{< pagebreak >}}\n\n\n\n\\FloatBarrier\n\n\\setcounter{section}{0}\n\\renewcommand{\\thesection}{\\Alph{section}}\n\n\\setcounter{table}{0}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\n\\setcounter{figure}{0}\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\n<!-- # Supplementary Material {.appendix} -->\n\n# Notation {.appendix}\n\n- $y^+$: The target class and also the index of the target class.\n- $y^-$: The non-target class and also the index of non-the target class.\n- $\\mathbf{y}^+$: The one-hot encoded output vector for the target class. \n- $\\theta$: Model parameters (unspecified).\n- $\\Theta$: Matrix of parameters. \n\n\n\n# Technical Details of Our Approach {.appendix} \n\n## Generating Counterfactuals through Gradient Descent {#sec-app-ce}\n\nIn this section, we provide some background on gradient-based counterfactual generators (@sec-app-ce-background) and discuss how we define convergence in this context (@sec-app-conv).\n\n### Background {#sec-app-ce-background}\n\nGradient-based counterfactual search was originally proposed by @wachter2017counterfactual. It generally solves the following unconstrained objective,\n\n$$\n\\begin{aligned}\n\\min_{\\mathbf{z}^\\prime \\in \\mathcal{Z}^L} \\left\\{  {\\text{yloss}(\\mathbf{M}_{\\theta}(g(\\mathbf{z}^\\prime)),\\mathbf{y}^+)}+ \\lambda {\\text{cost}(g(\\mathbf{z}^\\prime)) }  \\right\\} \n\\end{aligned} \n$$\n\nwhere $g: \\mathcal{Z} \\mapsto \\mathcal{X}$ is an invertible function that maps from the $L$-dimensional counterfactual state space to the feature space and $\\text{cost}(\\cdot)$ denotes one or more penalties that are used to induce certain properties of the counterfactual outcome. As above, $\\mathbf{y}^+$ denotes the target output and $\\mathbf{M}_{\\theta}(\\mathbf{x})$ returns the logit predictions of the underlying classifier for $\\mathbf{x}=g(\\mathbf{z})$.\n\nFor all generators used in this work we use standard logit crossentropy loss for $\\text{yloss}(\\cdot)$. All generators also penalize the distance ($\\ell_1$-norm) of counterfactuals from their original factual state. For *Generic* and *ECCo*, we have $\\mathcal{Z}:=\\mathcal{X}$ and $g(\\mathbf{z})=g(\\mathbf{z})^{-1}=\\mathbf{z}$, that is counterfactual are searched directly in the feature space. Conversely, *REVISE* traverses the latent space of a variational autoencoder (VAE) fitted to the training data, where $g(\\cdot)$ corresponds to the decoder [@joshi2019realistic]. In addition to the distance penalty, *ECCo* uses an additional penalty component that regularizes the energy associated with the counterfactual, $\\mathbf{x}^\\prime$ [@altmeyer2024faithful]. \n\n### Convergence {#sec-app-conv}\n\nAn important consideration when generating counterfactual explanations using gradient-based methods is how to define convergence. Two common choices are to 1) perform gradient descent over a fixed number of iterations $T$, or 2) conclude the search as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$. We prefer the latter for our purposes, because it explicitly defines convergence in terms of the black-box model, $\\mathbf{M}(\\mathbf{x})$.\n\nDefining convergence in this way allows for a more intuitive interpretation of the resulting counterfactual outcomes than with fixed $T$. Specifically, it allows us to think of counterfactuals as explaining 'high-confidence' predictions by the model for the target class $y^+$. Depending on the context and application, different choices of $\\tau$ can be considered as representing 'high-confidence' predictions.\n\n\n\n\n\n\n\n\n## Protecting Mutability Constraints with Linear Classifiers {#sec-app-constraints}\n\nIn @sec-constraints we explain that to avoid penalizing implausibility that arises due to mutability constraints, we impose a point mass prior on $p(\\mathbf{x})$ for the corresponding feature. We argue in @sec-constraints that this approach induces models to be less sensitive to immutable features and demonstrate this empirically in @sec-experiments. Below we derive the analytical results in @prp-mtblty.\n\n::: {.proof}\n\nLet $d_{\\text{mtbl}}$ and $d_{\\text{immtbl}}$ denote some mutable and immutable feature, respectively. Suppose that $\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}}$ and $\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}}$, where $\\mu_{k,d}$ denotes the conditional sample mean of feature $d$ in class $k$. In words, we assume that the immutable feature tends to take lower values for samples in the non-target class $y^-$ than in the target class $y^+$. We assume the opposite to hold for the mutable feature.\n\nAssuming multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, we have for the log likelihood ratio between any two classes $k,m \\in \\mathcal{K}$ [@hastie2009elements]:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})}=\\mathbf{x}^\\intercal \\Sigma^{-1}(\\mu_{k}-\\mu_{m})  + \\text{const}\n$$ {#eq-loglike}\n\nBy independence of $x_1,...,x_D$, the full log-likelihood ratio decomposes into:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\frac{\\mu_{k,d}-\\mu_{m,d}}{\\sigma_{d}^2} x_{d} + \\text{const}\n$$ {#eq-loglike-decomp}\n\nBy the properties of our classifier (*multinomial logistic regression*), we have:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\left( \\theta_{k,d} - \\theta_{m,d} \\right)x_d + \\text{const}\n$$ {#eq-multi}\n\nwhere $\\theta_{k,d}=\\Theta[k,d]$ denotes the coefficient on feature $d$ for class $k$. \n\nBased on @eq-loglike-decomp and @eq-multi we can identify that $(\\mu_{k,d}-\\mu_{m,d}) \\propto (\\theta_{k,d} - \\theta_{m,d})$ under the assumptions we made above. Hence, we have that $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$ and $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$\n\nLet $\\mathbf{x}^\\prime$ denote some randomly chosen individual from class $y^-$ and let $y^+ \\sim p(y)$ denote the randomly chosen target class. Then the partial derivative of the contrastive divergence penalty [@eq-div] with respect to coefficient $\\theta_{y^+,d}$ is equal to \n\n$$\n\\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left(\\text{div}(\\mathbf{x},\\mathbf{x^\\prime},\\mathbf{y};\\theta)\\right) = \\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left( \\left(-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]\\right) - \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^\\prime)[y^+]\\right) \\right) = x_{d}^\\prime - x_{d}\n$$ {#eq-grad}\n\nand equal to zero everywhere else.\n\nSince $(\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}})$ we are more likely to have $(x_{d_{\\text{immtbl}}}^\\prime - x_{d_{\\text{immtbl}}}) < 0$ than vice versa at initialization. Similarly, we are more likely to have $(x_{d_{\\text{mtbl}}}^\\prime - x_{d_{\\text{mtbl}}}) > 0$ since $(\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}})$.\n\nThis implies that if we do not protect feature $d_{\\text{immtbl}}$, the contrastive divergence penalty will decrease $\\theta_{y^-,d_{\\text{immtbl}}}$ thereby exacerbating the existing effect $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$. In words, not protecting the immutable feature would have the undesirable effect of making the classifier more sensitive to this feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for lower values of $d_{\\text{immtbl}}$. \n\nBy the same rationale, the contrastive divergence penalty can generally be expected to increase $\\theta_{y^-,d_{\\text{mtbl}}}$ exacerbating $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$. In words, this has the effect of making the classifier more sensitive to the mutable feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for higher values of $d_{\\text{mtbl}}$.\n\nThus, our proposed approach of protecting feature $d_{\\text{immtbl}}$ has the net affect of decreasing the classifier's sensitivity to the immutable feature relative to the mutable feature (i.e. no change in sensitivity for $d_{\\text{immtbl}}$ relative to increased sensitivity for $d_{\\text{mtbl}}$).\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Domain Constraints\n\nWe apply domain constraints on counterfactuals during training and evaluation. There are at least two good reasons for doing so. Firstly, within the context of explainability and algorithmic recourse, real-world attributes are often domain constrained: the *age* feature, for example, is lower bounded by zero and upper bounded by the maximum human lifespan. Secondly, domain constraints help mitigate training instabilities commonly associated with energy-based modelling [@grathwohl2020your;@altmeyer2024faithful].\n\nFor our image datasets, features are pixel values and hence the domain is constrained by the lower and upper bound of values that pixels can take depending on how they are scaled (in our case $[-1,1]$). For all other features $d$ in our synthetic and tabular datasets, we automatically infer domain constraints $[x_d^{\\text{LB}},x_d^{\\text{UB}}]$  as follows,\n\n$$\n\\begin{aligned}\nx_d^{\\text{LB}} &= \\arg\\min_{x_d} \\{\\mu_d - n_{\\sigma_d}\\sigma_d, \\arg \\min_{x_d} x_d\\} \\\\\nx_d^{\\text{UB}} &= \\arg\\max_{x_d} \\{\\mu_d + n_{\\sigma_d}\\sigma_d, \\arg \\max_{x_d} x_d\\} \n\\end{aligned}\n$$ {#eq-domain}\n\nwhere $\\mu_d$ and $\\sigma_d$ denote the sample mean and standard deviation of feature $d$. We set $n_{\\sigma_d}=3$ across the board but higher values and hence wider bounds may be appropriate depending on the application.\n\n\n\n## Training Details {#sec-app-training}\n\nIn this section, we describe the training procedure in detail. While the details laid out here are not crucial for understanding our proposed approach, they are of importance to anyone looking to implement counterfactual training. \n\n\n\n\n# Detailed Results {.appendix}\n\n## Qualitative Findings for Image Data\n\n::: {.callout-note}\n\n@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla). In fact, this is not even using *ECCo+* and still showing better results than the best results we achieved in our AAAI paper for JEM ensembles.\n\n:::\n\n![Counterfactual images for *MLP* with counterfactual training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}\n\n![Counterfactual images for *MLP* with conventional training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}\n\n## Grid Searches {#sec-app-grid}\n\n\n\n\n\n\n\n\n\n### Generator Parameters\n\nThe hyperparameter grid with varying generator parameters during training is shown in @exr-gen-params-final-run-train. The corresponding evaluation grid used for these experiments is shown in @exr-gen-params-final-run-eval.\n\n::: {#exr-gen-params-final-run-train}\n\n## Training Phase\n\n\n- Generator Parameters:\n    - Decision Threshold: `0.75, 0.9, 0.95`\n    - $\\lambda_{\\text{cost}}$: `0.001`\n    - $\\lambda_{\\text{energy}}$: `0.1, 0.5, 5.0, 10.0, 20.0`\n    - Learning Rate: `0.25`\n    - Maximum Iterations: `5, 25, 50`\n    - Optimizer: `sgd`\n- Generator: `ecco, generic, revise`\n- Model Parameters:\n    - Activation: `relu`\n    - No. Hidden: `32`\n    - No. Layers: `1`\n- Model: `mlp`\n- Training Parameters:\n    - Burnin: `0.0`\n    - Class Loss: `logitcrossentropy`\n    - Convergence: `threshold`\n    - $\\lambda_{\\text{adv}}$: `0.25`\n    - $\\lambda_{\\text{yloss}}$: `1.0`\n    - $\\lambda_{\\text{div}}$: `0.5`\n    - $\\lambda_{\\text{reg}}$: `0.01`\n    - Learning Rate: `0.001`\n    - No. Counterfactuals: `1000`\n    - No. Epochs: `100`\n    - No. Neighbours: `100`\n    - Objective: `full, vanilla`\n    - Optimizer: `adam`\n\n\n\n\n\n:::\n\n\n::: {#exr-gen-params-final-run-eval}\n\n## Evaluation Phase\n\n\n- Counterfactual Parameters:\n    - Convergence: `threshold`\n    - Decision Threshold: `0.95`\n    - Maximum Iterations: `50`\n    - No. Individuals: `100`\n    - No. Runs: `5`\n- Generator Parameters:\n    - Decision Threshold: `0.75`\n    - $\\lambda_{\\text{cost}}$: `0.0`\n    - $\\lambda_{\\text{energy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n    - Learning Rate: `0.25`\n    - Maximum Iterations: `30`\n    - Optimizer: `sgd`\n\n\n\n\n\n:::\n\n### High-Level Findings\n\n![](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png)\n\n![](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png)\n\n![](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/logs/objective---lambda_energy---generator_type---maxiter/percent_valid.png)\n\n![](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/logs/objective---lambda_energy---generator_type---maxiter/acc_val.png)\n\n### Detailed Findings\n\n\n\n\n\n\n#### Linearly Separable \n\n\n\n\n\n\n::: {#tbl-lin_sep-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{cccccc}\n  \\toprule\n  \\textbf{Decision Threshold} & \\textbf{Lambda Energy} & \\textbf{Maximum Iterations} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Decision Threshold} & \\textbf{Lambda Energy} & \\textbf{Maximum Iterations} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{6}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  0.75 & 0.1 & 5 & \\textit{ECCo} & -15.9 & 16.7 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{41.9}} & \\color{green}{\\textbf{8.13}} \\\\\n  0.75 & 0.1 & 5 & \\textit{REVISE} & -64.4 & 3.58 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{28.9}} & \\color{green}{\\textbf{2.28}} \\\\\n  0.75 & 0.1 & 25 & \\textit{Generic} & -11.3 & 11.5 \\\\\n  0.75 & 0.1 & 25 & \\textit{REVISE} & -55.3 & 4.18 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{10.9}} & \\color{green}{\\textbf{7.47}} \\\\\n  0.75 & 0.1 & 50 & \\textit{Generic} & -32.3 & 16.6 \\\\\n  0.75 & 0.1 & 50 & \\textit{REVISE} & -44.1 & 4.24 \\\\\n  0.9 & 0.1 & 5 & \\textit{ECCo} & -13.7 & 13.7 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{22.3}} & \\color{green}{\\textbf{4.87}} \\\\\n  0.9 & 0.1 & 5 & \\textit{REVISE} & -27.8 & 6.26 \\\\\n  0.9 & 0.1 & 25 & \\textit{ECCo} & 8.84 & 7.13 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{30.7}} & \\color{green}{\\textbf{5.77}} \\\\\n  0.9 & 0.1 & 25 & \\textit{REVISE} & -72.7 & 34.9 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{17.1}} & \\color{green}{\\textbf{7.98}} \\\\\n  0.9 & 0.1 & 50 & \\textit{Generic} & 3.82 & 8.86 \\\\\n  0.9 & 0.1 & 50 & \\textit{REVISE} & -45.8 & 2.79 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{35.7}} & \\color{green}{\\textbf{7.56}} \\\\\n  0.95 & 0.1 & 5 & \\textit{Generic} & -13.3 & 5.73 \\\\\n  0.95 & 0.1 & 5 & \\textit{REVISE} & -54.9 & 5.43 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{37}} & \\color{green}{\\textbf{5.31}} \\\\\n  0.95 & 0.1 & 25 & \\textit{Generic} & 29 & 8.6 \\\\\n  0.95 & 0.1 & 25 & \\textit{REVISE} & -40.5 & 41 \\\\\n  0.95 & 0.1 & 50 & \\textit{ECCo} & 13.2 & 4.89 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.1}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{35.5}} & \\color{green}{\\textbf{4.27}} \\\\\n  0.95 & 0.1 & 50 & \\textit{REVISE} & -60.4 & 45.8 \\\\\n  \\color{red}{\\textbf{0.75}} & \\color{red}{\\textbf{0.5}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{ECCo}} & \\color{red}{\\textbf{-0.94}} & \\color{red}{\\textbf{17.6}} \\\\\n  0.75 & 0.5 & 5 & \\textit{Generic} & -11.7 & 17.4 \\\\\n  0.75 & 0.5 & 5 & \\textit{REVISE} & -37.7 & 3.92 \\\\\n  0.75 & 0.5 & 25 & \\textit{ECCo} & 14.7 & 6.32 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{32.3}} & \\color{green}{\\textbf{3.49}} \\\\\n  0.75 & 0.5 & 25 & \\textit{REVISE} & -39.9 & 7.21 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{28.3}} & \\color{green}{\\textbf{5.05}} \\\\\n  0.75 & 0.5 & 50 & \\textit{Generic} & -9.72 & 9.19 \\\\\n  0.75 & 0.5 & 50 & \\textit{REVISE} & -30 & 5.22 \\\\\n  0.9 & 0.5 & 5 & \\textit{ECCo} & -7.38 & 13.6 \\\\\n  \\color{red}{\\textbf{0.9}} & \\color{red}{\\textbf{0.5}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{Generic}} & \\color{red}{\\textbf{-0.0952}} & \\color{red}{\\textbf{17.1}} \\\\\n  0.9 & 0.5 & 5 & \\textit{REVISE} & -50.6 & 4.77 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{39.1}} & \\color{green}{\\textbf{8.3}} \\\\\n  0.9 & 0.5 & 25 & \\textit{Generic} & 38.6 & 6.03 \\\\\n  0.9 & 0.5 & 25 & \\textit{REVISE} & -56.9 & 4.28 \\\\\n  0.9 & 0.5 & 50 & \\textit{ECCo} & -6.53 & 10.9 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{13.6}} & \\color{green}{\\textbf{12.8}} \\\\\n  0.9 & 0.5 & 50 & \\textit{REVISE} & -24.1 & 12.5 \\\\\n  0.95 & 0.5 & 5 & \\textit{ECCo} & 0.596 & 1.11 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{4.57}} & \\color{green}{\\textbf{5.88}} \\\\\n  0.95 & 0.5 & 5 & \\textit{REVISE} & -34.3 & 6.09 \\\\\n  0.95 & 0.5 & 25 & \\textit{ECCo} & 16.5 & 3.53 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{19.1}} & \\color{green}{\\textbf{4.43}} \\\\\n  0.95 & 0.5 & 25 & \\textit{REVISE} & -37.2 & 39.2 \\\\\n  0.95 & 0.5 & 50 & \\textit{ECCo} & -31 & 14.2 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{0.5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{24.8}} & \\color{green}{\\textbf{4.78}} \\\\\n  0.95 & 0.5 & 50 & \\textit{REVISE} & -1.28 & 2.7 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{37}} & \\color{green}{\\textbf{9.21}} \\\\\n  0.75 & 5 & 5 & \\textit{Generic} & 0.977 & 21.1 \\\\\n  0.75 & 5 & 5 & \\textit{REVISE} & -55.2 & 4.75 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{21.8}} & \\color{green}{\\textbf{5.97}} \\\\\n  0.75 & 5 & 25 & \\textit{Generic} & 17.8 & 5.33 \\\\\n  0.75 & 5 & 25 & \\textit{REVISE} & -68.1 & 8.69 \\\\\n  0.75 & 5 & 50 & \\textit{ECCo} & 0.864 & 9.51 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{8.56}} & \\color{green}{\\textbf{13.6}} \\\\\n  0.75 & 5 & 50 & \\textit{REVISE} & -71.8 & 3.18 \\\\\n  0.9 & 5 & 5 & \\textit{ECCo} & -3.59 & 20.3 \\\\\n  \\color{red}{\\textbf{0.9}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{Generic}} & \\color{red}{\\textbf{-1.18}} & \\color{red}{\\textbf{16.7}} \\\\\n  0.9 & 5 & 5 & \\textit{REVISE} & -58.6 & 3.55 \\\\\n  0.9 & 5 & 25 & \\textit{ECCo} & -1.73 & 9.07 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{21.9}} & \\color{green}{\\textbf{7.48}} \\\\\n  0.9 & 5 & 25 & \\textit{REVISE} & -44.4 & 4.23 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{30.8}} & \\color{green}{\\textbf{10.7}} \\\\\n  0.9 & 5 & 50 & \\textit{Generic} & 16.3 & 11.1 \\\\\n  0.9 & 5 & 50 & \\textit{REVISE} & -53.3 & 6.73 \\\\\n  \\color{red}{\\textbf{0.95}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{5}} & \\color{red}{\\textbf{ECCo}} & \\color{red}{\\textbf{-5.29}} & \\color{red}{\\textbf{24.5}} \\\\\n  0.95 & 5 & 5 & \\textit{Generic} & -5.71 & 17.4 \\\\\n  0.95 & 5 & 5 & \\textit{REVISE} & -36.5 & 4.01 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{26.9}} & \\color{green}{\\textbf{4.08}} \\\\\n  0.95 & 5 & 25 & \\textit{Generic} & 19.5 & 5.49 \\\\\n  0.95 & 5 & 25 & \\textit{REVISE} & -25 & 5 \\\\\n  0.95 & 5 & 50 & \\textit{ECCo} & 4.6 & 7.46 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{16.6}} & \\color{green}{\\textbf{4.3}} \\\\\n  0.95 & 5 & 50 & \\textit{REVISE} & -50.4 & 46.4 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{2.47}} & \\color{green}{\\textbf{8.57}} \\\\\n  0.75 & 10 & 5 & \\textit{Generic} & -7.23 & 9.13 \\\\\n  0.75 & 10 & 5 & \\textit{REVISE} & -67.2 & 5.75 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{29.9}} & \\color{green}{\\textbf{15.8}} \\\\\n  0.75 & 10 & 25 & \\textit{Generic} & 4.47 & 8.22 \\\\\n  0.75 & 10 & 25 & \\textit{REVISE} & -52 & 5.51 \\\\\n  0.75 & 10 & 50 & \\textit{ECCo} & 6.23 & 6.11 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{15.2}} & \\color{green}{\\textbf{10.2}} \\\\\n  0.75 & 10 & 50 & \\textit{REVISE} & -17 & 5.46 \\\\\n  0.9 & 10 & 5 & \\textit{ECCo} & -6.93 & 14.7 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{6.88}} & \\color{green}{\\textbf{5.11}} \\\\\n  0.9 & 10 & 5 & \\textit{REVISE} & -45.6 & 47.8 \\\\\n  0.9 & 10 & 25 & \\textit{ECCo} & 6.2 & 6.75 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{34.7}} & \\color{green}{\\textbf{9.74}} \\\\\n  0.9 & 10 & 25 & \\textit{REVISE} & -55.7 & 6.04 \\\\\n  0.9 & 10 & 50 & \\textit{ECCo} & 30.1 & 14.2 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{35}} & \\color{green}{\\textbf{7.11}} \\\\\n  0.9 & 10 & 50 & \\textit{REVISE} & -48.8 & 3.3 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{35.7}} & \\color{green}{\\textbf{3.83}} \\\\\n  0.95 & 10 & 5 & \\textit{Generic} & 2.85 & 7.9 \\\\\n  0.95 & 10 & 5 & \\textit{REVISE} & -34.9 & 4.56 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{33}} & \\color{green}{\\textbf{6.45}} \\\\\n  0.95 & 10 & 25 & \\textit{Generic} & 24.8 & 4.59 \\\\\n  0.95 & 10 & 25 & \\textit{REVISE} & -55.7 & 44 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{10}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{8.07}} & \\color{green}{\\textbf{7.68}} \\\\\n  0.95 & 10 & 50 & \\textit{Generic} & -7.11 & 9.2 \\\\\n  0.95 & 10 & 50 & \\textit{REVISE} & -26.1 & 18.6 \\\\\n  0.75 & 20 & 5 & \\textit{ECCo} & -10.8 & 20.8 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{12.6}} & \\color{green}{\\textbf{17.6}} \\\\\n  0.75 & 20 & 5 & \\textit{REVISE} & -21.6 & 3.34 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{17.2}} & \\color{green}{\\textbf{4.25}} \\\\\n  0.75 & 20 & 25 & \\textit{Generic} & -10.3 & 9.59 \\\\\n  0.75 & 20 & 25 & \\textit{REVISE} & -59 & 3.66 \\\\\n  \\color{green}{\\textbf{0.75}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{15.6}} & \\color{green}{\\textbf{8.38}} \\\\\n  0.75 & 20 & 50 & \\textit{Generic} & 3.92 & 6.2 \\\\\n  0.75 & 20 & 50 & \\textit{REVISE} & -70.9 & 5.16 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{26.3}} & \\color{green}{\\textbf{6.62}} \\\\\n  0.9 & 20 & 5 & \\textit{Generic} & 2.35 & 7.04 \\\\\n  0.9 & 20 & 5 & \\textit{REVISE} & -64.8 & 4.31 \\\\\n  0.9 & 20 & 25 & \\textit{ECCo} & 21.1 & 11.6 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{39}} & \\color{green}{\\textbf{5.98}} \\\\\n  0.9 & 20 & 25 & \\textit{REVISE} & -50.7 & 13.8 \\\\\n  \\color{green}{\\textbf{0.9}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{15.1}} & \\color{green}{\\textbf{7.46}} \\\\\n  0.9 & 20 & 50 & \\textit{Generic} & 8.53 & 6.95 \\\\\n  0.9 & 20 & 50 & \\textit{REVISE} & -36 & 6.14 \\\\\n  0.95 & 20 & 5 & \\textit{ECCo} & -23.8 & 17.8 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{5}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{7.23}} & \\color{green}{\\textbf{4.74}} \\\\\n  0.95 & 20 & 5 & \\textit{REVISE} & -34 & 4.11 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{25}} & \\color{green}{\\textbf{ECCo}} & \\color{green}{\\textbf{19.3}} & \\color{green}{\\textbf{14.6}} \\\\\n  0.95 & 20 & 25 & \\textit{Generic} & -1.35 & 7.71 \\\\\n  0.95 & 20 & 25 & \\textit{REVISE} & -52.8 & 5.11 \\\\\n  0.95 & 20 & 50 & \\textit{ECCo} & 18.7 & 3.84 \\\\\n  \\color{green}{\\textbf{0.95}} & \\color{green}{\\textbf{20}} & \\color{green}{\\textbf{50}} & \\color{green}{\\textbf{Generic}} & \\color{green}{\\textbf{37.3}} & \\color{green}{\\textbf{7.78}} \\\\\n  0.95 & 20 & 50 & \\textit{REVISE} & -28.3 & 15 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Linearly Separable data by energy penalty.\n\n:::\n\n\n\n\n\n<!-- {{< include /paper/sections/other/initial_grids.qmd >}} -->\n\n# Computation Details {.appendix}\n\n<!-- [@DHPC2022] -->\n## Hardware\n\nWe performed our experiments on a high-performance cluster. Details about the cluster will be disclosed upon publication to avoid revealing information that might interfere with the double-blind review process. Since our experiments involve highly parallel tasks and rather small models by today's standard, we have relied on distributed computing across multiple central processing units (CPU). Graphical processing units (GPU) were not required. For larger grid searches we used up to but typically less than 100 CPUs. \n\n## Software\n\nAll computations were performed in the Julia Programming Language [@bezanson2017julia]. We have developed a package for counterfactual training that leverages and extends the functionality provided by several existing packages, most notably [CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) [@altmeyer2023explaining] and the [Flux.jl](https://fluxml.ai/Flux.jl/v0.16/) library for deep learning [@innes2018fashionable;@innes2018flux]. For data-wrangling and presentation-ready tables we relied on [DataFrames.jl](https://dataframes.juliadata.org/v1.7/) [@milan2023dataframes] and [PrettyTables.jl](https://ronisbr.github.io/PrettyTables.jl/v2.4/) [@chagas2024pretty], respectively. For plots and visualizations we used both [Plots.jl](https://docs.juliaplots.org/v1.40/) [@PlotsJL] and [Makie.jl](https://docs.makie.org/v0.22/) [@danisch2021makie], in particular [AlgebraOfGraphics.jl](https://aog.makie.org/v0.9.3/). To distribute computational tasks across multiple processors, we have relied on [MPI.jl](https://juliaparallel.org/MPI.jl/v0.20/) [@byrne2021mpi].\n\n\n\n\n",
    "supporting": [
      "paper_files"
    ],
    "filters": []
  }
}