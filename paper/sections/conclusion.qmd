To conclude, state-of-the-art machine learning models are prone to learning complex representations that cannot be interpreted by humans. Existing explainability solutions cannot guarantee that explanations agree with the model's learned representation of data. As a step towards addressing this challenge, we introduced counterfactual training, a novel training regime that incentivizes highly-explainable models. Our approach leads to explanations that are both plausible (compliant with the underlying data-generating process) and actionable (compliant with user-specified mutability constraints), and thus meaningful to their recipients. Through extensive experiments, we demonstrated that CT satisfies its objective while promoting robustness and preserving the predictive performance of models. It can also be used to fine-tune conventionally-trained models and achieve similar gains. Thus, our work showcases that it is practical to simultaneously improve models and their explanations.