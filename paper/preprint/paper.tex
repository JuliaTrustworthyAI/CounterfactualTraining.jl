% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Latin Modern Roman}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{placeins}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
\newtheorem{corollary}{Research Question}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\newcounter{quartocalloutnteno}
\newcommand{\quartocalloutnte}[1]{\refstepcounter{quartocalloutnteno}\label{#1}}

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Counterfactual Training: Teaching Models Plausible and Actionable Explanations},
  pdfauthor={Patrick Altmeyer; Aleksander Buszydlik; Arie van Deursen; Cynthia C. S. Liem},
  pdfkeywords={Counterfactual Training, Counterfactual
Explanations, Algorithmic Recourse, Explainable AI, Representation
Learning},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\usepackage{lineno}
\linenumbers
\newcommand{\runninghead}{A Preprint }
\renewcommand{\runninghead}{Counterfactual Training (A Preprint) }
\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}
\def\asep{\\\\\\ } % default: all authors on same column
\def\asep{\And }
\author{\textbf{Patrick
Altmeyer}~\orcidlink{0000-0003-4726-8613}\\Faculty of Electrical
Engineering, Mathematics and Computer Science\\Delft University of
Technology\\\\\href{mailto:p.altmeyer@tudelft.nl}{p.altmeyer@tudelft.nl}\asep\textbf{Aleksander
Buszydlik}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Arie van
Deursen}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Cynthia C. S.
Liem}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime called Counterfactual Training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be plausible with
respect to the underlying data and actionable with respect to
stakeholder requirements. Much existing research has therefore focused
on developing post-hoc methods to generate counterfactuals that meet
these desiderata. In this work, we instead hold models directly
accountable for this desired end goal: Counterfactual Training employs
counterfactuals ad-hoc during the training phase to minimize the
divergence between learned representations and plausible, actionable
explanations. We demonstrate empirically and theoretically that our
proposed method facilitates training models that deliver inherently
desirable explanations while maintaining high predictive performance.
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
Counterfactual Training \sep Counterfactual
Explanations \sep Algorithmic Recourse \sep Explainable AI \sep 
Representation Learning



\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by advances in \textbf{representation learning}: instead of
relying on features and rules that are carefully hand-crafted by humans,
modern machine learning (ML) models are tasked with learning these
representations from scratch, guided by narrow objectives such as
predictive accuracy (\citeproc{ref-goodfellow2016deep}{I. Goodfellow,
Bengio, and Courville 2016}). Modern advances in computing have made it
possible to provide such models with ever greater degrees of freedom to
achieve that task, which has often led them to outperform traditionally
more parsimonious models. Unfortunately, in doing so they also learn
increasingly complex and highly sensitive representations that we can no
longer easily interpret.

This trend towards complexity for the sake of performance has come under
serious scrutiny in recent years. At the very cusp of the deep learning
revolution, Szegedy et al. (\citeproc{ref-szegedy2013intriguing}{2013})
showed that artificial neural networks (ANN) are sensitive to
adversarial examples: counterfactuals of model inputs that yield vastly
different model predictions despite being ``imperceptible'' in that they
are semantically indifferent from their factual counterparts. Despite
partially effective mitigation strategies such as \textbf{adversarial
training} (\citeproc{ref-goodfellow2014explaining}{I. J. Goodfellow,
Shlens, and Szegedy 2014}), truly robust deep learning (DL) remains
unattainable even for models that are considered shallow by today's
standards (\citeproc{ref-kolter2023keynote}{Kolter 2023}).

Part of the problem is that high degrees of freedom provide room for
many solutions that are locally optimal with respect to narrow
objectives (\citeproc{ref-wilson2020case}{Wilson 2020})\footnote{For
  clarity: we follow standard ML convention in using ``degrees of
  freedom'' to refer to the number of parameters estimated from data.}.
Based purely on predictive performance, these solutions may seem to
provide compelling explanations for the data, when in fact they are
based on purely associative, semantically meaningless patterns. This
poses two related challenges: firstly, it makes these models inherently
opaque, since humans cannot simply interpret what type of explanation
the complex learned representations correspond to; secondly, even if we
could resolve the first challenge, it is not obvious how to mitigate
models from learning representations that correspond to meaningless and
implausible explanations.

The first challenge has attracted an abundance of research on
\textbf{explainable AI} (XAI) which aims to develop tools to derive
explanations from complex model representations. This can mitigate a
scenario in which we deploy opaque models and blindly rely on their
predictions. On countless occasions, this scenario has already occurred
in practice and caused real harm to people who were affected adversely
and often unfairly by automated decision-making systems (ADMS) involving
opaque models (\citeproc{ref-oneil2016weapons}{O'Neil 2016}). Effective
XAI tools can aide us in monitoring models and providing recourse to
individuals to turn adverse outcomes (e.g.~``loan application
rejected'') into positive ones (``application accepted''). Wachter,
Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) propose
\textbf{counterfactual explanations} as an effective approach to achieve
this: they explain how factual inputs need to change in order for some
fitted model to produce some desired output, typically involving minimal
perturbations.

To our surprise, the second challenge has not yet attracted any
consolidated research effort. Specifically, there has been no concerted
effort towards improving model \textbf{explainability}, which we define
here as the degree to which learned representations correspond to
explanations that are interpretable and deemed \textbf{plausible} by
humans (see Definition~\ref{def-explainability}). Instead, the choice
has typically been to improve the capacity of XAI tools to identify the
subset explanations that are both plausible and valid for any given
model, independent of whether the learned representations are also
compatible with implausible explanations
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).
Fortunately, recent findings indicate that explainability can arise as
byproduct of regularization techniques aimed at other objectives such as
robustness, generalization and generative capacity Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}).

Building on these findings, we introduce \textbf{counterfactual
training}: a novel regularization technique geared explicitly towards
aligning model representations with plausible explanations. Our
contributions are as follows:

\begin{itemize}
\tightlist
\item
  We discuss existing related work on improving models and consolidate
  it through the lens of counterfactual explanations
  (Section~\ref{sec-lit}).
\item
  We present our proposed methodological framework that leverages
  faithful counterfactual explanations during the training phase of
  models to achieve the explainability objective
  (Section~\ref{sec-method}).
\item
  Through extensive experiments we demonstrate the counterfactual
  training improve model explainability while maintaining high
  predictive performance. We run ablation studies and grid searches to
  understand how the underlying model components and hyperparameters
  affect outcomes. (Section~\ref{sec-experiments}).
\end{itemize}

Despite limitations of our approach discussed in
Section~\ref{sec-discussion}, we conclude that counterfactual training
provides a practical framework for researchers and practitioners
interested in making opaque models more trustworthy
Section~\ref{sec-conclusion}. We also believe that this work serves as
an opportunity for XAI researchers to reevaluate the premise of
improving XAI tools without improving models.

\section{Related Literature}\label{sec-lit}

To the best of our knowledge, our proposed framework for counterfactual
training represents the first attempt to use counterfactual explanations
during training to improve model explainability. In high-level terms, we
define model explainability as the extent to which valid explanations
derived for an opaque model are also deemed plausible with respect to
the underlying data and stakeholder requirements. To make this more
concrete, we follow Augustin, Meinke, and Hein
(\citeproc{ref-augustin2020adversarial}{2020}) in tieing the concept of
explainability to the quality of counterfactual explanations that we can
generate for a given model. The authors show that counterfactual
explanations---understood here as minimal input perturbations that yield
some desired model prediction---are generally more meaningful if the
underlying model is more robust to adversarial examples. We can make
intuitive sense of this finding when looking at adversarial training
(AT) through the lens of representation learning with high degrees of
freedom: by inducing models to ``unlearn'' representations that are
susceptible to worst-case counterfactuals (i.e.~adversarial examples),
AT effectively removes some implausible explanations from the solution
space.

\subsection{Adversarial Examples are Counterfactual
Explanations}\label{adversarial-examples-are-counterfactual-explanations}

This interpretation of the link between explainability through
counterfactuals on one side, and robustness to adversarial examples on
the other, is backed by empirical evidence. Sauer and Geiger
(\citeproc{ref-sauer2021counterfactual}{2021}) demonstrate that using
counterfactual images during classifier training improves model
robustness. Similarly, Abbasnejad et al.
(\citeproc{ref-abbasnejad2020counterfactual}{2020}) argue that
counterfactuals represent potentially useful training data in machine
learning, especially in supervised settings where inputs may be
reasonably mapped to multiple outputs. They, too, demonstrate the
augmenting the training data of image classifiers can improve
generalization. Teney, Abbasnedjad, and Hengel
(\citeproc{ref-teney2020learning}{2020}) propose an approach using
counterfactuals in training that does not rely on data augmentation:
they argue that counterfactual pairs typically already exist in training
datasets. Specifically, their approach relies on, firstly, identifying
similar input samples with different annotations and, secondly, ensuring
that the gradient of the classifier aligns with the vector between pairs
of counterfactual inputs using the cosine distance as a loss function.

In the natural language processing (NLP) domain, counterfactuals have
similarly been used to improve models through data augmentation: Wu et
al. (\citeproc{ref-wu2021polyjuice}{2021}), propose \emph{Polyjuice}, a
general-purpose counterfactual generator for language models. They
demonstrate empirically that augmenting training data through
\emph{Polyjuice} counterfactuals improves robustness in a number of NLP
tasks. Balashankar et al.
(\citeproc{ref-balashankar2023improving}{2023}) also use
\emph{Polyjuice} to augment NLP datasets through diverse counterfactuals
and show that classifier robustness improves up to 20\%. Finally, Luu
and Inoue (\citeproc{ref-luu2023counterfactual}{2023}) introduce
Counterfactual Adversarial Training (CAT), which also aims at improving
generalization and robustness of language models. Specifically, they
propose to proceed as follows: firstly, they identify training samples
that are subject to high predictive uncertainty; secondly, they generate
counterfactual explanations for those samples; and, finally, they
fine-tune the given language model on the augmented dataset that
includes the generated counterfactuals.

There have also been several attempts at formalizing the relationship
between counterfactual explanations (CE) and adversarial examples (AE).
Pointing to clear similarities in how CE and AE are generated,
Freiesleben (\citeproc{ref-freiesleben2022intriguing}{2022}) makes the
case for jointly studying the opaqueness and robustness problem in
representation learning. Formally, AE can be seen as the subset of CE,
for which misclassification is achieved
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022}). Similarly,
Pawelczyk et al. (\citeproc{ref-pawelczyk2022exploring}{2022}) show that
CE and AE are equivalent under certain conditions and derive theoretical
upper bounds on the distances between them.

Two recent works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of post-hoc counterfactual explanations. Firstly,
Ross, Lakkaraju, and Bastani (\citeproc{ref-ross2021learning}{2024})
propose a way to train models that are guaranteed to provide recourse
for individuals to move from an adverse outcome to some positive target
class with high probability. The approach proposed by Ross, Lakkaraju,
and Bastani (\citeproc{ref-ross2021learning}{2024}) builds on
adversarial training, where in this context susceptibility to targeted
adversarial examples for the positive class is explicitly induced. The
proposed method allows for imposing a set of actionability constraints
ex-ante: for example, users can specify that certain features
(e.g.~\emph{age}, \emph{gender}, \ldots) are immutable. Secondly, Guo,
Nguyen, and Yadav (\citeproc{ref-guo2023counternet}{2023}) are the first
to propose an end-to-end training pipeline that includes counterfactual
explanations as part of the training procedure. In particular, they
propose a specific network architecture that includes a predictor and CE
generator network, where the parameters of the CE generator network are
learnable. Counterfactuals are generated during each training iteration
and fed back to the predictor network. In contrast to Guo, Nguyen, and
Yadav (\citeproc{ref-guo2023counternet}{2023}), we impose no
restrictions on the neural network architecture at all.

\subsection{Beyond Robustness}\label{beyond-robustness}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a work
closely related to this one, Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) show that explainability can
be improved through model averaging and refined model objectives. The
authors propose a way to generate counterfactuals that are maximally
\textbf{faithful} to the model in that they are consistent with what the
model has learned about the underlying data. Formally, they rely on
tools from energy-based modelling to minimize the divergence between the
distribution of counterfactuals and the conditional posterior over
inputs learned by the model. Their proposed counterfactual explainer,
\emph{ECCCo}, yields plausible explanations if and only if the
underlying model has learned representations that align with them. They
find that both deep ensembles
(\citeproc{ref-lakshminarayanan2016simple}{Lakshminarayanan, Pritzel,
and Blundell 2017}) and joint energy-based models (JEMs)
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}) tend to do
well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are most called for when
models are underspecified by the available data
(\citeproc{ref-wilson2020case}{Wilson 2020}). Averaging across solutions
mitigates the aforementioned risk of relying on a single locally optimal
representations that corresponds to semantically meaningless
explanations for the data. Previous work by Schut et al.
(\citeproc{ref-schut2021generating}{2021}) similarly found that
generating plausible (``interpretable'') counterfactual explanations is
almost trivial for deep ensembles that have also undergone adversarial
training. The case for JEMs is even clearer: they involve a hybrid
objective that induces both high predictive performance and generative
capacity (\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}). This
is closely related to the idea of aligning models with plausible
explanations and has inspired our proposed counterfactual training
objective, as we explain in Section~\ref{sec-method}.

\section{Counterfactual Training}\label{sec-method}

Counterfactual training combines ideas from adversarial training,
energy-based modelling and counterfactuals explanations with the
explicit objective of aligning representations with plausible
explanations that comply with user requirements. In the context of CE,
plausibility has broadly been defined as the degree to which
counterfactuals comply with the underlying data generating process
(\citeproc{ref-poyiadzi2020face}{Poyiadzi et al. 2020};
\citeproc{ref-guidotti2022counterfactual}{Guidotti 2022};
\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}). Plausibility
is a necessary but insufficient condition for using CE to provide
algorithmic recourse (AR) to individuals affected by opaque models in
practice. This is because for recourse recommendations to be
\textbf{actionable}, they need to not only result in plausible
counterfactuals but also be attainable. A plausible CE for a rejected
20-year-old loan applicant, for example, might reveal that their
application would have been accepted, if only they were 20 years older.
Ignoring all other features, this complies with the definition of
plausibility if 40-year-old individuals are in fact more credit-worthy
on average than young adults. But of course this CE does not qualify for
providing actionable recourse to the applicant since \emph{age} is not a
mutable feature. For our intents and purposes, counterfactual training
aims at improving model explainability by aligning models with
counterfactuals that meet both desiderata, plausibility and
actionability. Formally, we define explainability as follows:

\begin{definition}[Model
Explainability]\protect\hypertarget{def-explainability}{}\label{def-explainability}

Let \(\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}\) denote a
supervised classification model that maps from the \(D\)-dimensional
input space \(\mathcal{X}\) to representations
\(\phi(\mathbf{x};\theta)\) and finally to the \(K\)-dimensional output
space \(\mathcal{Y}\). Assume that for any given input-output pair
\(\{\mathbf{x},\mathbf{y}\}_i\) there exists a counterfactual
\(\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})\)
where \(\arg\max_y{\mathbf{y}^{+}}=y^+\) and \(y^+\) denotes the index
of the target class.

We say that \(\mathbf{M}_\theta\) is \textbf{explainable} to the extent
that faithfully generated counterfactuals are plausible (i.e.~consistent
with the data) and actionable. Formally, we define these properties as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Plausibility)
  \(\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is some small region around \(\mathbf{x}^{\prime}\).
\item
  (Actionability) Permutations \(\Delta\) are subject to actionability
  constraints.
\end{enumerate}

We consider counterfactuals as faithful to the extent that they are
consistent with what the model has learned about the input data. Let
\(p_\theta(\mathbf{x}|\mathbf{y}^{+})\) denote the conditional posterior
over inputs, then formally:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  (Faithfulness)
  \(\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is defined as above.
\end{enumerate}

\end{definition}

The definitions of faithfulness and plausibility in
Definition~\ref{def-explainability} are the same as in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}), with adapted notation.
Actionability constraints in Definition~\ref{def-explainability} vary
and depend on the context in which \(\mathbf{M}_\theta\) is deployed. In
this work, we focus on domain and mutability constraints for individual
features \(x_d\) for \(d=1,...,D\). We limit ourselves to classification
tasks for reasons discussed in Section~\ref{sec-discussion}.

\subsection{Our Proposed Objective}\label{our-proposed-objective}

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
explanation generated through gradient descent over \(T\) iterations as
initially proposed by Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). For our purposes, we
let \(T\) vary and consider the counterfactual search as converged as
soon as the predicted probability for the target class has reached a
pre-determined threshold, \(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\)\footnote{For
  detailed background information on gradient-based counterfactual
  search and convergence see Section~\ref{sec-app-ce}.}.

To train models with high explainability as defined in
Definition~\ref{def-explainability}, we propose to leverage
counterfactuals in the following objective,

\begin{equation}\phantomsection\label{eq-obj}{
\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x},\mathbf{x}_T^\prime,y;\theta) + \lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y})
}\end{equation}

where \(\text{yloss}(\cdot)\) is any conventional classification loss
that induces discriminative performance (e.g.~cross-entropy). The two
additional components in Equation~\ref{eq-obj} are explained in more
detail below. For now, they can be sufficiently described as inducing
explainability directly and indirectly by penalizing: 1) the contrastive
divergence, \(\text{div}(\cdot)\), between counterfactuals
\(\mathbf{x}_T^\prime\) and observed samples \(x\) and, 2) the
adversarial loss, \(\text{advloss}(.)\), with respect to nascent
counterfactuals \(\mathbf{x}_{t\leq T}^\prime\). The tradeoff between
the different components can be governed by adjusting the strengths of
the penalties \(\lambda_{\text{div}}\) and \(\lambda_{\text{adv}}\).

\subsubsection{Directly Inducing Explainability through Contrastive
Divergence}\label{directly-inducing-explainability-through-contrastive-divergence}

Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) observe that
any classifier can be re-interpreted as a joint energy-based model (JEM)
that learns to discriminate output classes conditional on inputs and
generate inputs. They show that JEMs can be trained to perform well at
both tasks by directly maximizing the joint log-likelihood factorized as
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\).
The first factor can be optimized using conventional cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\)
Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) minimize the
contrastive divergence between samples drawn from
\(p_\theta(\mathbf{x})\) and training observations, i.e.~samples from
\(p(\mathbf{x})\).

A key empirical finding in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) was that JEMs tend to do
well with respect to the plausibility objective in
Definition~\ref{def-explainability}. If we consider samples drawn from
\(p_\theta(\mathbf{x})\) as counterfactuals, this is an expected
finding, because the JEM objective effectively minimizes the divergence
between the conditional posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\).
To generate samples, Grathwohl et al.
(\citeproc{ref-grathwohl2020your}{2020}) rely on Stochastic Gradient
Langevin Dynamics (SGLD) using an uninformative prior for
initialization. This is where we depart from their methodology: instead
of generating samples through SGLD, we propose using counterfactual
explainers to generate counterfactuals for observed training samples.
Specifically, we have

\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x},\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x},y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation}

where \(\mathcal{E}_\theta(\cdot)\) denotes the energy function. In
particular, we set
\(\mathcal{E}_\theta(\mathbf{x},\mathbf{y})=-\mathbf{M}_\theta(\mathbf{x})[y^+]\)
where \(y^+\) denotes the index of the target class. We generate samples
\(\mathbf{x}_T^\prime\) by first randomly sampling the target class
\(y^+ \sim p(y)\) and then generating a counterfactual explanation for
that target over \(T\) iterations using a gradient-based counterfactual
generator. This is similar to how conditional sampling is used to draw
from \(p_\theta(\mathbf{x})\) in Grathwohl et al.
(\citeproc{ref-grathwohl2020your}{2020}).

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while at same time
increasing the energy of counterfactuals (negative samples)
(\citeproc{ref-du2019implicit}{Du and Mordatch 2020}). As the generated
counterfactuals get more plausible (Definition~\ref{def-explainability})
over the cause of training, these two opposing effects gradually balance
each out (\citeproc{ref-lippe2024uvadlc}{Lippe 2024}).

The departure from SGLD allows us to tap into the vast repertoire of
explainers that have been proposed in the literature to meet different
desiderata. Typically, these methods facilitate the imposition of domain
and mutability constraints, for example. In principle, any existing
approach for generating counterfactual explanations is viable, so long
as it does not violate the faithfulness condition. Like JEMs
(\citeproc{ref-murphy2022probabilistic}{Murphy 2022}), counterfactual
training can be considered as a form of contrastive representation
learning.

\subsubsection{Indirectly Inducing Explainability through Adversarial
Robustness}\label{indirectly-inducing-explainability-through-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
(\citeproc{ref-luu2023counterfactual}{Luu and Inoue 2023};
\citeproc{ref-balashankar2023improving}{Balashankar et al. 2023}) or
adversarial examples
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022};
\citeproc{ref-pawelczyk2022exploring}{Pawelczyk et al. 2022}). This
leaves some flexibility with respect to the exact choice for
\(\text{advloss}(\cdot)\) in Equation~\ref{eq-obj}. An intuitive
functional form to use, though likely not the only reasonable choice, is
inspired by adversarial training:

\begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation}

Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as adversarial examples as long as the
magnitude of the perturbation to any individual feature is at most
\(\varepsilon\). This is closely aligned with Szegedy et al.
(\citeproc{ref-szegedy2013intriguing}{2013}), who define an adversarial
attack as an ``imperceptible non-random perturbation''. Thus, we choose
to work with a different distinction between CE and AE than Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}), who considers
misclassification as the key distinguishing feature of AE. One of the
key observations in this work is that we can leverage counterfactual
explanations during training and get adversarial examples, essentially
for free.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any counterfactual explainer that relies on gradient
descent in the feature space for optimization
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, et al. 2023}).
In this context, domain constraints can be imposed by simply projecting
counterfactuals back to the specified domain, if the previous gradient
step resulted in updated feature values that were out-of-domain.
Mutability constraints can similarly be enforced by setting partial
derivatives to zero to ensure that features are only mutated in the
allowed direction, if at all.

Since actionability constraints are binding at test time, we should also
impose them when generating \(\mathbf{x}^\prime\) during each training
iteration to align model representations with user requirements. Through
their effect on \(\mathbf{x}^\prime\), both types of constraints
influence model outcomes through Equation~\ref{eq-div}. Here it is
crucial that we avoid penalizing implausibility that arises due to
mutability constraints. For any mutability-constrained feature \(d\)
this can be achieved by enforcing
\(\mathbf{x}[d] - \mathbf{x}^\prime[d]:=0\) whenever perturbing
\(\mathbf{x}^\prime[d]\) in the direction of \(\mathbf{x}[d]\) would
violate mutability constraints. Specifically, we set
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) if

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  We have \(\mathbf{x}[d]>\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be decreased in practice.
\item
  We have \(\mathbf{x}[d]<\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be increased in practice.
\end{enumerate}

From a Bayesian perspective, setting
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) can be understood as assuming
a point mass prior for \(p(\mathbf{x})\) with respect to feature \(d\).
Intuitively, we think of this simply in terms ignoring implausibility
costs with respect to immutable features, which effectively forces the
model to instead seek plausibility with respect to the remaining
features. This in turn results in lower overall sensitivity to immutable
features, which we demonstrate empirically for different classifiers in
Section~\ref{sec-experiments}. Under certain conditions, this results
holds theoretically{[}For the proof, see the supplementary appendix.{]}:

\begin{proposition}[Protecting Immutable
Features]\protect\hypertarget{prp-mtblty}{}\label{prp-mtblty}

Let
\(f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})\)
denote a linear classifier with softmax activation \(\mathcal{S}\)
(i.e.~\emph{multinomial logistic regression}) where
\(y\in\{1,...,K\}=\mathcal{K}\) and \(\mathbf{x} \in \mathbb{R}^D\). If
we assume multivariate Gaussian class densities with common diagonal
covariance matrix \(\Sigma_k=\Sigma\) for all \(k \in \mathcal{K}\),
then protecting an immutable feature from the contrastive divergence
penalty (Equation~\ref{eq-div}) will result in lower classifier
sensitivity to that feature relative to the remaining features, provided
that at least one of those is mutable and discriminative.

\end{proposition}

It is worth highlighting that Proposition~\ref{prp-mtblty} assumes
independence of features. This raises a valid concern about the effect
of protecting immutable features in the presence of proxy features that
remain unprotected. We discuss this limitation in
Section~\ref{sec-discussion}.

\subsection{Illustration}\label{illustration}

To better convey the intuition underlying our proposed method, we
illustrate different model outcomes in Example~\ref{exm-grad}.

\begin{example}[Prediction of Consumer Credit
Default]\protect\hypertarget{exm-grad}{}\label{exm-grad}

Suppose we are interested in predicting the likelihood that loan
applicants default on their credit. We have access to historical data on
previous loan takers comprised of a binary outcome variable
(\(y\in\{1=\text{default},2=\text{no default}\}\)) two input features:
1) the subjects' \emph{age}, which we define as immutable, and 2) the
subjects' existing level of \emph{debt}, which we define as mutable.

We have simulated this scenario using synthetic data with independent
features and Gaussian class-conditional densities in
Figure~\ref{fig-poc}. The four panels in Figure~\ref{fig-poc} show the
outcomes for different training procedures using the same model
architecture each time (a linear classifier). In each case, we show the
linear decision boundary (green) and the training data colored according
to their ground-truth label: orange points belong to the target class,
\(y^+=2\), blue points belong to the non-target class, \(y^-=1\). Stars
indicate counterfactuals in the target class generated at test time
using generic gradient descent until convergence.

In panel (a), we have trained our model conventionally, and we do not
impose mutability constraints at test time. The generated
counterfactuals are all valid, but not plausible: they are clearly
distinguishable from the ground-truth data. In panel (b), we have
trained our model with counterfactual training, once again not imposing
mutability constraints at test time. We observe that the counterfactuals
are clearly plausible, therefore meeting the first objective of
Definition~\ref{def-explainability}.

In panel (c), we have used conventional training again, this time
imposing the mutability constraint on \emph{age} at test time.
Counterfactuals are valid but involve some substantial reductions in
\emph{debt} for some individuals, in particular very young applicants.
By comparison, counterfactual paths are shorter on average in panel (d),
where we have used counterfactual training and protected immutable
features as described in Section~\ref{sec-constraints}. In particular,
we observe that due to the classifier's lower sensitivity to \emph{age},
recourse recommendations with respect to \emph{debt} are much more
homogenous, in that they do not disproportionately punish younger
individuals. The counterfactuals are also plausible with respect to the
mutable feature. Thus, we consider the model in panel (d) as the most
explainable according to Definition~\ref{def-explainability}.

\end{example}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Visual illustration of how counterfactual
training improves explainability. See Example~\ref{exm-grad} for
details.}

\end{figure}%

\section{Experiments}\label{sec-experiments}

In this section, we present experiments that we have conducted in order
to answer the following research questions:

\begin{corollary}[Plausibility]\protect\hypertarget{cor-plaus}{}\label{cor-plaus}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) induce models to learn plausible explanations?

\end{corollary}

\begin{corollary}[Actionability]\protect\hypertarget{cor-action}{}\label{cor-action}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) yield more favorable algorithmic recourse
outcomes in the presence of actionability constraints?

\end{corollary}

Beyond this, we are also interested in understanding how robust our
answers to RQ~\ref{cor-plaus} and RQ~\ref{cor-action} are:

\begin{corollary}[Hyperparameters]\protect\hypertarget{cor-hyper}{}\label{cor-hyper}

What are the effects of different hyperparameter choices with respect to
Equation~\ref{eq-obj}?

\end{corollary}

\subsection{Experimental Setup}\label{experimental-setup}

\subsection{Experimental Results}\label{experimental-results}

\section{Discussion}\label{sec-discussion}

\subsection{Approach/Future Directions}\label{approachfuture-directions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Limited to classification models.
\item
  Training instabilities.
\item
  Hyperparameter sensitivity -\textgreater{} can we do better than grid
  search? (Bayes opt, \ldots)
\end{enumerate}

\subsection{Limitations}\label{limitations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Proxy attributes of immutable features.
\item
  Increased training time.
\item
  Fairness and caveats (aware it's not a classical approach in this
  context, but there is a clear link).
\end{enumerate}

\section{Conclusion}\label{sec-conclusion}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abbasnejad2020counterfactual}
Abbasnejad, Ehsan, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van
den Hengel. 2020. {``Counterfactual Vision and Language Learning.''} In
\emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 10041--51.
\url{https://doi.org/10.1109/CVPR42600.2020.01006}.

\bibitem[\citeproctext]{ref-altmeyer2023explaining}
Altmeyer, Patrick, Arie van Deursen, et al. 2023. {``Explaining
Black-Box Models Through Counterfactuals.''} In \emph{Proceedings of the
JuliaCon Conferences}, 1:130. 1.

\bibitem[\citeproctext]{ref-altmeyer2024faithful}
Altmeyer, Patrick, Mojtaba Farmanbar, Arie van Deursen, and Cynthia CS
Liem. 2024. {``Faithful Model Explanations Through Energy-Constrained
Conformal Counterfactuals.''} In \emph{Proceedings of the AAAI
Conference on Artificial Intelligence}, 38:10829--37. 10.

\bibitem[\citeproctext]{ref-augustin2020adversarial}
Augustin, Maximilian, Alexander Meinke, and Matthias Hein. 2020.
{``Adversarial Robustness on in-and Out-Distribution Improves
Explainability.''} In \emph{European Conference on Computer Vision},
228--45. Springer.

\bibitem[\citeproctext]{ref-balashankar2023improving}
Balashankar, Ananth, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed
Chi, Jilin Chen, and Alex Beutel. 2023. {``Improving Classifier
Robustness Through Active Generative Counterfactual Data
Augmentation.''} In \emph{Findings of the Association for Computational
Linguistics: EMNLP 2023}, 127--39.

\bibitem[\citeproctext]{ref-bezanson2017julia}
Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017.
{``Julia: A Fresh Approach to Numerical Computing.''} \emph{SIAM Review}
59 (1): 65--98. \url{https://doi.org/10.1137/141000671}.

\bibitem[\citeproctext]{ref-milan2023dataframes}
Bouchet-Valat, Milan, and Bogumił Kamiński. 2023. {``DataFrames.jl:
Flexible and Fast Tabular Data in Julia.''} \emph{Journal of Statistical
Software} 107 (4): 1--32. \url{https://doi.org/10.18637/jss.v107.i04}.

\bibitem[\citeproctext]{ref-byrne2021mpi}
Byrne, Simon, Lucas C. Wilcox, and Valentin Churavy. 2021. {``MPI.jl:
Julia Bindings for the Message Passing Interface.''} \emph{Proceedings
of the JuliaCon Conferences} 1 (1): 68.
\url{https://doi.org/10.21105/jcon.00068}.

\bibitem[\citeproctext]{ref-chagas2024pretty}
Chagas, Ronan Arraes Jardim, Ben Baumgold, Glen Hertz, Hendrik Ranocha,
Mark Wells, Nathan Boyer, Nicholas Ritchie, et al. 2024.
{``Ronisbr/PrettyTables.jl: V2.4.0.''} Zenodo.
\url{https://doi.org/10.5281/zenodo.13835553}.

\bibitem[\citeproctext]{ref-PlotsJL}
Christ, Simon, Daniel Schwabeneder, Christopher Rackauckas, Michael
Krabbe Borregaard, and Thomas Breloff. 2023. {``Plots.jl -- a User
Extendable Plotting API for the Julia Programming Language.''}
https://doi.org/\url{https://doi.org/10.5334/jors.431}.

\bibitem[\citeproctext]{ref-danisch2021makie}
Danisch, Simon, and Julius Krumbiegel. 2021. {``{Makie.jl}: Flexible
High-Performance Data Visualization for {Julia}.''} \emph{Journal of
Open Source Software} 6 (65): 3349.
\url{https://doi.org/10.21105/joss.03349}.

\bibitem[\citeproctext]{ref-du2019implicit}
Du, Yilun, and Igor Mordatch. 2020. {``Implicit Generation and
Generalization in Energy-Based Models.''}
\url{https://arxiv.org/abs/1903.08689}.

\bibitem[\citeproctext]{ref-freiesleben2022intriguing}
Freiesleben, Timo. 2022. {``The Intriguing Relation Between
Counterfactual Explanations and Adversarial Examples.''} \emph{Minds and
Machines} 32 (1): 77--109.

\bibitem[\citeproctext]{ref-goodfellow2014explaining}
Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014.
{``Explaining and Harnessing Adversarial Examples.''}
\url{https://arxiv.org/abs/1412.6572}.

\bibitem[\citeproctext]{ref-goodfellow2016deep}
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \emph{Deep
{Learning}}. {MIT Press}.

\bibitem[\citeproctext]{ref-grathwohl2020your}
Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
Mohammad Norouzi, and Kevin Swersky. 2020. {``Your Classifier Is
Secretly an Energy Based Model and You Should Treat It Like One.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-guidotti2022counterfactual}
Guidotti, Riccardo. 2022. {``Counterfactual Explanations and How to Find
Them: Literature Review and Benchmarking.''} \emph{Data Mining and
Knowledge Discovery}, 1--55.

\bibitem[\citeproctext]{ref-guo2023counternet}
Guo, Hangzhi, Thanh H. Nguyen, and Amulya Yadav. 2023. {``CounterNet:
End-to-End Training of Prediction Aware Counterfactual Explanations.''}
In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining}, 577--89. KDD '23. New York, NY, USA:
Association for Computing Machinery.
\url{https://doi.org/10.1145/3580305.3599290}.

\bibitem[\citeproctext]{ref-hastie2009elements}
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning}. Springer New York.
\url{https://doi.org/10.1007/978-0-387-84858-7}.

\bibitem[\citeproctext]{ref-innes2018fashionable}
Innes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco
Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and
Viral Shah. 2018. {``Fashionable Modelling with Flux.''}
\url{https://arxiv.org/abs/1811.01457}.

\bibitem[\citeproctext]{ref-innes2018flux}
Innes, Mike. 2018. {``Flux: {Elegant} Machine Learning with {Julia}.''}
\emph{Journal of Open Source Software} 3 (25): 602.
\url{https://doi.org/10.21105/joss.00602}.

\bibitem[\citeproctext]{ref-joshi2019realistic}
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and
Joydeep Ghosh. 2019. {``Towards Realistic Individual Recourse and
Actionable Explanations in Black-Box Decision Making Systems.''}
\url{https://arxiv.org/abs/1907.09615}.

\bibitem[\citeproctext]{ref-kolter2023keynote}
Kolter, Zico. 2023.{``{Keynote Addresses: SaTML 2023 }.''} In \emph{2023
IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
xvi--. Los Alamitos, CA, USA: IEEE Computer Society.
\url{https://doi.org/10.1109/SaTML54575.2023.00009}.

\bibitem[\citeproctext]{ref-lakshminarayanan2016simple}
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2017.
{``Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.''} \emph{Advances in Neural Information Processing Systems}
30.

\bibitem[\citeproctext]{ref-lippe2024uvadlc}
Lippe, Phillip. 2024. {``{UvA Deep Learning Tutorials}.''}
\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}.

\bibitem[\citeproctext]{ref-luu2023counterfactual}
Luu, Hoai Linh, and Naoya Inoue. 2023. {``Counterfactual Adversarial
Training for Improving Robustness of Pre-Trained Language Models.''} In
\emph{Proceedings of the 37th Pacific Asia Conference on Language,
Information and Computation}, 881--88.

\bibitem[\citeproctext]{ref-murphy2022probabilistic}
Murphy, Kevin P. 2022. \emph{Probabilistic {Machine Learning}: {An}
Introduction}. {MIT Press}.

\bibitem[\citeproctext]{ref-oneil2016weapons}
O'Neil, Cathy. 2016. \emph{Weapons of Math Destruction: {How} Big Data
Increases Inequality and Threatens Democracy}. {Crown}.

\bibitem[\citeproctext]{ref-pawelczyk2022exploring}
Pawelczyk, Martin, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and
Himabindu Lakkaraju. 2022. {``Exploring Counterfactual Explanations
Through the Lens of Adversarial Examples: A Theoretical and Empirical
Analysis.''} In \emph{Proceedings of the 25th International Conference
on Artificial Intelligence and Statistics}, edited by Gustau
Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, 151:4574--94.
Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v151/pawelczyk22a.html}.

\bibitem[\citeproctext]{ref-poyiadzi2020face}
Poyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and
Peter Flach. 2020. {``{FACE}: {Feasible} and Actionable Counterfactual
Explanations.''} In \emph{Proceedings of the {AAAI}/{ACM Conference} on
{AI}, {Ethics}, and {Society}}, 344--50.

\bibitem[\citeproctext]{ref-ross2021learning}
Ross, Alexis, Himabindu Lakkaraju, and Osbert Bastani. 2024. {``Learning
Models for Actionable Recourse.''} In \emph{Proceedings of the 35th
International Conference on Neural Information Processing Systems}. NIPS
'21. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-sauer2021counterfactual}
Sauer, Axel, and Andreas Geiger. 2021. {``Counterfactual Generative
Networks.''} \url{https://arxiv.org/abs/2101.06046}.

\bibitem[\citeproctext]{ref-schut2021generating}
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan
Sacaleanu, Yarin Gal, et al. 2021. {``Generating {Interpretable
Counterfactual Explanations By Implicit Minimisation} of {Epistemic} and
{Aleatoric Uncertainties}.''} In \emph{International {Conference} on
{Artificial Intelligence} and {Statistics}}, 1756--64. {PMLR}.

\bibitem[\citeproctext]{ref-szegedy2013intriguing}
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. {``Intriguing
Properties of Neural Networks.''} \url{https://arxiv.org/abs/1312.6199}.

\bibitem[\citeproctext]{ref-teney2020learning}
Teney, Damien, Ehsan Abbasnedjad, and Anton van den Hengel. 2020.
{``Learning What Makes a Difference from Counterfactual Examples and
Gradient Supervision.''} In \emph{Computer Vision--ECCV 2020: 16th
European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part
x 16}, 580--99. Springer.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box:
{Automated} Decisions and the {GDPR}.''} \emph{Harv. JL \& Tech.} 31:
841. \url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wilson2020case}
Wilson, Andrew Gordon. 2020. {``The Case for Bayesian Deep Learning.''}
\url{https://arxiv.org/abs/2001.10995}.

\bibitem[\citeproctext]{ref-wu2021polyjuice}
Wu, Tongshuang, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld.
2021. {``Polyjuice: Generating Counterfactuals for Explaining,
Evaluating, and Improving Models.''} In \emph{Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers)}, edited by Chengqing Zong, Fei Xia, Wenjie Li,
and Roberto Navigli, 6707--23. Online: Association for Computational
Linguistics. \url{https://doi.org/10.18653/v1/2021.acl-long.523}.

\end{CSLReferences}

\newpage{}

\FloatBarrier

\renewcommand{\thesection}{\Alph{section}}

\renewcommand{\thetable}{A\arabic{table}}

\renewcommand{\thefigure}{A\arabic{figure}}

\section{Notation}\label{notation}

\begin{itemize}
\tightlist
\item
  \(y^+\): The target class and also the index of the target class.
\item
  \(y^-\): The non-target class and also the index of non-the target
  class.
\item
  \(\mathbf{y}^+\): The one-hot encoded output vector for the target
  class.
\item
  \(\theta\): Model parameters (unspecified).
\item
  \(\Theta\): Matrix of parameters.
\end{itemize}

\section{Technical Details of Our
Approach}\label{technical-details-of-our-approach}

\subsection{Generating Counterfactuals through Gradient
Descent}\label{sec-app-ce}

In this section, we provide some background on gradient-based
counterfactual generators (Section~\ref{sec-app-ce-background}) and
discuss how we define convergence in this context
(Section~\ref{sec-app-conv}).

\subsubsection{Background}\label{sec-app-ce-background}

Gradient-based counterfactual search was originally proposed by Wachter,
Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). It generally solves
the following unconstrained objective,

\[
\begin{aligned}
\min_{\mathbf{z}^\prime \in \mathcal{Z}^L} \left\{  {\text{yloss}(\mathbf{M}_{\theta}(g(\mathbf{z}^\prime)),\mathbf{y}^+)}+ \lambda {\text{cost}(g(\mathbf{z}^\prime)) }  \right\} 
\end{aligned} 
\]

where \(g: \mathcal{Z} \mapsto \mathcal{X}\) is an invertible function
that maps from the \(L\)-dimensional counterfactual state space to the
feature space and \(\text{cost}(\cdot)\) denotes one or more penalties
that are used to induce certain properties of the counterfactual
outcome. As above, \(\mathbf{y}^+\) denotes the target output and
\(\mathbf{M}_{\theta}(\mathbf{x})\) returns the logit predictions of the
underlying classifier for \(\mathbf{x}=g(\mathbf{z})\).

For all generators used in this work we use standard logit crossentropy
loss for \(\text{yloss}(\cdot)\). All generators also penalize the
distance (\(\ell_1\)-norm) of counterfactuals from their original
factual state. For \emph{Generic} and \emph{ECCo}, we have
\(\mathcal{Z}:=\mathcal{X}\) and
\(g(\mathbf{z})=g(\mathbf{z})^{-1}=\mathbf{z}\), that is counterfactual
are searched directly in the feature space. Conversely, \emph{REVISE}
traverses the latent space of a variational autoencoder (VAE) fitted to
the training data, where \(g(\cdot)\) corresponds to the decoder
(\citeproc{ref-joshi2019realistic}{Joshi et al. 2019}). In addition to
the distance penalty, \emph{ECCo} uses an additional penalty component
that regularizes the energy associated with the counterfactual,
\(\mathbf{x}^\prime\) (\citeproc{ref-altmeyer2024faithful}{Altmeyer et
al. 2024}).

\subsubsection{Convergence}\label{sec-app-conv}

An important consideration when generating counterfactual explanations
using gradient-based methods is how to define convergence. Two common
choices are to 1) perform gradient descent over a fixed number of
iterations \(T\), or 2) conclude the search as soon as the predicted
probability for the target class has reached a pre-determined threshold,
\(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\). We
prefer the latter for our purposes, because it explicitly defines
convergence in terms of the black-box model, \(\mathbf{M}(\mathbf{x})\).

Defining convergence in this way allows for a more intuitive
interpretation of the resulting counterfactual outcomes than with fixed
\(T\). Specifically, it allows us to think of counterfactuals as
explaining `high-confidence' predictions by the model for the target
class \(y^+\). Depending on the context and application, different
choices of \(\tau\) can be considered as representing `high-confidence'
predictions.

\subsection{Protecting Mutability Constraints with Linear
Classifiers}\label{sec-app-constraints}

In Section~\ref{sec-constraints} we explain that to avoid penalizing
implausibility that arises due to mutability constraints, we impose a
point mass prior on \(p(\mathbf{x})\) for the corresponding feature. We
argue in Section~\ref{sec-constraints} that this approach induces models
to be less sensitive to immutable features and demonstrate this
empirically in Section~\ref{sec-experiments}. Below we derive the
analytical results in Proposition~\ref{prp-mtblty}.

\begin{proof}
Let \(d_{\text{mtbl}}\) and \(d_{\text{immtbl}}\) denote some mutable
and immutable feature, respectively. Suppose that
\(\mu_{y^-,d_{\text{immtbl}}} < \mu_{y^+,d_{\text{immtbl}}}\) and
\(\mu_{y^-,d_{\text{mtbl}}} > \mu_{y^+,d_{\text{mtbl}}}\), where
\(\mu_{k,d}\) denotes the conditional sample mean of feature \(d\) in
class \(k\). In words, we assume that the immutable feature tends to
take lower values for samples in the non-target class \(y^-\) than in
the target class \(y^+\). We assume the opposite to hold for the mutable
feature.

Assuming multivariate Gaussian class densities with common diagonal
covariance matrix \(\Sigma_k=\Sigma\) for all \(k \in \mathcal{K}\), we
have for the log likelihood ratio between any two classes
\(k,m \in \mathcal{K}\) (\citeproc{ref-hastie2009elements}{Hastie,
Tibshirani, and Friedman 2009}):

\begin{equation}\phantomsection\label{eq-loglike}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})}=\mathbf{x}^\intercal \Sigma^{-1}(\mu_{k}-\mu_{m})  + \text{const}
}\end{equation}

By independence of \(x_1,...,x_D\), the full log-likelihood ratio
decomposes into:

\begin{equation}\phantomsection\label{eq-loglike-decomp}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})} = \sum_{d=1}^D \frac{\mu_{k,d}-\mu_{m,d}}{\sigma_{d}^2} x_{d} + \text{const}
}\end{equation}

By the properties of our classifier (\emph{multinomial logistic
regression}), we have:

\begin{equation}\phantomsection\label{eq-multi}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})} = \sum_{d=1}^D \left( \theta_{k,d} - \theta_{m,d} \right)x_d + \text{const}
}\end{equation}

where \(\theta_{k,d}=\Theta[k,d]\) denotes the coefficient on feature
\(d\) for class \(k\).

Based on Equation~\ref{eq-loglike-decomp} and Equation~\ref{eq-multi} we
can identify that
\((\mu_{k,d}-\mu_{m,d}) \propto (\theta_{k,d} - \theta_{m,d})\) under
the assumptions we made above. Hence, we have that
\((\theta_{y^-,d_{\text{immtbl}}} - \theta_{y^+,d_{\text{immtbl}}}) < 0\)
and
\((\theta_{y^-,d_{\text{mtbl}}} - \theta_{y^+,d_{\text{mtbl}}}) > 0\)

Let \(\mathbf{x}^\prime\) denote some randomly chosen individual from
class \(y^-\) and let \(y^+ \sim p(y)\) denote the randomly chosen
target class. Then the partial derivative of the contrastive divergence
penalty Equation~\ref{eq-div} with respect to coefficient
\(\theta_{y^+,d}\) is equal to

\begin{equation}\phantomsection\label{eq-grad}{
\frac{\partial}{\partial\theta_{y^+,d}} \left(\text{div}(\mathbf{x},\mathbf{x^\prime},\mathbf{y};\theta)\right) = \frac{\partial}{\partial\theta_{y^+,d}} \left( \left(-\mathbf{M}_\theta(\mathbf{x})[y^+]\right) - \left(-\mathbf{M}_\theta(\mathbf{x}^\prime)[y^+]\right) \right) = x_{d}^\prime - x_{d}
}\end{equation}

and equal to zero everywhere else.

Since \((\mu_{y^-,d_{\text{immtbl}}} < \mu_{y^+,d_{\text{immtbl}}})\) we
are more likely to have
\((x_{d_{\text{immtbl}}}^\prime - x_{d_{\text{immtbl}}}) < 0\) than vice
versa at initialization. Similarly, we are more likely to have
\((x_{d_{\text{mtbl}}}^\prime - x_{d_{\text{mtbl}}}) > 0\) since
\((\mu_{y^-,d_{\text{mtbl}}} > \mu_{y^+,d_{\text{mtbl}}})\).

This implies that if we do not protect feature \(d_{\text{immtbl}}\),
the contrastive divergence penalty will decrease
\(\theta_{y^-,d_{\text{immtbl}}}\) thereby exacerbating the existing
effect
\((\theta_{y^-,d_{\text{immtbl}}} - \theta_{y^+,d_{\text{immtbl}}}) < 0\).
In words, not protecting the immutable feature would have the
undesirable effect of making the classifier more sensitive to this
feature, in that it would be more likely to predict class \(y^-\) as
opposed to \(y^+\) for lower values of \(d_{\text{immtbl}}\).

By the same rationale, the contrastive divergence penalty can generally
be expected to increase \(\theta_{y^-,d_{\text{mtbl}}}\) exacerbating
\((\theta_{y^-,d_{\text{mtbl}}} - \theta_{y^+,d_{\text{mtbl}}}) > 0\).
In words, this has the effect of making the classifier more sensitive to
the mutable feature, in that it would be more likely to predict class
\(y^-\) as opposed to \(y^+\) for higher values of \(d_{\text{mtbl}}\).

Thus, our proposed approach of protecting feature \(d_{\text{immtbl}}\)
has the net affect of decreasing the classifier's sensitivity to the
immutable feature relative to the mutable feature (i.e.~no change in
sensitivity for \(d_{\text{immtbl}}\) relative to increased sensitivity
for \(d_{\text{mtbl}}\)).
\end{proof}

\subsection{Domain Constraints}\label{domain-constraints}

We apply domain constraints on counterfactuals during training and
evaluation. There are at least two good reasons for doing so. Firstly,
within the context of explainability and algorithmic recourse,
real-world attributes are often domain constrained: the \emph{age}
feature, for example, is lower bounded by zero and upper bounded by the
maximum human lifespan. Secondly, domain constraints help mitigate
training instabilities commonly associated with energy-based modelling
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020};
\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).

For our image datasets, features are pixel values and hence the domain
is constrained by the lower and upper bound of values that pixels can
take depending on how they are scaled (in our case \([-1,1]\)). For all
other features \(d\) in our synthetic and tabular datasets, we
automatically infer domain constraints
\([x_d^{\text{LB}},x_d^{\text{UB}}]\) as follows,

\begin{equation}\phantomsection\label{eq-domain}{
\begin{aligned}
x_d^{\text{LB}} &= \arg\min_{x_d} \{\mu_d - n_{\sigma_d}\sigma_d, \arg \min_{x_d} x_d\} \\
x_d^{\text{UB}} &= \arg\max_{x_d} \{\mu_d + n_{\sigma_d}\sigma_d, \arg \max_{x_d} x_d\} 
\end{aligned}
}\end{equation}

where \(\mu_d\) and \(\sigma_d\) denote the sample mean and standard
deviation of feature \(d\). We set \(n_{\sigma_d}=3\) across the board
but higher values and hence wider bounds may be appropriate depending on
the application.

\subsection{Training Details}\label{sec-app-training}

In this section, we describe the training procedure in detail. While the
details laid out here are not crucial for understanding our proposed
approach, they are of importance to anyone looking to implement
counterfactual training.

\section{Detailed Results}\label{detailed-results}

\subsection{Qualitative Findings for Image
Data}\label{qualitative-findings-for-image-data}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note}, rightrule=.15mm, titlerule=0mm]

Figure~\ref{fig-mnist} shows much more plausible (faithful)
counterfactuals for a model with CT than the model with conventional
training (Figure~\ref{fig-mnist-vanilla}). In fact, this is not even
using \emph{ECCo+} and still showing better results than the best
results we achieved in our AAAI paper for JEM ensembles.

\end{tcolorbox}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_mlp.png}}

}

\caption{\label{fig-mnist}Counterfactual images for \emph{MLP} with
counterfactual training. The underlying generator, \emph{ECCo}, aims to
generate counterfactuals that are faithful to the model
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_mlp_vanilla.png}}

}

\caption{\label{fig-mnist-vanilla}Counterfactual images for \emph{MLP}
with conventional training. The underlying generator, \emph{ECCo}, aims
to generate counterfactuals that are faithful to the model
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).}

\end{figure}%

\subsection{Grid Searches}\label{sec-app-grid}

To assess the hyperparameter sensitivity of our proposed training regime
we ran multiple large grid searches for all of our synthetic datasets.
We have grouped these grid searches into multiple categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Generator Parameters} (Section~\ref{sec-app-grid-gen}):
  Investigates the effect of changing hyperparameters that affect the
  counterfactual outcomes during the training phase.
\item
  \textbf{Penalty Strengths} (Section~\ref{sec-app-grid-pen}):
  Investigates the effect of changing the penalty strengths in out
  proposed objective (Equation~\ref{eq-obj}).
\item
  \textbf{Other Parameters} (Section~\ref{sec-app-grid-train}):
  Investigates the effect of changing other training parameters,
  including the total number of generated counterfactuals in each epoch.
\end{enumerate}

We begin by summarizing the high-level findings in
Section~\ref{sec-app-grid-hl}. For each of the categories,
Section~\ref{sec-app-grid-gen} to Section~\ref{sec-app-grid-train} then
present all details including the parameter grids, average performance
outcomes and key evaluation metrics for the generated counterfactuals.

\subsubsection{High-Level Findings}\label{sec-app-grid-hl}

Overall, we find that Counterfactual Training (CT) achieves it key
objectives consistently across all hyperparameter settings and also
broadly across datasets. We do observe strong sensitivity to certain
hyperparameters, but clear and manageable patterns emerge in those
cases. With respect to the underlying training data distribution, we
find that outcomes for CT seem to depend to some degree on class
separability.

\paragraph{Predictive Performance}\label{predictive-performance}

We find that CT is associated with little to no decrease in average
predictive performance for our synthetic datasets: test accuracy and
F1-scores decrease by at most \textasciitilde1 percentage point, but
generally much less (Table~\ref{tbl-acc-gen}, Table~\ref{tbl-acc-pen},
Table~\ref{tbl-acc-train}). Variation across hyperparameters is
negligible as indicated by small standard deviations for these metrics
across the board.

\paragraph{Key Counterfactual
Outcomes}\label{key-counterfactual-outcomes}

\subsubsection{Generator Parameters}\label{sec-app-grid-gen}

The hyperparameter grid with varying generator parameters during
training is shown in Note~\ref{nte-gen-params-final-run-train}. The
corresponding evaluation grid used for these experiments is shown in
Note~\ref{nte-gen-params-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-gen-params-final-run-train}: Training Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-gen-params-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    Decision Threshold: \texttt{0.75,\ 0.9,\ 0.95}
  \item
    \(\lambda_{\text{energy}}\): \texttt{0.1,\ 0.5,\ 5.0,\ 10.0,\ 20.0}
  \item
    Maximum Iterations: \texttt{5,\ 25,\ 50}
  \end{itemize}
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-gen-params-final-run-eval}: Evaluation Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-gen-params-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{energy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\paragraph{Accuracy}\label{accuracy}

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-gen}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-gen-params-final-run-train}) and evaluation-phase
parameters (Note~\ref{nte-gen-params-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circles & Accuracy & Full & 0.997 & 0.00309 \\
  Circles & Accuracy & Vanilla & 0.998 & 0.000557 \\
  Circles & F1-score & Full & 0.997 & 0.00309 \\
  Circles & F1-score & Vanilla & 0.998 & 0.000558 \\
  Lin Sep & Accuracy & Full & 0.999 & 0.00201 \\
  Lin Sep & Accuracy & Vanilla & 1 & 0 \\
  Lin Sep & F1-score & Full & 0.999 & 0.00201 \\
  Lin Sep & F1-score & Vanilla & 1 & 0 \\
  Moons & Accuracy & Full & 0.999 & 0.000696 \\
  Moons & Accuracy & Vanilla & 1 & 0.00111 \\
  Moons & F1-score & Full & 0.999 & 0.000696 \\
  Moons & F1-score & Vanilla & 1 & 0.00111 \\
  Over & Accuracy & Full & 0.915 & 0.00477 \\
  Over & Accuracy & Vanilla & 0.917 & 0.00123 \\
  Over & F1-score & Full & 0.915 & 0.00478 \\
  Over & F1-score & Vanilla & 0.917 & 0.00124 \\\bottomrule

\end{longtable}

\paragraph{Plausibility}\label{plausibility-1}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-gen_params-plaus-circles} to
Figure~\ref{fig-grid-gen_params-plaus-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-gen_params-plaus-circles}Average outcomes for
the plausibility measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-gen_params-plaus-lin_sep}Average outcomes for
the plausibility measure across hyperparameters. Data: Linearly
Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-gen_params-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-gen_params-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\paragraph{Cost}\label{cost}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-gen_params-cost-circles} to
Figure~\ref{fig-grid-gen_params-cost-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}}

}

\caption{\label{fig-grid-gen_params-cost-circles}Average outcomes for
the cost measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}}

}

\caption{\label{fig-grid-gen_params-cost-lin_sep}Average outcomes for
the cost measure across hyperparameters. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}}

}

\caption{\label{fig-grid-gen_params-cost-moons}Average outcomes for the
cost measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}}

}

\caption{\label{fig-grid-gen_params-cost-over}Average outcomes for the
cost measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\subsubsection{Penalty Strengths}\label{sec-app-grid-pen}

The hyperparameter grid with varying penalty strengths during training
is shown in Note~\ref{nte-pen-final-run-train}. The corresponding
evaluation grid used for these experiments is shown in
Note~\ref{nte-pen-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-pen-final-run-train}: Training Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-pen-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{adv}}\): \texttt{0.1,\ 0.25,\ 1.0}
  \item
    \(\lambda_{\text{div}}\): \texttt{0.01,\ 0.1,\ 1.0}
  \item
    \(\lambda_{\text{reg}}\): \texttt{0.0,\ 0.01,\ 0.1,\ 0.25,\ 0.5}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-pen-final-run-eval}: Evaluation Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-pen-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{energy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\paragraph{Accuracy}\label{accuracy-1}

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-pen}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-pen-final-run-train}) and evaluation-phase parameters
(Note~\ref{nte-pen-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circles & Accuracy & Full & 0.994 & 0.0144 \\
  Circles & Accuracy & Vanilla & 0.998 & 0.000875 \\
  Circles & F1-score & Full & 0.994 & 0.0145 \\
  Circles & F1-score & Vanilla & 0.998 & 0.000875 \\
  Lin Sep & Accuracy & Full & 0.998 & 0.00772 \\
  Lin Sep & Accuracy & Vanilla & 1 & 0 \\
  Lin Sep & F1-score & Full & 0.998 & 0.00773 \\
  Lin Sep & F1-score & Vanilla & 1 & 0 \\
  Moons & Accuracy & Full & 0.987 & 0.0351 \\
  Moons & Accuracy & Vanilla & 0.998 & 0.0101 \\
  Moons & F1-score & Full & 0.987 & 0.0352 \\
  Moons & F1-score & Vanilla & 0.998 & 0.0102 \\
  Over & Accuracy & Full & 0.911 & 0.0217 \\
  Over & Accuracy & Vanilla & 0.916 & 0.00236 \\
  Over & F1-score & Full & 0.911 & 0.0219 \\
  Over & F1-score & Vanilla & 0.916 & 0.00236 \\\bottomrule

\end{longtable}

\paragraph{Plausibility}\label{plausibility-2}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-pen-plaus-circles} to
Figure~\ref{fig-grid-pen-plaus-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-pen-plaus-circles}Average outcomes for the
plausibility measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-pen-plaus-lin_sep}Average outcomes for the
plausibility measure across hyperparameters. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-pen-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-pen-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\paragraph{Cost}\label{cost-1}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-pen-cost-circles} to
Figure~\ref{fig-grid-pen-cost-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}}

}

\caption{\label{fig-grid-pen-cost-circles}Average outcomes for the cost
measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}}

}

\caption{\label{fig-grid-pen-cost-lin_sep}Average outcomes for the cost
measure across hyperparameters. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}}

}

\caption{\label{fig-grid-pen-cost-moons}Average outcomes for the cost
measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}}

}

\caption{\label{fig-grid-pen-cost-over}Average outcomes for the cost
measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\subsubsection{Other Parameters}\label{sec-app-grid-train}

The hyperparameter grid with other varying training parameters is shown
in Note~\ref{nte-train-final-run-train}. The corresponding evaluation
grid used for these experiments is shown in
Note~\ref{nte-train-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-train-final-run-train}: Training Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-train-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    Burnin: \texttt{0.0,\ 0.5}
  \item
    No.~Counterfactuals: \texttt{100,\ 1000}
  \item
    No.~Epochs: \texttt{50,\ 100}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colframe=quarto-callout-note-color-frame, toptitle=1mm, breakable, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, coltitle=black, bottomtitle=1mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, arc=.35mm, title={Note \ref*{nte-train-final-run-eval}: Evaluation Phase}, rightrule=.15mm, titlerule=0mm]

\quartocalloutnte{nte-train-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{energy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\paragraph{Accuracy}\label{accuracy-2}

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-train}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-train-final-run-train}) and evaluation-phase parameters
(Note~\ref{nte-train-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circles & Accuracy & Full & 0.995 & 0.00431 \\
  Circles & Accuracy & Vanilla & 0.998 & 0.000566 \\
  Circles & F1-score & Full & 0.995 & 0.00432 \\
  Circles & F1-score & Vanilla & 0.998 & 0.000566 \\
  Lin Sep & Accuracy & Full & 0.999 & 0.00231 \\
  Lin Sep & Accuracy & Vanilla & 1 & 0 \\
  Lin Sep & F1-score & Full & 0.999 & 0.00231 \\
  Lin Sep & F1-score & Vanilla & 1 & 0 \\
  Moons & Accuracy & Full & 0.996 & 0.0136 \\
  Moons & Accuracy & Vanilla & 0.988 & 0.022 \\
  Moons & F1-score & Full & 0.996 & 0.0136 \\
  Moons & F1-score & Vanilla & 0.988 & 0.022 \\
  Over & Accuracy & Full & 0.914 & 0.00563 \\
  Over & Accuracy & Vanilla & 0.918 & 0.00116 \\
  Over & F1-score & Full & 0.914 & 0.0057 \\
  Over & F1-score & Vanilla & 0.918 & 0.00116 \\\bottomrule

\end{longtable}

\paragraph{Plausibility}\label{plausibility-3}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-train-plaus-circles} to
Figure~\ref{fig-grid-train-plaus-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-train-plaus-circles}Average outcomes for the
plausibility measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-train-plaus-lin_sep}Average outcomes for the
plausibility measure across hyperparameters. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-train-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}}

}

\caption{\label{fig-grid-train-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\paragraph{Cost}\label{cost-2}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-train-cost-circles} to
Figure~\ref{fig-grid-train-cost-over}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}}

}

\caption{\label{fig-grid-train-cost-circles}Average outcomes for the
cost measure across hyperparameters. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}}

}

\caption{\label{fig-grid-train-cost-lin_sep}Average outcomes for the
cost measure across hyperparameters. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}}

}

\caption{\label{fig-grid-train-cost-moons}Average outcomes for the cost
measure across hyperparameters. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}}

}

\caption{\label{fig-grid-train-cost-over}Average outcomes for the cost
measure across hyperparameters. Data: Overlapping.}

\end{figure}%

\section{Computation Details}\label{computation-details}

\subsection{Hardware}\label{hardware}

We performed our experiments on a high-performance cluster. Details
about the cluster will be disclosed upon publication to avoid revealing
information that might interfere with the double-blind review process.
Since our experiments involve highly parallel tasks and rather small
models by today's standard, we have relied on distributed computing
across multiple central processing units (CPU). Graphical processing
units (GPU) were not required. Model training for the largest grid
searches with 270 unique parameter combinations was parallelized across
34 CPUs with 2GB memory each. The time to completion varied by dataset
for reasons discussed in Section~\ref{sec-discussion}: 0h49m
(\emph{Moons}), 1h4m (\emph{Linearly Separable}), 1h49m
(\emph{Circles}), 3h52m (\emph{Overlapping}). Model evaluations for
large grid searches were parallelized across 20 CPUs with 3GB memory
each. Evaluations for all data sets took less than one hour
(\textless1h) to complete.

\subsection{Software}\label{software}

All computations were performed in the Julia Programming Language
(\citeproc{ref-bezanson2017julia}{Bezanson et al. 2017}). We have
developed a package for counterfactual training that leverages and
extends the functionality provided by several existing packages, most
notably
\href{https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl}{CounterfactualExplanations.jl}
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, et al. 2023})
and the \href{https://fluxml.ai/Flux.jl/v0.16/}{Flux.jl} library for
deep learning (\citeproc{ref-innes2018fashionable}{Michael Innes et al.
2018}; \citeproc{ref-innes2018flux}{Mike Innes 2018}). For
data-wrangling and presentation-ready tables we relied on
\href{https://dataframes.juliadata.org/v1.7/}{DataFrames.jl}
(\citeproc{ref-milan2023dataframes}{Bouchet-Valat and Kamiński 2023})
and
\href{https://ronisbr.github.io/PrettyTables.jl/v2.4/}{PrettyTables.jl}
(\citeproc{ref-chagas2024pretty}{Chagas et al. 2024}), respectively. For
plots and visualizations we used both
\href{https://docs.juliaplots.org/v1.40/}{Plots.jl}
(\citeproc{ref-PlotsJL}{Christ et al. 2023}) and
\href{https://docs.makie.org/v0.22/}{Makie.jl}
(\citeproc{ref-danisch2021makie}{Danisch and Krumbiegel 2021}), in
particular \href{https://aog.makie.org/v0.9.3/}{AlgebraOfGraphics.jl}.
To distribute computational tasks across multiple processors, we have
relied on \href{https://juliaparallel.org/MPI.jl/v0.20/}{MPI.jl}
(\citeproc{ref-byrne2021mpi}{Byrne, Wilcox, and Churavy 2021}).




\end{document}
