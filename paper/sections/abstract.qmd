# Abstract

We propose a novel model objective that leverages counterfactual explanations to increase the explanatory capacity of trained models. Counterfactual explanations have emerged as a popular tool to explain predictions made by opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. Much existing research has focused on generating counterfactuals that are not only valid but also deemed acceptable with respect to the stakeholder requirements and plausible with respect to the underlying data. Recent work has shown that under this premise, the task of learning plausible explanations---ones that respect the conditional distribution of the target class---is effectively reassigned from the model itself to the post-hoc counterfactual explainer. Building on that work, we propose a novel objective function that employs counterfactuals (ad-hoc) during the training phase to minimize the divergence between the learned representations and the explanations. Through extensive experiments, we demonstrate that our proposed method facilitates training models that deliver inherently plausible explanations while maintaining high predictive performance.