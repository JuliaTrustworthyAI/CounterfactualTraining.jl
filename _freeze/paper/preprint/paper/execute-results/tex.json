{
  "hash": "a8223ddf51e2a9e9ebe8fff23a588627",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  arxiv-pdf:\n    keep-tex: true  \n    linenumbers: true\n    doublespacing: false\n    runninghead: \"Counterfactual Training (A Preprint)\"\n    authorcols: true\n---\n\n\n# Abstract\n\nCounterfactual Explanations (CE) have emerged as a popular tool to explain predictions made by opaque machine learning models: they explain how factual inputs need to change in order for some fitted model to produce some desired output. Much existing research has focused on identifying explanations that are not only valid but also deemed desirable with respect to the underlying data and stakeholder requirements. Recent work has shown that under this premise, the task of learning desirable explanations is effectively reassigned from the model itself to the (post-hoc) counterfactual explainer. Building on that work, we propose a novel model objective that leverages counterfactuals during the training phase (ad-hoc) in order to minimize the divergence between learned representations and desirable explanations. Through extensive experiments, we demonstrate that our proposed methodology facilitates training models that inherently deliver desirable explanations while maintaining high predictive performance. \n\n\n\n# Introduction\n\nToday's prominence of artificial intelligence (AI) has largely been driven by advances in **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern AIs are tasked with learning these representations from scratch, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such AIs with ever greater degrees of freedom to achieve that task, which has often led them to outperform traditionally more parsimonious models. Unfortunately, in doing so they also learn increasingly complex and highly sensitive representations that we can no longer easily interpret.\n\nThis trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @goodfellow2014explaining showed that artificial neural networks (ANN) are sensitive to adversarial examples (AE): counterfactuals of model inputs that yield vastly different model predictions despite being semantically indifferent from their factual counterparts. Despite partially effective mitigation strategies such as **adversarial training**, truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. \n\nPart of the problem is that high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case]. Based purely on predictive performance, these solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two related challenges: firstly, it makes these models inherently opaque, since humans cannot simply interpret what type of explanation the complex learned representations correspond to; secondly, even if we could resolve the first challenge, it is not obvious how to mitigate models from learning representations that correspond to meaningless and undesirable explanations. \n\nThe first challenge has attracted an abundance of research on **explainable AI** (XAI) which aims to develop tools to derive explanations from complex model representations. This can mitigate a scenario in which we deploy opaque models and blindly rely on their predictions. On countless occasions, this scenario has already occurred in practice and caused real harm to people who were affected adversely and often unfairly by automated decision-making systems involving opaque models [@oneil2016weapons]. Effective XAI tools can aide us in monitoring models and providing recourse to affected individuals [@wachter2017counterfactual].\n\nTo our surprise, the second challenge has not yet attracted any consolidated research effort. Specifically, there has been no concerted effort towards improving model **explainability**, which we define here as the degree to which learned representations correspond to explanations that are deemed desirable by humans. Instead, the choice has typically been to improve the capacity of XAI tools to identify the subset explanations that are both desirable and valid for any given model, independent of whether the learned representations are also compatible with undesirable explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that explainability can arise as byproduct of regularization techniques aimed at other objectives such as robustness, generalization and generative capacity [@schut2021generating, @augustin2020adversarial, @altmeyer2024faithful]. \n\nBuilding on these findings, we introduce **counterfactual training**: a novel regularization technique geared explicitly towards aligning model representations with desirable explanations. Our contributions are as follows:\n\n- We discuss existing related work on improving models and consolidate it through the lens of counterfactual explanations (@sec-lit).\n- We present our proposed methodological framework that leverages faithful counterfactual explanations during the training phase of models to achieve the explainability objective (@sec-method).\n- Through extensive experiments we demonstrate the counterfactual training improve model explainability while maintaining high predictive performance. We run ablation studies and grid searches to understand how the underlying model components and hyperparameters affect outcomes. (@sec-experiments). \n\nDespite limitations of our approach discussed in @sec-discussion, we conclude that counterfactual training provides a practical framework for researchers and practitioners interested in making opaque models more trustworthy [@sec-conclusion]. We also believe that this work serves as an opportunity for XAI researchers to reevaluate the premise of improving XAI tools without improving models. \n\n\n\n\n# Related Literature {#sec-lit}\n\n## Background on Counterfactual Explanations\n\n[@wachter2017counterfactual;@joshi2019realistic;@altmeyer2024faithful]\n\n## Learning Representations\n\n> For example, joint-energy models\n\n## Generalization and Robustness\n\n@sauer2021counterfactual generate counterfactual images for MNIST and ImageNet through independent mechanisms (IM): each IM learns class-conditional input distributions over a specific lower-dimensional, semantically meaningful factor, such as *texture*, *shape* and *background*. They demonstrate that using these generated counterfactuals during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, demonstrate the augmenting the training data of image classifiers can improve generalization. \n\n@teney2020learning propose an approach using counterfactuals in training that does not rely on data augmentation: they argue that counterfactual pairs typically already exist in training datasets. Specifically, their approach relies on, firstly, identifying similar input samples with different annotations and, secondly, ensuring that the gradient of the classifier aligns with the vector between pairs of counterfactual inputs using the cosine distance as a loss function (referred to as *gradient supervision*) (***this might be useful for our task as well***). In the natural language processing (NLP) domain, counterfactuals have similarly been used to improve models through data augmentation: @wu2021polyjuice, propose POLYJUICE, a general-purpose counterfactual generator for language models. They demonstrate empirically that augmenting training data through POLYJUICE counterfactuals improves robustness in a number of NLP tasks. \n\n## Link to Adversarial Training\n\nFrom this perspective, adversarial training induces models to \"unlearn\" representations that are susceptible to the semantically most meaningless explanations---adversarial examples. \n\n@freiesleben2022intriguing propose two definitional differences between Adversarial Examples (AE) and Counterfactual Explanations (CE): firstly, and more importantly according to the authors, the term AE implies missclassification, which is not the case for CE (***this might be a useful notion for use to distinguish between adversarials and explanations during training***); secondly, they argue that closeness plays a more critical role in the context of CE but confess that even counterfactuals that are not close might be relevant explanations. @pawelczyk2022exploring show that CE and AE are equivalent under certain conditions and derive upper bounds on the distances between them. \n\n## Closely Related\n\n@guo2023counternet are the first to propose end-to-end training pipeline that includes counterfactual explanations as part of the training prodeduce. In particular, they propose a specific network architecture that includes a predictor and CE generator network (***akin a GAN?***), where the parameters of the CE generator network are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor network (***here we are aligned***). In contrast, we impose no restrictions on the neural network architecture at all. (***to ensure the one-hot encoding of categorical features is maintained, they simple use softmax (might be interesting for CE.jl)***) Interestingly, the authors find that their approach is sensitive to the choice of the loss function: only MSE seems to lead to good performance. They also demonstrate theoretically, that the objective function is difficult to optimize due to divergent gradients and suffers from poor adversarial robustness. (***because partial gradients with respect to the classification loss component and the counterfactual validity component point in opposite directions***). To mitigate these issues, the authors use block-wise gradient descent: they first update with respect to classification loss and then use a second update with respect to the other loss components (***this might be useful for our task as well***). @ross2021learning propose a way to train models that are guaranteed to provide recourse for individuals with high probability. The approach builds on adversarial training (***here we are aligned***), where in this context adversarial examples are actively encouraged to exist, but only target attacks with respect to the positive class. The proposed method allows for imposing a set of actionable recourse ex-ante: for example, users can impose mutability constraints for features (***here we are aligned***). (***To solve their objective function more efficiently, they use a first-order Taylor approximation to approximate the recourse loss component (might be applicable in our case)***)\n\n@luu2023counterfactual introduce Counterfactual Adversarial Training (CAT) with intention of improving generalization and robustness of language models. Specifically, they propose to proceed as follows: firstly, identify training samples that are subject to high predictive uncertainty (entropy); secondly, generate counterfactual explanations for those samples; and, finally, finetune the model on the augmented dataset that includes the generated counterfactuals.\n\n\n\n# Counterfactual Training {#sec-method}\n\n\n\n# Experiments {#sec-experiments}\n\n## Experimental Setup\n\n## Experimental Results\n\n\n\n# Discussion {#sec-discussion}\n\n\n\n# Conclusion {#sec-conclusion}\n\n\n\n\n\n# References {-}\n\n::: {#refs}\n:::\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\\FloatBarrier\n\n\\setcounter{section}{0}\n\\renewcommand{\\thesection}{\\Alph{section}}\n\n\\setcounter{table}{0}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\n\\setcounter{figure}{0}\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\n<!-- # Supplementary Material {.appendix} -->\n\n# Training Details {.appendix} \n\n## Initial Grid Search\n\n\n\n\n\n\nFor the initial round of experiments we \n\n### Generator Parameters\n\nThe hyperparameter grids for the first investigation of the effect of generator parameters are shown in @exr-gen-params-first-run-train and @exr-gen-params-first-run-eval.\n\n::: {#exr-gen-params-first-run-train}\n\n## Training Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{cost}}$: `0.0, 0.001, 0.1`\n    - $\\lambda_{\\text{div}}$: `0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 15.0`\n    - Learning Rate: `1.0`\n    - Maximum Iterations: `20, 50, 100`\n    - Optimizerimizer: `sgd`\n- Generator: `ecco, generic, omni, revise`\n- Training Parameters:\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n\n::: {#exr-gen-params-first-run-eval}\n\n## Evaluation Phase\n\n\n- Counterfactual Parameters:\n    - Convergence: `max_iter`\n    - Maximum Iterations: `100`\n    - No. Individuals: `100`\n    - No. Runs: `5`\n- Generator Parameters:\n    - $\\lambda_{\\text{cost}}$: `0.0`\n    - $\\lambda_{\\text{div}}$: `0.1, 0.5, 1.0, 5.0, 10.0, 20.0`\n    - Learning Rate: `1.0`\n    - Maximum Iterations: `50`\n    - Optimizerimizer: `sgd`\n\n\n\n\n\n:::\n\n\n\n\n\n\n#### Linearly Separable \n\n- **Energy Penalty** (@tbl-lin_sep-lambda_energy_exper): *ECCo* generally does yield better results than *Vanilla* for higher choices of the energy penalty (10,15) during training. *Generic* performs poorly accross the board. *Omni* seems to have an anchoring effect, in that it never performs terribly but also never as good as the best *ECCo* results. *REVISE* performs poorly across the board.\n- **Cost** (@tbl-lin_sep-lambda_cost_exper): Results for all generators (except *Omni*) are quite bad, which can likely be attributed to extremely bad results for some choices of the **Energy Penalty** (results here are averaged). For *ECCo* and *Generic*, higher cost values generally lead to worse results.\n- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n- **Validity**: *ECCo* almost always valid except for very low values during training and high values at evaluation time. *Generic* often has poor validity.\n- **Accuracy**: Seems largely unaffected.\n\n\n\n\n\n\n::: {#tbl-lin_sep-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  full & 0.01 & \\textit{ECCo} & $-9.91 \\cdot 10^{11}$ & $2.25 \\cdot 10^{12}$ \\\\\n  full & 0.01 & \\textit{Generic} & $-5.71 \\cdot 10^{17}$ & $1.3 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.54}} & \\color{blue}{\\textbf{0.116}} \\\\\n  full & 0.01 & \\textit{REVISE} & -15.6 & 13.2 \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -4.28 & 3.52 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -4.45 & 3.47 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -4.91 & 4.24 \\\\\n  full & 0.05 & \\textit{ECCo} & $-5.63 \\cdot 10^{5}$ & $1.28 \\cdot 10^{6}$ \\\\\n  full & 0.05 & \\textit{Generic} & $-8.35 \\cdot 10^{17}$ & $1.9 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.53}} & \\color{blue}{\\textbf{0.114}} \\\\\n  full & 0.05 & \\textit{REVISE} & -15 & 12.6 \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -4.4 & 3.66 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -5.25 & 4.62 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -4.94 & 4.22 \\\\\n  full & 0.1 & \\textit{ECCo} & $-6.74 \\cdot 10^{5}$ & $1.53 \\cdot 10^{6}$ \\\\\n  full & 0.1 & \\textit{Generic} & $-1.72 \\cdot 10^{11}$ & $3.9 \\cdot 10^{11}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.56}} & \\color{blue}{\\textbf{0.124}} \\\\\n  full & 0.1 & \\textit{REVISE} & -15.6 & 13.2 \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -4.28 & 3.52 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -4.45 & 3.48 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -4.91 & 4.25 \\\\\n  full & 0.5 & \\textit{ECCo} & -11.8 & 9.83 \\\\\n  full & 0.5 & \\textit{Generic} & $-1.06 \\cdot 10^{18}$ & $2.42 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.54}} & \\color{blue}{\\textbf{0.123}} \\\\\n  full & 0.5 & \\textit{REVISE} & -15 & 12.6 \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -5.25 & 4.61 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -4.95 & 4.22 \\\\\n  full & 1 & \\textit{ECCo} & -11.5 & 11.1 \\\\\n  full & 1 & \\textit{Generic} & $-1.71 \\cdot 10^{11}$ & $3.88 \\cdot 10^{11}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.59}} & \\color{blue}{\\textbf{0.117}} \\\\\n  full & 1 & \\textit{REVISE} & -15.7 & 13.3 \\\\\n  vanilla & 1 & \\textit{ECCo} & -4.28 & 3.51 \\\\\n  vanilla & 1 & \\textit{Generic} & -4.44 & 3.47 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -5.11 & 4.46 \\\\\n  vanilla & 1 & \\textit{REVISE} & -4.91 & 4.25 \\\\\n  full & 5 & \\textit{ECCo} & -3.99 & 3.12 \\\\\n  full & 5 & \\textit{Generic} & $-4.88 \\cdot 10^{17}$ & $1.11 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.53}} & \\color{blue}{\\textbf{0.117}} \\\\\n  full & 5 & \\textit{REVISE} & -14.6 & 12.1 \\\\\n  vanilla & 5 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 5 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -5.25 & 4.61 \\\\\n  vanilla & 5 & \\textit{REVISE} & -4.95 & 4.22 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.31}} & \\color{blue}{\\textbf{0.735}} \\\\\n  full & 10 & \\textit{Generic} & $-1.7 \\cdot 10^{11}$ & $3.86 \\cdot 10^{11}$ \\\\\n  full & 10 & \\textit{Omniscient} & -2.53 & 0.117 \\\\\n  full & 10 & \\textit{REVISE} & -15.5 & 13 \\\\\n  vanilla & 10 & \\textit{ECCo} & -4.28 & 3.51 \\\\\n  vanilla & 10 & \\textit{Generic} & -4.44 & 3.47 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 10 & \\textit{REVISE} & -4.91 & 4.24 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.01}} & \\color{blue}{\\textbf{0.488}} \\\\\n  full & 15 & \\textit{Generic} & $-4.91 \\cdot 10^{17}$ & $1.12 \\cdot 10^{18}$ \\\\\n  full & 15 & \\textit{Omniscient} & -2.53 & 0.116 \\\\\n  full & 15 & \\textit{REVISE} & -14.4 & 11.7 \\\\\n  vanilla & 15 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 15 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -5.25 & 4.6 \\\\\n  vanilla & 15 & \\textit{REVISE} & -4.95 & 4.23 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Linearly Separable data by energy penalty.\n\n:::\n\n<!-- Cost -->\n\n\n\n\n\n\n::: {#tbl-lin_sep-lambda_cost_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{cost}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{cost}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  full & 0 & \\textit{ECCo} & $-5.32 \\cdot 10^{3}$ & $1.21 \\cdot 10^{4}$ \\\\\n  full & 0 & \\textit{Generic} & $-1.03 \\cdot 10^{18}$ & $2.34 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.64}} & \\color{blue}{\\textbf{0.125}} \\\\\n  full & 0 & \\textit{REVISE} & -15.4 & 12.9 \\\\\n  vanilla & 0 & \\textit{ECCo} & -4.34 & 3.58 \\\\\n  vanilla & 0 & \\textit{Generic} & -4.41 & 3.48 \\\\\n  vanilla & 0 & \\textit{Omniscient} & -5.18 & 4.54 \\\\\n  vanilla & 0 & \\textit{REVISE} & -4.93 & 4.23 \\\\\n  full & 0.001 & \\textit{ECCo} & -362 & 811 \\\\\n  full & 0.001 & \\textit{Generic} & $-2.65 \\cdot 10^{17}$ & $6.03 \\cdot 10^{17}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.001}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.49}} & \\color{blue}{\\textbf{0.115}} \\\\\n  full & 0.001 & \\textit{REVISE} & -15.5 & 13 \\\\\n  vanilla & 0.001 & \\textit{ECCo} & -4.34 & 3.58 \\\\\n  vanilla & 0.001 & \\textit{Generic} & -4.41 & 3.48 \\\\\n  vanilla & 0.001 & \\textit{Omniscient} & -5.18 & 4.53 \\\\\n  vanilla & 0.001 & \\textit{REVISE} & -4.93 & 4.23 \\\\\n  full & 0.1 & \\textit{ECCo} & $-3.72 \\cdot 10^{11}$ & $8.46 \\cdot 10^{11}$ \\\\\n  full & 0.1 & \\textit{Generic} & $-4.49 \\cdot 10^{14}$ & $1.02 \\cdot 10^{15}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.5}} & \\color{blue}{\\textbf{0.112}} \\\\\n  full & 0.1 & \\textit{REVISE} & -14.6 & 12.2 \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -4.34 & 3.58 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -4.41 & 3.48 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -5.18 & 4.54 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -4.93 & 4.24 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Linearly Separable data by cost penalty.\n\n:::\n\n\n#### Moons\n\n- **Energy Penalty** (@tbl-moons-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, except for very low choices of the energy penalty during training for which it performs abismal. *Generic* performs quite badly across the board for high enough choices of the energy penalty at evaluation time. *Omni* has small positive effect. *REVISE* performs poorly across the board.\n- **Cost (distance penalty)**: *Generic* generally does better for higher values, while *ECCo* does better for lower values.\n- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n- **Validity**: *ECCo* generally achieves full validity except for very low choices the energy penalty during training and high choices at evaluation time. *Generic* performs poorly for high choices of the energy penalty during evaluation.\n- **Accuracy**: Largely unaffected although *ECCo* suffers a bit for very low choices the energy penalty during training. *REVISE* suffers a lot in general (around 10 percentage points).\n\n\n\n\n\n\n::: {#tbl-moons-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  full & 0.01 & \\textit{ECCo} & $-2.8 \\cdot 10^{22}$ & $6.39 \\cdot 10^{22}$ \\\\\n  full & 0.01 & \\textit{Generic} & $-4.89 \\cdot 10^{30}$ & $1.11 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.74}} & \\color{blue}{\\textbf{5.08}} \\\\\n  full & 0.01 & \\textit{REVISE} & -572 & $1.25 \\cdot 10^{3}$ \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -11.2 & 13 \\\\\n  full & 0.05 & \\textit{ECCo} & $-1.55 \\cdot 10^{16}$ & $3.52 \\cdot 10^{16}$ \\\\\n  full & 0.05 & \\textit{Generic} & $-2.22 \\cdot 10^{20}$ & $5 \\cdot 10^{20}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.41}} & \\color{blue}{\\textbf{4.48}} \\\\\n  full & 0.05 & \\textit{REVISE} & $-1.04 \\cdot 10^{3}$ & $2.3 \\cdot 10^{3}$ \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 0.1 & \\textit{ECCo} & $-3.41 \\cdot 10^{3}$ & $7.73 \\cdot 10^{3}$ \\\\\n  full & 0.1 & \\textit{Generic} & $-5.22 \\cdot 10^{30}$ & $1.19 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.78}} & \\color{blue}{\\textbf{5.12}} \\\\\n  full & 0.1 & \\textit{REVISE} & -288 & 594 \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 0.5 & \\textit{ECCo} & -7.09 & 7.51 \\\\\n  full & 0.5 & \\textit{Generic} & $-1.11 \\cdot 10^{31}$ & $2.53 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.58}} & \\color{blue}{\\textbf{4.83}} \\\\\n  full & 0.5 & \\textit{REVISE} & $-1.19 \\cdot 10^{3}$ & $2.64 \\cdot 10^{3}$ \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 1 & \\textit{ECCo} & -6.06 & 6.33 \\\\\n  full & 1 & \\textit{Generic} & $-1.58 \\cdot 10^{33}$ & $3.59 \\cdot 10^{33}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.66}} & \\color{blue}{\\textbf{4.89}} \\\\\n  full & 1 & \\textit{REVISE} & $-1.16 \\cdot 10^{3}$ & $2.59 \\cdot 10^{3}$ \\\\\n  vanilla & 1 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 1 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 1 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.57}} & \\color{blue}{\\textbf{2.07}} \\\\\n  full & 5 & \\textit{Generic} & $-1.17 \\cdot 10^{28}$ & $2.66 \\cdot 10^{28}$ \\\\\n  full & 5 & \\textit{Omniscient} & -4.29 & 4.31 \\\\\n  full & 5 & \\textit{REVISE} & -530 & $1.16 \\cdot 10^{3}$ \\\\\n  vanilla & 5 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 5 & \\textit{Generic} & -11.7 & 12.7 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 5 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.76}} & \\color{blue}{\\textbf{0.974}} \\\\\n  full & 10 & \\textit{Generic} & $-1.54 \\cdot 10^{33}$ & $3.51 \\cdot 10^{33}$ \\\\\n  full & 10 & \\textit{Omniscient} & -4.44 & 4.56 \\\\\n  full & 10 & \\textit{REVISE} & $-1.52 \\cdot 10^{3}$ & $3.4 \\cdot 10^{3}$ \\\\\n  vanilla & 10 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 10 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 10 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.37}} & \\color{blue}{\\textbf{0.365}} \\\\\n  full & 15 & \\textit{Generic} & $-5.32 \\cdot 10^{28}$ & $1.21 \\cdot 10^{29}$ \\\\\n  full & 15 & \\textit{Omniscient} & -4.34 & 4.38 \\\\\n  full & 15 & \\textit{REVISE} & -473 & $1.03 \\cdot 10^{3}$ \\\\\n  vanilla & 15 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 15 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 15 & \\textit{REVISE} & -11.3 & 13.1 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Moons data by energy penalty.\n\n:::\n\n#### Circles\n\n- **Energy Penalty** (@tbl-circles-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, though primarily for low to medium choices of the energy penalty (<=5) during training. The same goes for *Generic*, which sometimes outperforms *ECCo* (for small energy penalty at evaluation time). *Omni* does alright for lower energy penalty at evaluation time, but loses out for higher choices. *REVISE* performs poorly across the board (except very low choices at evaluation time).\n- **Cost (distance penalty)**: *ECCo* and *Generic* generally achieve the best results when no cost penalty is used during training. Both *Omni* and *REVISE* are largely unaffected.\n- **Maximum Iterations**: *ECCo* consistently yields better results for higher numbers of iterations. *Generic* generally does best for a medium number (50). *Omni* is sometimes invalid (**???**).\n- **Validity**: *ECCo* tends to outperform its *Vanilla* counterpart, though primarily for low to medium choices of the energy penalty (<=5) during training and evaluation. *Vanilla* typically worse across the board.\n- **Accuracy**: Mostly unaffected, but *REVISE* again consistently some deterioration and *ECCo* deteriorates for high choices of energy penalty during training, reflecting other outcomes above.\n\n\n\n\n\n\n::: {#tbl-circles-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.26}} & \\color{blue}{\\textbf{0.423}} \\\\\n  full & 0.01 & \\textit{Generic} & -1.49 & 0.71 \\\\\n  full & 0.01 & \\textit{Omniscient} & -5.21 & 5.25 \\\\\n  full & 0.01 & \\textit{REVISE} & $-2.71 \\cdot 10^{26}$ & $6.37 \\cdot 10^{26}$ \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -9.33 & 7.34 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -8.89 & 6.88 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -8.67 & 6.87 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -8.65 & 6.8 \\\\\n  full & 0.05 & \\textit{ECCo} & -1.29 & 0.397 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.21}} & \\color{blue}{\\textbf{0.356}} \\\\\n  full & 0.05 & \\textit{Omniscient} & -5.08 & 5.09 \\\\\n  full & 0.05 & \\textit{REVISE} & $-5.91 \\cdot 10^{27}$ & $1.36 \\cdot 10^{28}$ \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -9.35 & 7.32 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -8.85 & 6.87 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -8.7 & 6.96 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -8.52 & 6.76 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.2}} & \\color{blue}{\\textbf{0.383}} \\\\\n  full & 0.1 & \\textit{Generic} & -1.5 & 0.735 \\\\\n  full & 0.1 & \\textit{Omniscient} & -5.17 & 5.23 \\\\\n  full & 0.1 & \\textit{REVISE} & $-3.06 \\cdot 10^{26}$ & $7.7 \\cdot 10^{26}$ \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -9.33 & 7.32 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -8.88 & 6.86 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -8.69 & 6.9 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -8.68 & 6.81 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.12}} & \\color{blue}{\\textbf{0.217}} \\\\\n  full & 0.5 & \\textit{Generic} & -1.21 & 0.352 \\\\\n  full & 0.5 & \\textit{Omniscient} & -5.09 & 5.12 \\\\\n  full & 0.5 & \\textit{REVISE} & $-5.97 \\cdot 10^{27}$ & $1.37 \\cdot 10^{28}$ \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -9.35 & 7.3 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -8.89 & 6.92 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -8.68 & 6.93 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -8.53 & 6.75 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.1}} & \\color{blue}{\\textbf{0.163}} \\\\\n  full & 1 & \\textit{Generic} & -1.49 & 0.726 \\\\\n  full & 1 & \\textit{Omniscient} & -5.16 & 5.2 \\\\\n  full & 1 & \\textit{REVISE} & $-3.09 \\cdot 10^{26}$ & $7.22 \\cdot 10^{26}$ \\\\\n  vanilla & 1 & \\textit{ECCo} & -9.34 & 7.36 \\\\\n  vanilla & 1 & \\textit{Generic} & -8.86 & 6.85 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -8.7 & 6.9 \\\\\n  vanilla & 1 & \\textit{REVISE} & -8.69 & 6.85 \\\\\n  full & 5 & \\textit{ECCo} & -1.75 & 0.154 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.21}} & \\color{blue}{\\textbf{0.363}} \\\\\n  full & 5 & \\textit{Omniscient} & -5.14 & 5.16 \\\\\n  full & 5 & \\textit{REVISE} & $-1.1 \\cdot 10^{28}$ & $2.5 \\cdot 10^{28}$ \\\\\n  vanilla & 5 & \\textit{ECCo} & -9.36 & 7.32 \\\\\n  vanilla & 5 & \\textit{Generic} & -8.88 & 6.91 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -8.7 & 6.93 \\\\\n  vanilla & 5 & \\textit{REVISE} & -8.52 & 6.73 \\\\\n  full & 10 & \\textit{ECCo} & $-1.02 \\cdot 10^{6}$ & $2.32 \\cdot 10^{6}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.49}} & \\color{blue}{\\textbf{0.702}} \\\\\n  full & 10 & \\textit{Omniscient} & -5.13 & 5.16 \\\\\n  full & 10 & \\textit{REVISE} & $-3.74 \\cdot 10^{26}$ & $9.09 \\cdot 10^{26}$ \\\\\n  vanilla & 10 & \\textit{ECCo} & -9.31 & 7.33 \\\\\n  vanilla & 10 & \\textit{Generic} & -8.87 & 6.86 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -8.7 & 6.89 \\\\\n  vanilla & 10 & \\textit{REVISE} & -8.69 & 6.83 \\\\\n  full & 15 & \\textit{ECCo} & $-3.31 \\cdot 10^{13}$ & $7.54 \\cdot 10^{13}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.22}} & \\color{blue}{\\textbf{0.37}} \\\\\n  full & 15 & \\textit{Omniscient} & -5.2 & 5.23 \\\\\n  full & 15 & \\textit{REVISE} & $-9.01 \\cdot 10^{27}$ & $2.06 \\cdot 10^{28}$ \\\\\n  vanilla & 15 & \\textit{ECCo} & -9.38 & 7.34 \\\\\n  vanilla & 15 & \\textit{Generic} & -8.86 & 6.87 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -8.69 & 6.96 \\\\\n  vanilla & 15 & \\textit{REVISE} & -8.51 & 6.73 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Circles data by energy penalty.\n\n:::\n\n",
    "supporting": [
      "paper_files"
    ],
    "filters": []
  }
}