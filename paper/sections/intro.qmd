# Introduction

Today's prominence of artificial intelligence (AI) has largely been driven by advances in **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern AIs are tasked with learning these representations from scratch, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such AIs with ever greater degrees of freedom to achieve that task, which has often led them to outperform traditionally more parsimonious models. Unfortunately, in doing so they also learn increasingly complex and highly sensitive representations that we can no longer easily interpret.

This trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @goodfellow2014explaining showed that artificial neural networks (ANN) are sensitive to adversarial examples (AE): counterfactuals of model inputs that yield vastly different model predictions despite being semantically indifferent from their factual counterparts. Despite partially effective mitigation strategies such as **adversarial training**, truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. 

Part of the problem is that great degrees of freedom provide room for many locally optimal solutions when using narrow objectives [@wilson2020case]. Based purely on predictive performance, these solutions may seem to provide compelling explanations for the data at hand, even though they are in fact not grounded in meaningful semantics. 
In other words, the greatest strength of modern representation learning is also its greatest pitfall. 

This opaqueness has another dire consquence: since we cannot easily interpret the mapping from inputs to outputs, deploying such models in practive effectively means blindly relying on model predictions. On countless occasions, this has already caused real harm to people who were affected adversely and often unfairly by automated decision-making systems involving opaque models [@oneil2016weapons]. Prominent voices have therefore argued to completely abolish the use of such "black boxes" along with any attempts to explain their predictions [@rudin2019stop]: after all, if we had some interpretable abstraction of a "black box" that explains its decisions with full fidelity, there would be no need for the "black box" at all, because we could simply rely on the abstraction for decision-making. 

While we sympathise with that stance and share the aforementioned concerns, the reality is that the trend towards complexity has not been reversed. If anything, it has accelerated on the back of unprecented investments in computing infrastructure. With opaque AI here to stay, we believe that **explainable AI** (XAI) will conintue to play an important role in dealing with "black boxes". 