{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\FloatBarrier\n",
        "\n",
        "<!-- \\setcounter{section}{0} -->\n",
        "\\renewcommand{\\thesection}{\\Alph{section}}\n",
        "\n",
        "<!-- \\setcounter{table}{0} -->\n",
        "\\renewcommand{\\thetable}{A\\arabic{table}}\n",
        "\n",
        "<!-- \\setcounter{figure}{0} -->\n",
        "\\renewcommand{\\thefigure}{A\\arabic{figure}}\n",
        "\n",
        "<!-- # Supplementary Material {.appendix} -->\n",
        "\n",
        "\n",
        "# Notation {.appendix}\n",
        "\n",
        "- $y^+$: The target class and also the index of the target class.\n",
        "- $y^-$: The non-target class and also the index of non-the target class.\n",
        "- $\\mathbf{y}^+$: The one-hot encoded output vector for the target class. \n",
        "- $\\theta$: Model parameters (unspecified).\n",
        "- $\\Theta$: Matrix of parameters. \n",
        "\n",
        "## Other Technical Details\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "MMD({X}^\\prime,\\tilde{X}^\\prime) &= \\frac{1}{m(m-1)}\\sum_{i=1}^m\\sum_{j\\neq i}^m k(x_i,x_j) \\\\ &+ \\frac{1}{n(n-1)}\\sum_{i=1}^n\\sum_{j\\neq i}^n k(\\tilde{x}_i,\\tilde{x}_j) \\\\ &- \\frac{2}{mn}\\sum_{i=1}^m\\sum_{j=1}^n k(x_i,\\tilde{x}_j)\n",
        "\\end{aligned}\n",
        "$$ {#eq-mmd}\n",
        "\n",
        "\n",
        "\n",
        "# Technical Details of Our Approach {.appendix} \n",
        "\n",
        "\n",
        "## Generating Counterfactuals through Gradient Descent {#sec-app-ce}\n",
        "\n",
        "In this section, we provide some background on gradient-based counterfactual generators (@sec-app-ce-background) and discuss how we define convergence in this context (@sec-app-conv).\n",
        "\n",
        "### Background {#sec-app-ce-background}\n",
        "\n",
        "Gradient-based counterfactual search was originally proposed by @wachter2017counterfactual. It generally solves the following unconstrained objective,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{\\mathbf{z}^\\prime \\in \\mathcal{Z}^L} \\left\\{  {\\text{yloss}(\\mathbf{M}_{\\theta}(g(\\mathbf{z}^\\prime)),\\mathbf{y}^+)}+ \\lambda {\\text{cost}(g(\\mathbf{z}^\\prime)) }  \\right\\} \n",
        "\\end{aligned} \n",
        "$$\n",
        "\n",
        "where $g: \\mathcal{Z} \\mapsto \\mathcal{X}$ is an invertible function that maps from the $L$-dimensional counterfactual state space to the feature space and $\\text{cost}(\\cdot)$ denotes one or more penalties that are used to induce certain properties of the counterfactual outcome. As above, $\\mathbf{y}^+$ denotes the target output and $\\mathbf{M}_{\\theta}(\\mathbf{x})$ returns the logit predictions of the underlying classifier for $\\mathbf{x}=g(\\mathbf{z})$.\n",
        "\n",
        "For all generators used in this work we use standard logit crossentropy loss for $\\text{yloss}(\\cdot)$. All generators also penalize the distance ($\\ell_1$-norm) of counterfactuals from their original factual state. For *Generic* and *ECCo*, we have $\\mathcal{Z}:=\\mathcal{X}$ and $g(\\mathbf{z})=g(\\mathbf{z})^{-1}=\\mathbf{z}$, that is counterfactual are searched directly in the feature space. Conversely, *REVISE* traverses the latent space of a variational autoencoder (VAE) fitted to the training data, where $g(\\cdot)$ corresponds to the decoder [@joshi2019realistic]. In addition to the distance penalty, *ECCo* uses an additional penalty component that regularizes the energy associated with the counterfactual, $\\mathbf{x}^\\prime$ [@altmeyer2024faithful]. \n",
        "\n",
        "### Convergence {#sec-app-conv}\n",
        "\n",
        "An important consideration when generating counterfactual explanations using gradient-based methods is how to define convergence. Two common choices are to 1) perform gradient descent over a fixed number of iterations $T$, or 2) conclude the search as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$. We prefer the latter for our purposes, because it explicitly defines convergence in terms of the black-box model, $\\mathbf{M}(\\mathbf{x})$.\n",
        "\n",
        "Defining convergence in this way allows for a more intuitive interpretation of the resulting counterfactual outcomes than with fixed $T$. Specifically, it allows us to think of counterfactuals as explaining 'high-confidence' predictions by the model for the target class $y^+$. Depending on the context and application, different choices of $\\tau$ can be considered as representing 'high-confidence' predictions.\n"
      ],
      "id": "491391fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "projectdir = splitpath(pwd()) |>\n",
        "    ss -> joinpath(ss[1:findall([s == \"CounterfactualTraining.jl\" for s in ss])[1]]...) \n",
        "cd(projectdir)\n",
        "\n",
        "using CTExperiments\n",
        "using CTExperiments.CounterfactualExplanations\n",
        "using CTExperiments.CounterfactualTraining\n",
        "using CTExperiments.Flux\n",
        "using CTExperiments.Plots\n",
        "using CTExperiments.TaijaPlotting\n",
        "using Plots.PlotMeasures"
      ],
      "id": "2e209b3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Protecting Mutability Constraints with Linear Classifiers {#sec-app-constraints}\n",
        "\n",
        "In @sec-constraints we explain that to avoid penalizing implausibility that arises due to mutability constraints, we impose a point mass prior on $p(\\mathbf{x})$ for the corresponding feature. We argue in @sec-constraints that this approach induces models to be less sensitive to immutable features and demonstrate this empirically in @sec-experiments. Below we derive the analytical results in @prp-mtblty.\n",
        "\n",
        "::: {.proof}\n",
        "\n",
        "Let $d_{\\text{mtbl}}$ and $d_{\\text{immtbl}}$ denote some mutable and immutable feature, respectively. Suppose that $\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}}$ and $\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}}$, where $\\mu_{k,d}$ denotes the conditional sample mean of feature $d$ in class $k$. In words, we assume that the immutable feature tends to take lower values for samples in the non-target class $y^-$ than in the target class $y^+$. We assume the opposite to hold for the mutable feature.\n",
        "\n",
        "Assuming multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, we have for the log likelihood ratio between any two classes $k,m \\in \\mathcal{K}$ [@hastie2009elements]:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})}=\\mathbf{x}^\\intercal \\Sigma^{-1}(\\mu_{k}-\\mu_{m})  + \\text{const}\n",
        "$$ {#eq-loglike}\n",
        "\n",
        "By independence of $x_1,...,x_D$, the full log-likelihood ratio decomposes into:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\frac{\\mu_{k,d}-\\mu_{m,d}}{\\sigma_{d}^2} x_{d} + \\text{const}\n",
        "$$ {#eq-loglike-decomp}\n",
        "\n",
        "By the properties of our classifier (*multinomial logistic regression*), we have:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\left( \\theta_{k,d} - \\theta_{m,d} \\right)x_d + \\text{const}\n",
        "$$ {#eq-multi}\n",
        "\n",
        "where $\\theta_{k,d}=\\Theta[k,d]$ denotes the coefficient on feature $d$ for class $k$. \n",
        "\n",
        "Based on @eq-loglike-decomp and @eq-multi we can identify that $(\\mu_{k,d}-\\mu_{m,d}) \\propto (\\theta_{k,d} - \\theta_{m,d})$ under the assumptions we made above. Hence, we have that $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$ and $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$\n",
        "\n",
        "Let $\\mathbf{x}^\\prime$ denote some randomly chosen individual from class $y^-$ and let $y^+ \\sim p(y)$ denote the randomly chosen target class. Then the partial derivative of the contrastive divergence penalty [@eq-div] with respect to coefficient $\\theta_{y^+,d}$ is equal to \n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left(\\text{div}(\\mathbf{x},\\mathbf{x^\\prime},\\mathbf{y};\\theta)\\right) = \\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left( \\left(-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]\\right) - \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^\\prime)[y^+]\\right) \\right) = x_{d}^\\prime - x_{d}\n",
        "$$ {#eq-grad}\n",
        "\n",
        "and equal to zero everywhere else.\n",
        "\n",
        "Since $(\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}})$ we are more likely to have $(x_{d_{\\text{immtbl}}}^\\prime - x_{d_{\\text{immtbl}}}) < 0$ than vice versa at initialization. Similarly, we are more likely to have $(x_{d_{\\text{mtbl}}}^\\prime - x_{d_{\\text{mtbl}}}) > 0$ since $(\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}})$.\n",
        "\n",
        "This implies that if we do not protect feature $d_{\\text{immtbl}}$, the contrastive divergence penalty will decrease $\\theta_{y^-,d_{\\text{immtbl}}}$ thereby exacerbating the existing effect $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$. In words, not protecting the immutable feature would have the undesirable effect of making the classifier more sensitive to this feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for lower values of $d_{\\text{immtbl}}$. \n",
        "\n",
        "By the same rationale, the contrastive divergence penalty can generally be expected to increase $\\theta_{y^-,d_{\\text{mtbl}}}$ exacerbating $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$. In words, this has the effect of making the classifier more sensitive to the mutable feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for higher values of $d_{\\text{mtbl}}$.\n",
        "\n",
        "Thus, our proposed approach of protecting feature $d_{\\text{immtbl}}$ has the net affect of decreasing the classifier's sensitivity to the immutable feature relative to the mutable feature (i.e. no change in sensitivity for $d_{\\text{immtbl}}$ relative to increased sensitivity for $d_{\\text{mtbl}}$).\n",
        "\n",
        ":::"
      ],
      "id": "f127ef0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "using CTExperiments: \n",
        "    get_ce_data, \n",
        "    train_val_split, \n",
        "    build_model, \n",
        "    LinearModel, \n",
        "    MLPModel,\n",
        "    get_input_encoder\n",
        "\n",
        "# Callback:\n",
        "using CTExperiments: get_log_reg_params, get_decision_boundary\n",
        "\n",
        "function plot_db(model, ces; _xlab=\"Debt\", _ylab=\"Age\", plot_contour::Bool=false)\n",
        "\n",
        "    x = (ce -> ce.counterfactual).(ces) |> xs -> reduce(hcat, xs)\n",
        "    x0 = (ce -> ce.factual).(ces) |> xs -> reduce(hcat, xs)\n",
        "\n",
        "    xlab = _constraint[1] == \"both\" ? \"$(_xlab) (mutable)\" : \"$(_xlab) (immutable)\"\n",
        "    ylab = _constraint[2] == \"both\" ? \"$(_ylab) (mutable)\" : \"$(_ylab) (immutable)\"\n",
        "\n",
        "    # Data and decision boundary:\n",
        "    plt = Plots.scatter(\n",
        "        ce_data, \n",
        "        # label=[\"1=Default\" \"2=No Default\"], \n",
        "        # legend_position=:topright, \n",
        "        xlab=xlab, ylab=ylab,  \n",
        "        axis=nothing, \n",
        "        legend=false,\n",
        "        title=_title\n",
        "    )\n",
        "    if length(model) == 1\n",
        "        coeff = get_log_reg_params(model)\n",
        "        db = get_decision_boundary(coeff)\n",
        "        Plots.abline!(plt, db.slope, db.intercept; lw=5, label=\"Dec. Boundary\")\n",
        "    end\n",
        "\n",
        "    # Counterfactuals:\n",
        "    target = 2\n",
        "    factual = 1\n",
        "    if !any(isnothing.(x))\n",
        "        yhat = [argmax(y) for y in eachcol(model(x))]\n",
        "        yhat0 = [argmax(y) for y in eachcol(model(x0))]\n",
        "        idx_plotted = yhat0.==factual .|| yhat0.==target\n",
        "        if any(idx_plotted)\n",
        "\n",
        "            # # Paths:\n",
        "            # u = []\n",
        "            # v = []\n",
        "            # for (i,ce) in enumerate(eachcol(x))\n",
        "            #     Δ = ce - x0[:,i]\n",
        "            #     push!(u, Δ[1])\n",
        "            #     push!(v, Δ[2])\n",
        "            # end\n",
        "            # Plots.quiver!(x0[1,idx_plotted], x0[2,idx_plotted], quiver=(u[idx_plotted], v[idx_plotted]), color=yhat0[idx_plotted])\n",
        "            \n",
        "            # End points:\n",
        "            Plots.scatter!(x[1,idx_plotted], x[2,idx_plotted], label=[\"CE (y⁺=1)\" \"CE (y⁺=2)\"], ms=15, shape=:star, color=yhat[idx_plotted], group=yhat[idx_plotted], mscolor=yhat0[idx_plotted])\n",
        "\n",
        "        end\n",
        "    end\n",
        "    display(plt)\n",
        "end\n",
        "\n",
        "# Data:\n",
        "specs = [\n",
        "    # (\"(a)\", VanillaObjective(needs_ce=true), [\"both\", \"both\"]),\n",
        "    # (\"(b)\", FullObjective(lambda=[1.0,0.5,0.01,0.1]), [\"both\", \"both\"]),\n",
        "    # (\"(c)\", VanillaObjective(needs_ce=true), [\"both\", \"none\"]),\n",
        "    (\"(d)\", FullObjective(lambda=[1.0,0.5,0.01,0.1]), [\"both\", \"none\"]),\n",
        "]"
      ],
      "id": "451d504a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "using CounterfactualExplanations.Convergence\n",
        "using Random\n",
        "Random.seed!(42)\n",
        "\n",
        "models = []\n",
        "plts = []\n",
        "\n",
        "for (title, obj, constraint) in specs\n",
        "\n",
        "    # Globals for the callback:\n",
        "    global _title = title\n",
        "    global _constraint = constraint\n",
        "\n",
        "    # Data:\n",
        "    data = LinearlySeparable(\n",
        "        n_train=3000,\n",
        "        batchsize=50,\n",
        "        mutability=constraint\n",
        "    )\n",
        "    global ce_data = get_ce_data(data)\n",
        "    val_size = data.n_validation / (data.n_validation + data.n_train)\n",
        "    train_set, val_set, _ = train_val_split(data, ce_data, val_size)\n",
        "\n",
        "    # Model:\n",
        "    nin = size(first(train_set)[1], 1)\n",
        "    nout = size(first(train_set)[2], 1)\n",
        "    model = build_model(MLPModel(), nin, nout)\n",
        "\n",
        "    # Objective:\n",
        "    opt = AMSGrad()\n",
        "    generator = GenericGenerator(opt=Descent(0.1))\n",
        "    opt_state = Flux.setup(opt, model)\n",
        "    conv = DecisionThresholdConvergence(decision_threshold=0.75, max_iter=30)\n",
        "\n",
        "    model, logs = counterfactual_training(\n",
        "        obj,\n",
        "        model,\n",
        "        generator,\n",
        "        train_set,\n",
        "        opt_state;\n",
        "        val_set = val_set,\n",
        "        nepochs = 100,\n",
        "        mutability = Symbol.(constraint),\n",
        "        callback = plot_db,\n",
        "        nce=1000,\n",
        "        convergence=conv, \n",
        "        verbose=3\n",
        "    )\n",
        "\n",
        "    push!(models, model)\n",
        "    push!(plts, current())\n",
        "end"
      ],
      "id": "8c99d160",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "plt = Plots.plot(\n",
        "    plts..., \n",
        "    layout=(1,4), \n",
        "    size=(1150,250),  \n",
        "    left_margin = 10mm, \n",
        "    bottom_margin = 5mm,\n",
        "    top_margin = 3mm,\n",
        "    right_margin = 10mm,\n",
        ")\n",
        "display(plt)\n",
        "Plots.savefig(plt, \"paper/figures/poc.svg\")"
      ],
      "id": "996265a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "plts = []\n",
        "titles = [\"(a)\", \"(b)\"]\n",
        "_xlab = \"Existing Debt\"\n",
        "_ylab = \"Age\"\n",
        "for (i,model) in enumerate(models)\n",
        "    coeff = get_log_reg_params(model)\n",
        "    db = get_decision_boundary(coeff)\n",
        "    xlab = constraints[i][1] == \"both\" ? \"$(_xlab) (mutable)\" : \"$(_xlab) (immutable)\"\n",
        "    ylab = constraints[i][2] == \"both\" ? \"$(_ylab) (mutable)\" : \"$(_ylab) (immutable)\"\n",
        "    plt = Plots.scatter(ce_data; xlab=xlab, ylab=ylab, bottom_margin = 5mm, left_margin = 5mm, label=[\"Default\" \"No Default\"], title=titles[i])\n",
        "    Plots.abline!(plt, db.slope, db.intercept; lw=5, label=\"Dec. Boundary\")\n",
        "\n",
        "    push!(plts, plt)\n",
        "end\n",
        "\n",
        "plt = Plots.plot(plts..., layout=(1,2), size=(600,300))\n",
        "Plots.savefig(plt, \"paper/figures/app_mtblty.svg\")"
      ],
      "id": "43455310",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain Constraints\n",
        "\n",
        "We apply domain constraints on counterfactuals during training and evaluation. There are at least two good reasons for doing so. Firstly, within the context of explainability and algorithmic recourse, real-world attributes are often domain constrained: the *age* feature, for example, is lower bounded by zero and upper bounded by the maximum human lifespan. Secondly, domain constraints help mitigate training instabilities commonly associated with energy-based modelling [@grathwohl2020your;@altmeyer2024faithful].\n",
        "\n",
        "For our image datasets, features are pixel values and hence the domain is constrained by the lower and upper bound of values that pixels can take depending on how they are scaled (in our case $[-1,1]$). For all other features $d$ in our synthetic and tabular datasets, we automatically infer domain constraints $[x_d^{\\text{LB}},x_d^{\\text{UB}}]$  as follows,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "x_d^{\\text{LB}} &= \\arg\\min_{x_d} \\{\\mu_d - n_{\\sigma_d}\\sigma_d, \\arg \\min_{x_d} x_d\\} \\\\\n",
        "x_d^{\\text{UB}} &= \\arg\\max_{x_d} \\{\\mu_d + n_{\\sigma_d}\\sigma_d, \\arg \\max_{x_d} x_d\\} \n",
        "\\end{aligned}\n",
        "$$ {#eq-domain}\n",
        "\n",
        "where $\\mu_d$ and $\\sigma_d$ denote the sample mean and standard deviation of feature $d$. We set $n_{\\sigma_d}=3$ across the board but higher values and hence wider bounds may be appropriate depending on the application.\n",
        "\n",
        "\n",
        "## Training Details {#sec-app-training}\n",
        "\n",
        "In this section, we describe the training procedure in detail. While the details laid out here are not crucial for understanding our proposed approach, they are of importance to anyone looking to implement counterfactual training. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Detailed Results {.appendix}\n",
        "\n",
        "## Qualitative Findings for Image Data\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla). In fact, this is not even using *ECCo+* and still showing better results than the best results we achieved in our AAAI paper for JEM ensembles.\n",
        "\n",
        ":::\n",
        "\n",
        "![Counterfactual images for *MLP* with counterfactual training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}\n",
        "\n",
        "![Counterfactual images for *MLP* with conventional training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}\n",
        "\n",
        "\n",
        "## Grid Searches {#sec-app-grid}"
      ],
      "id": "51706d35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "projectdir = splitpath(pwd()) |>\n",
        "    ss -> joinpath(ss[1:findall([s == \"CounterfactualTraining.jl\" for s in ss])[1]]...) \n",
        "cd(projectdir)\n",
        "\n",
        "using CTExperiments\n",
        "using CTExperiments.CSV\n",
        "using CTExperiments.DataFrames\n",
        "using CTExperiments.StatsBase\n",
        "\n",
        "using DotEnv\n",
        "DotEnv.load!()"
      ],
      "id": "3006e124",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "res_dir = ENV[\"FINAL_GRID_RESULTS\"]"
      ],
      "id": "b137b497",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To assess the hyperparameter sensitivity of our proposed training regime we ran multiple large grid searches for all of our synthetic datasets. We have grouped these grid searches into multiple categories: \n",
        "\n",
        "1. **Generator Parameters** (@sec-app-grid-gen): Investigates the effect of changing hyperparameters that affect the counterfactual outcomes during the training phase.\n",
        "2. **Penalty Strengths** (@sec-app-grid-pen): Investigates the effect of changing the penalty strengths in out proposed objective (@eq-obj).\n",
        "3. **Other Parameters** (@sec-app-grid-train): Investigates the effect of changing other training parameters, including the total number of generated counterfactuals in each epoch.\n",
        "\n",
        "We begin by summarizing the high-level findings in @sec-app-grid-hl. For each of the categories, @sec-app-grid-gen to @sec-app-grid-train then present all details including the exact parameter grids, average predictive performance outcomes and key evaluation metrics for the generated counterfactuals. \n",
        "\n",
        "### Evaluation Details\n",
        "\n",
        "To measure predictive performance, we compute the accuracy and F1-score for all models on test data (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). With respect to explanatory performance, we report here our findings for the (im)plausibility and cost of counterfactuals at test time. Since the computation of our proposed divergence metric (@eq-impl-div) is memory-intensive, we rely on the distance-based metric for the grid searches. For the counterfactual evaluation, we draw factual samples from the training data for the grid searches to avoid data leakage with respect to our final results reported in the body of the paper. Specifically, we want to avoid choosing our default hyperparameters based on results on the test data. Since we are optimizing for explainability, not predictive performance, we still present test accuracy and F1-scores. \n",
        "\n",
        "#### Predictive Performance\n",
        "\n",
        "We find that CT is associated with little to no decrease in average predictive performance for our synthetic datasets: test accuracy and F1-scores decrease by at most ~1 percentage point, but generally much less (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). Variation across hyperparameters is negligible as indicated by small standard deviations for these metrics across the board. \n",
        "\n",
        "#### Counterfactual Outcomes {#sec-app-grid-hl}\n",
        "\n",
        "Overall, we find that Counterfactual Training (CT) achieves it key objectives consistently across all hyperparameter settings and also broadly across datasets: plausibility is improved by up to ~60 percentage points (ppts) for the *Circles* data (e.g. @fig-grid-gen_params-plaus-circles), ~25-30ppts for the *Moons* data (e.g. @fig-grid-gen_params-plaus-moons) and ~10-20ppts for the *Linearly Separable* data (e.g. @fig-grid-gen_params-plaus-lin_sep). At the same time, the average costs of faithful counterfactuals are reduced in many cases by around ~20-25ppts for  *Circles* (e.g. @fig-grid-gen_params-cost-circles) and up to ~50ppts for *Moons* (e.g. @fig-grid-gen_params-cost-moons). For the *Linearly Separable* data, costs are generally increased although typically by less than 10ppts (e.g. @fig-grid-gen_params-cost-lin_sep), which reflects a common tradeoff between costs and plausibility [@altmeyer2024faithful]. \n",
        "\n",
        "We do observe strong sensitivity to certain hyperparameters, with clear an manageable patterns. Concerning generator parameters, we firstly find that using *REVISE* to generate counterfactuals during training typically yields the worst outcomes out of all generators, often leading to a substantial decrease in plausibility. This finding can be attributed to the fact that *REVISE* effectively assigns the task of learning plausible explanations from the model itself to a surrogate VAE. In other words, counterfactuals generated by *REVISE* are less faithful to the model that *ECCo* and *Generic*, and hence we would expect them to be a less effective and, in fact, potentially detrimental role in our training regime. Secondly, we observe that allowing for a higher number of maximum steps $T$ for the counterfactual search generally yields better outcomes. This is intuitive, because it allows more counterfactuals to reach maturity in any given iteration. Looking in particular at the results for *Linearly Separable*, it seems that higher values for $T$ in combination with higher decision thresholds ($\\tau$) yields the best results when using *ECCo*. But depending on the degree of class separability of the underlying data, a high decision-threshold can also affect results adversely, as evident from the results for the *Overlapping* data (@fig-grid-gen_params-plaus-over): here we find that CT generally fails to achieve its objective because only a tiny proportion of counterfactuals ever reaches maturity.\n",
        "\n",
        "Regarding penalty strengths, we find that the strength of the energy regularization, $\\lambda_{\\text{reg}}$ is a key hyperparameter, while sensitivity with respect to $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$ is much less evident. In particular, we observe that not regularizing energy enough or at all typically leads to poor performance in terms of decreased plausibility and increased costs, in particular for *Circles* (@fig-grid-pen-plaus-circles), *Linearly Separable* (@fig-grid-pen-plaus-lin_sep) and *Overlapping* (@fig-grid-pen-plaus-over). High values of $\\lambda_{\\text{reg}}$ can increase the variability in outcomes, in particular when combined with high values for $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$, but this effect is less pronounced.\n",
        "\n",
        "Finally, concerning other hyperparameters we observe that the effectiveness and stability of CT is positively associated with the number of counterfactuals generated during each training epoch, in particular for *Circles* (@fig-grid-train-plaus-circles) and *Moons* (@fig-grid-train-plaus-moons). We further find that a higher number of training epochs is beneficial as expected, where we tested training models for 50 and 100 epochs. Interestingly, we find that it is not necessary to employ CT during the entire training phase to achieve the desired improvements in explainability: specifically, we have tested training models conventionally during the first half of training before switching to CT after this initial burn-in period. \n",
        "\n",
        "### Generator Parameters {#sec-app-grid-gen}"
      ],
      "id": "52541f73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_dir = joinpath(res_dir, \"gen_params/mlp\")\n",
        "data_dirs = readdir(grid_dir) |> x -> joinpath.(grid_dir, x) |> x -> x[isdir.(x)]\n",
        "eval_grids = (p -> EvaluationGrid(joinpath(p,\"grid_config.toml\"))).(data_dirs)\n",
        "data_names = basename.(data_dirs)\n",
        "suffix = \"evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/\""
      ],
      "id": "0c881c57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter grid with varying generator parameters during training is shown in @nte-gen-params-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-gen-params-final-run-eval.\n",
        "\n",
        "::: {#nte-gen-params-final-run-train .callout-note}\n",
        "\n",
        "## Training Phase"
      ],
      "id": "f3f86162"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/grid_config.toml\")) \n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "4aacc084",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {#nte-gen-params-final-run-eval .callout-note}\n",
        "\n",
        "## Evaluation Phase"
      ],
      "id": "e8018342"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/evaluation/evaluation_grid_config.toml\"))\n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "4e176a99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "::: {#tbl-acc-gen}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}"
      ],
      "id": "3b5fd65b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "df = CTExperiments.aggregate_performance(eval_grids; byvars=[\"objective\"]) \n",
        "get_table_inputs(df, \"mean\";backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs; wrap_table=false)"
      ],
      "id": "2b8450e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Predictive performance measures by dataset and objective averaged across training-phase parameters (@nte-gen-params-final-run-train) and evaluation-phase parameters (@nte-gen-params-final-run-eval).\n",
        "\n",
        ":::\n",
        "\n",
        "#### Plausibility"
      ],
      "id": "e1ec6081"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-gen_params-plaus\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the plausibility measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "09a8a68c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"plausibility_distance_from_target.png\"\n",
        "fig_caption = \"Average outcomes for the plausibility measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption) \n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "d26f3fb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cost"
      ],
      "id": "290f3c6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-gen_params-cost\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the cost measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "b25ecc7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"distance.png\"\n",
        "fig_caption = \"Average outcomes for the cost measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption)\n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "671eb850",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Penalty Strengths {#sec-app-grid-pen}"
      ],
      "id": "351ae84d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_dir = joinpath(res_dir, \"penalties/mlp\")\n",
        "data_dirs = readdir(grid_dir) |> x -> joinpath.(grid_dir, x) |> x -> x[isdir.(x)]\n",
        "eval_grids = (p -> EvaluationGrid(joinpath(p,\"grid_config.toml\"))).(data_dirs)\n",
        "data_names = basename.(data_dirs)\n",
        "suffix = \"evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/\""
      ],
      "id": "2f70c1a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter grid with varying penalty strengths during training is shown in @nte-pen-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-pen-final-run-eval.\n",
        "\n",
        "::: {#nte-pen-final-run-train .callout-note}\n",
        "\n",
        "## Training Phase"
      ],
      "id": "69faca79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/grid_config.toml\")) \n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "c00243ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {#nte-pen-final-run-eval .callout-note}\n",
        "\n",
        "## Evaluation Phase"
      ],
      "id": "47a96785"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/evaluation/evaluation_grid_config.toml\"))\n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "c84c20b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "::: {#tbl-acc-pen}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}"
      ],
      "id": "c53ce94c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "df = CTExperiments.aggregate_performance(eval_grids; byvars=[\"objective\"]) \n",
        "get_table_inputs(df, \"mean\";backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs; wrap_table=false)"
      ],
      "id": "bf738714",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Predictive performance measures by dataset and objective averaged across training-phase parameters (@nte-pen-final-run-train) and evaluation-phase parameters (@nte-pen-final-run-eval).\n",
        "\n",
        ":::\n",
        "\n",
        "#### Plausibility"
      ],
      "id": "1e4fded5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-pen-plaus\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the plausibility measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "638a039e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"plausibility_distance_from_target.png\"\n",
        "fig_caption = \"Average outcomes for the plausibility measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption) \n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "6dd28241",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cost"
      ],
      "id": "bae5dbf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-pen-cost\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the cost measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "c21b0bf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"distance.png\"\n",
        "fig_caption = \"Average outcomes for the cost measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption)\n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "a3631345",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Parameters {#sec-app-grid-train}"
      ],
      "id": "4dfb183d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_dir = joinpath(res_dir, \"training_params/mlp\")\n",
        "data_dirs = readdir(grid_dir) |> x -> joinpath.(grid_dir, x) |> x -> x[isdir.(x)]\n",
        "eval_grids = (p -> EvaluationGrid(joinpath(p,\"grid_config.toml\"))).(data_dirs)\n",
        "data_names = basename.(data_dirs)\n",
        "suffix = \"evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/\""
      ],
      "id": "3dd9a329",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter grid with other varying training parameters is shown in @nte-train-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-train-final-run-eval.\n",
        "\n",
        "::: {#nte-train-final-run-train .callout-note}\n",
        "\n",
        "## Training Phase"
      ],
      "id": "f9129b12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/grid_config.toml\")) \n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "2b830d45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {#nte-train-final-run-eval .callout-note}\n",
        "\n",
        "## Evaluation Phase"
      ],
      "id": "ef906573"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(grid_dir, \"lin_sep/evaluation/evaluation_grid_config.toml\"))\n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "e93aed30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "::: {#tbl-acc-train}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}"
      ],
      "id": "dcf146a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "df = CTExperiments.aggregate_performance(eval_grids; byvars=[\"objective\"]) \n",
        "get_table_inputs(df, \"mean\";backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs; wrap_table=false)"
      ],
      "id": "3e82500a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Predictive performance measures by dataset and objective averaged across training-phase parameters (@nte-train-final-run-train) and evaluation-phase parameters (@nte-train-final-run-eval).\n",
        "\n",
        ":::\n",
        "\n",
        "#### Plausibility"
      ],
      "id": "77b6d7f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-train-plaus\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the plausibility measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "72dded9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"plausibility_distance_from_target.png\"\n",
        "fig_caption = \"Average outcomes for the plausibility measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption) \n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "7072e7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cost"
      ],
      "id": "39eb927a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "\n",
        "fig_label_prefix = \"grid-train-cost\"\n",
        "fig_labels = (nm -> \"fig-$(fig_label_prefix)-$nm\").(data_names)\n",
        "_str = \"The results with respect to the cost measure are shown in @$(fig_labels[1]) to @$(fig_labels[end]).\"\n",
        "println(_str)"
      ],
      "id": "69950f1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        " \n",
        "imgfname = \"distance.png\"\n",
        "fig_caption = \"Average outcomes for the cost measure across hyperparameters.\"\n",
        "full_paths = joinpath.(data_dirs, joinpath(suffix,imgfname))\n",
        "include_img_commands = CTExperiments.get_img_command(data_names, full_paths, fig_labels; fig_caption)\n",
        "_str = join(include_img_commands, \"\\n\\n\")\n",
        "println(_str)"
      ],
      "id": "eba676ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- {{< include /paper/sections/other/initial_grids.qmd >}} -->\n",
        "\n",
        "\n",
        "# Computation Details {.appendix}\n",
        "\n",
        "<!-- [@DHPC2022] -->\n",
        "## Hardware {#sec-hardware}\n",
        "\n",
        "We performed our experiments on a high-performance cluster. Details about the cluster will be disclosed upon publication to avoid revealing information that might interfere with the double-blind review process. Since our experiments involve highly parallel tasks and rather small models by today's standard, we have relied on distributed computing across multiple central processing units (CPU). Graphical processing units (GPU) were not required. Model training for the largest grid searches with 270 unique parameter combinations was parallelized across 34 CPUs with 2GB memory each. The time to completion varied by dataset for reasons discussed in @sec-discussion: 0h49m (*Moons*), 1h4m (*Linearly Separable*), 1h49m (*Circles*), 3h52m (*Overlapping*). Model evaluations for large grid searches were parallelized across 20 CPUs with 3GB memory each. Evaluations for all data sets took less than one hour (<1h) to complete. \n",
        "\n",
        "## Software\n",
        "\n",
        "All computations were performed in the Julia Programming Language [@bezanson2017julia]. We have developed a package for counterfactual training that leverages and extends the functionality provided by several existing packages, most notably [CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) [@altmeyer2023explaining] and the [Flux.jl](https://fluxml.ai/Flux.jl/v0.16/) library for deep learning [@innes2018fashionable;@innes2018flux]. For data-wrangling and presentation-ready tables we relied on [DataFrames.jl](https://dataframes.juliadata.org/v1.7/) [@milan2023dataframes] and [PrettyTables.jl](https://ronisbr.github.io/PrettyTables.jl/v2.4/) [@chagas2024pretty], respectively. For plots and visualizations we used both [Plots.jl](https://docs.juliaplots.org/v1.40/) [@PlotsJL] and [Makie.jl](https://docs.makie.org/v0.22/) [@danisch2021makie], in particular [AlgebraOfGraphics.jl](https://aog.makie.org/v0.9.3/). To distribute computational tasks across multiple processors, we have relied on [MPI.jl](https://juliaparallel.org/MPI.jl/v0.20/) [@byrne2021mpi].\n"
      ],
      "id": "0444d9c0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.1",
      "path": "/Users/paltmeyer/Library/Jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}