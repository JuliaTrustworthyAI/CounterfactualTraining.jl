---
engine: julia
julia: 
  exeflags: ["--project=experiments/"]
---

```{julia}
cd("../")

using CTExperiments
using CTExperiments.CSV
using CTExperiments.DataFrames
using CTExperiments.StatsBase
using DotEnv

DotEnv.load!()
```

# Appendix

## Initial Grid Search

```{julia}
results_dir = ENV["INITIAL_RUN"]
```

### Generator Params

```{julia}
gen_params_dir = joinpath(results_dir, "gen_params/mlp")
```

#### Linearly Separable 

- **Energy Penalty** (@tbl-lin_sep-lambda_energy_exper): *ECCo* generally does yield better results than *Vanilla* for higher choices of the energy penalty (10,15) during training. *Generic* performs poorly accross the board. *Omni* seems to have an anchoring effect, in that it never performs terribly but also never as good as the best *ECCo* results. *REVISE* performs poorly across the board.
- **Cost (distance penalty)**: Results for all generators (except *Omni*) are quite bad, which can likely be attributed to extremely bad results for some choices of the **Energy Penalty** (results here are averaged). For *ECCo* and *Generic*, higher cost values generally lead to worse results.
- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. 
- **Validity**: *ECCo* almost always valid except for very low values during training and high values at evaluation time. *Generic* often has poor validity.
- **Accuracy**: Seems largely unaffected.

```{julia}
df = CSV.read("$(gen_params_dir)/lin_sep/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv", DataFrame)
df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> 
  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>
  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type])
```

::: {.content-hidden unless-format="pdf"}

```{julia}
#| output: asis
get_table_inputs(df, "value"; alpha=0.9, byvars="lambda_energy_exper", backend=Val(:latex)) |>
    inputs -> tabulate_results(inputs; label="tbl-lin_sep-lambda_energy_exper", title="Results for Linearly Separable data by energy penalty.")
```


:::

#### Moons

```{julia}
df = CSV.read("$(gen_params_dir)/moons/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv", DataFrame)
df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> 
  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>
  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type]) 
```

::: {.content-hidden unless-format="pdf"}

```{julia}
#| output: asis
get_table_inputs(df, "value"; alpha=0.9, byvars="lambda_energy_exper", backend=Val(:latex)) |>
    inputs -> tabulate_results(inputs; label="tbl-moons-lambda_energy_exper", title="Results for Moons data by energy penalty.")
```

:::

#### Circles

```{julia}
df = CSV.read("$(gen_params_dir)/circles/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv", DataFrame)
df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> 
  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>
  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type]) 
```

::: {.content-hidden unless-format="pdf"}

```{julia}
#| output: asis
get_table_inputs(df, "value"; alpha=0.9, byvars="lambda_energy_exper", backend=Val(:latex)) |>
    inputs -> tabulate_results(inputs; label="tbl-circles-lambda_energy_exper", title="Results for Circles data by energy penalty.")
```

:::