# Related Literature {#sec-lit}

To make the desiderata for our framework more concrete, we follow @augustin2020adversarial in tying explainability to the quality of CEs that can be generated for a given model. The authors show that CEs (understood as minimal input perturbations that yield some desired model prediction) tend to be more meaningful if the underlying model is more robust to adversarial examples. We can make intuitive sense of this finding if we look at adversarial training (AT) through the lens of representation learning with high degrees of freedom. As argued before, learned representations may be sensitive to producing implausible explanations and mispredicting for worst-case counterfactuals (i.e., AEs). Thus, by inducing models to "unlearn" susceptiblity to such examples, adversarial training can effectively remove implausible explanations from the solution space.

## Adversarial Examples are Counterfactuals

The interpretation of the link between explainability through counterfactuals on the one side, and robustness to adversarial examples on the other is backed by empirical evidence. @sauer2021counterfactual demonstrate that using counterfactual images during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, show that augmenting the training data of (image) classifiers can improve generalization performance. Finally, @teney2020learning argue that counterfactual pairs tend to exist in training data. Hence, their approach aims to identify similar input samples with different annotations and ensure that the gradient of the classifier aligns with the vector between such pairs of counterfactual inputs using a cosine distance loss function. 

CEs have also been used to improve models in the natural language processing domain. For example, @wu2021polyjuice2 propose *Polyjuice*, a general-purpose CE generator for language models and demonstrate that the augmentation of training data with *Polyjuice* improves robustness in a number of tasks, while @luu2023counterfactual introduce the *Counterfactual Adversarial Training* (CAT) framework that aims to improve generalization and robustness of language models by generating counterfactuals for training samples that are subject to high predictive uncertainty. 

There have also been several attempts at formalizing the relationship between counterfactual explanations and adversarial examples. Pointing to clear similarities in how CEs and AEs are generated, @freiesleben2022intriguing makes the case for jointly studying the opaqueness and robustness problems in representation learning. Formally, AEs can be seen as the subset of CEs for which misclassification is achieved [@freiesleben2022intriguing]. Similarly, @pawelczyk2022exploring show that CEs and AEs are equivalent under certain conditions. 

Two other works are closely related to ours in that they use counterfactuals during training with the explicit goal of affecting certain properties of the post-hoc counterfactual explanations. Firstly, @ross2021learning propose a way to train models that guarantee recourse to a positive target class with high probability. Their approach builds on adversarial training by explicitly inducing susceptibility to targeted AEs for the positive class. Additionally, the method allows for imposing a set of actionability constraints ex-ante. For example, users can specify that certain features are immutable. Secondly, @guo2023counternet are the first to propose an end-to-end training pipeline that includes CEs as part of the training procedure. Their *CounterNet* network architecture includes a predictor and a CE generator, where the parameters of the CE generator are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor. In contrast, we impose no restrictions on the ANN architecture at all. 

## Aligning Representations with Explanations 

Improving the adversarial robustness of models is not the only path towards aligning representations with plausible explanations. In a closely related work, @altmeyer2024faithful show that explainability can be improved through model averaging and refined model objectives. They propose a way to generate counterfactuals that are maximally faithful to the model in that they are consistent with what the model has learned about the underlying data. Formally, they rely on tools from energy-based modelling [@teh2003energy] to minimize the divergence between the distribution of counterfactuals and the conditional posterior over inputs learned by the model. Their counterfactual explainer, *ECCCo*, yields plausible explanations if and only if the underlying model has learned representations that align with them. The authors find that both deep ensembles [@lakshminarayanan2016simple] and joint energy-based models (JEMs) [@grathwohl2020your] tend to do well in this regard. 

Once again it helps to look at these findings through the lens of representation learning with high degrees of freedom. Deep ensembles are approximate Bayesian model averages, which are particularly effective when models are underspecified by the available data [@wilson2020case]. Averaging across solutions mitigates the aforementioned risk of overrelying on a single locally optimal representation that corresponds to semantically meaningless explanations for the data. Likewise, previous work of @schut2021generating found that generating plausible ("interpretable") CEs is almost trivial for deep ensembles that have undergone adversarial training. The case for JEMs is even clearer: they optimize a hybrid objective that induces both high predictive performance and strong generative capacity [@grathwohl2020your], which bears resemblance to the idea of aligning models with plausible explanations and has inspired our CT objective.