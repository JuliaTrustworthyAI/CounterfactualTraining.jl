\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{abstract}
Counterfactual Explanations have emerged as a popular tool to explain
predictions made by opaque machine learning models: they explain how
factual inputs need to change in order for some fitted model to produce
some desired output. Much existing research has focused on identifying
explanations that are not only valid but also deemed plausible and
desirable with respect to the underlying data and stakeholder
requirements. Recent work has shown that under this premise, the task of
learning plausible explanations is effectively reassigned from the model
itself to the (post-hoc) counterfactual explainer. Building on that
work, we propose a novel model objective that leverages counterfactuals
during the training phase (ad-hoc) in order to minimize the divergence
between learned representations and plausible explanations. Through
extensive experiments, we demonstrate that our proposed methodology
facilitates training models that inherently deliver plausible
explanations while maintaining high predictive performance.
\keywords{Counterfactual Explanations \and Explainable
AI \and Representation Learning}
\end{abstract}

\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by advances in \textbf{representation learning}: instead of
relying on features and rules that are carefully hand-crafted by humans,
modern machine learning (ML) models are tasked with learning these
representations from scratch, guided by narrow objectives such as
predictive accuracy \cite{goodfellow2016deep}. Modern advances in
computing have made it possible to provide such models with ever greater
degrees of freedom to achieve that task, which has often led them to
outperform traditionally more parsimonious models. Unfortunately, in
doing so they also learn increasingly complex and highly sensitive
representations that we can no longer easily interpret.

This trend towards complexity for the sake of performance has come under
serious scrutiny in recent years. At the very cusp of the deep learning
revolution, \cite{szegedy2013intriguing} showed that artificial neural
networks (ANN) are sensitive to adversarial examples: counterfactuals of
model inputs that yield vastly different model predictions despite being
``imperceptible'' in that they are semantically indifferent from their
factual counterparts. Despite partially effective mitigation strategies
such as \textbf{adversarial training} \cite{goodfellow2014explaining},
truly robust deep learning (DL) remains unattainable even for models
that are considered shallow by today's standards
\cite{kolter2023keynote}.

Part of the problem is that high degrees of freedom provide room for
many solutions that are locally optimal with respect to narrow
objectives \cite{wilson2020case}\footnote{For clarity: we follow
  standard ML convention in using ``degrees of freedom'' to refer to the
  number of parameters estimated from data.}. Based purely on predictive
performance, these solutions may seem to provide compelling explanations
for the data, when in fact they are based on purely associative,
semantically meaningless patterns. This poses two related challenges:
firstly, it makes these models inherently opaque, since humans cannot
simply interpret what type of explanation the complex learned
representations correspond to; secondly, even if we could resolve the
first challenge, it is not obvious how to mitigate models from learning
representations that correspond to meaningless and implausible
explanations.

The first challenge has attracted an abundance of research on
\textbf{explainable AI} (XAI) which aims to develop tools to derive
explanations from complex model representations. This can mitigate a
scenario in which we deploy opaque models and blindly rely on their
predictions. On countless occasions, this scenario has already occurred
in practice and caused real harm to people who were affected adversely
and often unfairly by automated decision-making systems (ADMS) involving
opaque models \cite{oneil2016weapons}. Effective XAI tools can aide us
in monitoring models and providing recourse to individuals to turn
adverse outcomes (e.g.~``loan application rejected'') into positive ones
(``application accepted''). \cite{wachter2017counterfactual} propose
\textbf{counterfactual explanations} as an effective approach to achieve
this: they explain how factual inputs need to change in order for some
fitted model to produce some desired output, typically involving minimal
perturbations.

To our surprise, the second challenge has not yet attracted any
consolidated research effort. Specifically, there has been no concerted
effort towards improving model \textbf{explainability}, which we define
here as the degree to which learned representations correspond to
explanations that are interpretable and deemed \textbf{plausible} by
humans (see Definition~\ref{def-explainability}). Instead, the choice
has typically been to improve the capacity of XAI tools to identify the
subset explanations that are both plausible and valid for any given
model, independent of whether the learned representations are also
compatible with implausible explanations \cite{altmeyer2024faithful}.
Fortunately, recent findings indicate that explainability can arise as
byproduct of regularization techniques aimed at other objectives such as
robustness, generalization and generative capacity
\cite{schut2021generating}.

Building on these findings, we introduce \textbf{counterfactual
training}: a novel regularization technique geared explicitly towards
aligning model representations with plausible explanations. Our
contributions are as follows:

\begin{itemize}
\tightlist
\item
  We discuss existing related work on improving models and consolidate
  it through the lens of counterfactual explanations
  (Section~\ref{sec-lit}).
\item
  We present our proposed methodological framework that leverages
  faithful counterfactual explanations during the training phase of
  models to achieve the explainability objective
  (Section~\ref{sec-method}).
\item
  Through extensive experiments we demonstrate the counterfactual
  training improve model explainability while maintaining high
  predictive performance. We run ablation studies and grid searches to
  understand how the underlying model components and hyperparameters
  affect outcomes. (Section~\ref{sec-experiments}).
\end{itemize}

Despite limitations of our approach discussed in
Section~\ref{sec-discussion}, we conclude that counterfactual training
provides a practical framework for researchers and practitioners
interested in making opaque models more trustworthy
Section~\ref{sec-conclusion}. We also believe that this work serves as
an opportunity for XAI researchers to reevaluate the premise of
improving XAI tools without improving models.

\section{Related Literature}\label{sec-lit}

To the best of our knowledge, our proposed framework for counterfactual
training represents the first attempt to use counterfactual explanations
during training to improve model explainability. In high-level terms, we
define model explainability as the extent to which valid explanations
derived for an opaque model are also deemed plausible with respect to
the underlying data and stakeholder requirements. To make this more
concrete, we follow \cite{augustin2020adversarial} in tieing the concept
of explainability to the quality of counterfactual explanations that we
can generate for a given model. The authors show that counterfactual
explanations---understood here as minimal input perturbations that yield
some desired model prediction---are generally more meaningful if the
underlying model is more robust to adversarial examples. We can make
intuitive sense of this finding when looking at adversarial training
(AT) through the lens of representation learning with high degrees of
freedom: by inducing models to ``unlearn'' representations that are
susceptible to worst-case counterfactuals (i.e.~adversarial examples),
AT effectively removes some implausible explanations from the solution
space.

\subsection{Adversarial Examples are Counterfactual
Explanations}\label{adversarial-examples-are-counterfactual-explanations}

This interpretation of the link between explainability through
counterfactuals on one side, and robustness to adversarial examples on
the other, is backed by empirical evidence.
\cite{sauer2021counterfactual} demonstrate that using counterfactual
images during classifier training improves model robustness. Similarly,
\cite{abbasnejad2020counterfactual} argue that counterfactuals represent
potentially useful training data in machine learning, especially in
supervised settings where inputs may be reasonably mapped to multiple
outputs. They, too, demonstrate the augmenting the training data of
image classifiers can improve generalization. \cite{teney2020learning}
propose an approach using counterfactuals in training that does not rely
on data augmentation: they argue that counterfactual pairs typically
already exist in training datasets. Specifically, their approach relies
on, firstly, identifying similar input samples with different
annotations and, secondly, ensuring that the gradient of the classifier
aligns with the vector between pairs of counterfactual inputs using the
cosine distance as a loss function. In the natural language processing
(NLP) domain, counterfactuals have similarly been used to improve models
through data augmentation: \cite{wu2021polyjuice}, propose
\emph{POLYJUICE}, a general-purpose counterfactual generator for
language models. They demonstrate empirically that augmenting training
data through \emph{POLYJUICE} counterfactuals improves robustness in a
number of NLP tasks. \cite{luu2023counterfactual} introduce
Counterfactual Adversarial Training (CAT), which also aims at improving
generalization and robustness of language models. Specifically, they
propose to proceed as follows: firstly, they identify training samples
that are subject to high predictive uncertainty; secondly, they generate
counterfactual explanations for those samples; and, finally, they
fine-tune the given language model on the augmented dataset that
includes the generated counterfactuals.

There have also been several attempts at formalizing the relationship
between counterfactual explanations (CE) and adversarial examples (AE).
Pointing to clear similarities in how CE and AE are generated,
\cite{freiesleben2022intriguing} makes the case for jointly studying the
opaqueness and robustness problem in representation learning. Formally,
AE can be seen as the subset of CE, for which misclassification is
achieved \cite{freiesleben2022intriguing}. Similarly,
\cite{pawelczyk2022exploring} show that CE and AE are equivalent under
certain conditions and derive theoretical upper bounds on the distances
between them.

Two recent works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of post-hoc counterfactual explanations. Firstly,
\cite{ross2021learning} propose a way to train models that are
guaranteed to provide recourse for individuals to move from an adverse
outcome to some positive target class with high probability. The
approach proposed by \cite{ross2021learning} builds on adversarial
training, where in this context susceptibility to targeted adversarial
examples for the positive class is explicitly induced. The proposed
method allows for imposing a set of actionability constraints ex-ante:
for example, users can specify that certain features (e.g.~\emph{age},
\emph{gender}, \ldots) are immutable. Secondly, \cite{guo2023counternet}
are the first to propose an end-to-end training pipeline that includes
counterfactual explanations as part of the training procedure. In
particular, they propose a specific network architecture that includes a
predictor and CE generator network, where the parameters of the CE
generator network are learnable. Counterfactuals are generated during
each training iteration and fed back to the predictor network. In
contrast to \cite{guo2023counternet}, we impose no restrictions on the
neural network architecture at all.

\subsection{Beyond Robustness}\label{beyond-robustness}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a work
closely related to this one, \cite{altmeyer2024faithful} show that
explainability can be improved through model averaging and refined model
objectives. The authors propose a way to generate counterfactuals that
are maximally \textbf{faithful} to the model in that they are consistent
with what the model has learned about the underlying data. Formally,
they rely on tools from energy-based modelling to minimize the
divergence between the distribution of counterfactuals and the
conditional posterior over inputs learned by the model. Their proposed
counterfactual explainer, \emph{ECCCo}, yields plausible explanations if
and only if the underlying model has learned representations that align
with them. They find that both deep ensembles
\cite{lakshminarayanan2016simple} and joint energy-based models (JEMs)
\cite{grathwohl2020your} tend to do well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are most called for when
models are underspecified by the available data \cite{wilson2020case}.
Averaging across solutions mitigates the aforementioned risk of relying
on a single locally optimal representations that corresponds to
semantically meaningless explanations for the data. Previous work by
\cite{schut2021generating} similarly found that generating plausible
(``interpretable'') counterfactual explanations is almost trivial for
deep ensembles that have also undergone adversarial training. The case
for JEMs is even clearer: they involve a hybrid objective that induces
both high predictive performance and generative capacity
\cite{grathwohl2020your}. This is closely related to the idea of
aligning models with plausible explanations and has inspired our
proposed counterfactual training objective, as we explain in
Section~\ref{sec-method}.

\section{Counterfactual Training}\label{sec-method}

Counterfactual training combines ideas from adversarial training,
energy-based modelling and counterfactuals explanations with the
explicit objective of aligning representations with plausible
explanations that comply with user requirements. In the context of CE,
plausibility has broadly been defined as the degree to which
counterfactuals comply with the underlying data generating process
\cite{poyiadzi2020face, guidotti2022counterfactual, altmeyer2024faithful}.
Plausibility is a necessary but insufficient condition for using CE to
provide algorithmic recourse (AR) to individuals affected by opaque
models in practice. This is because for recourse recommendations to be
\textbf{actionable}, they need to not only result in plausible
counterfactuals but also be attainable. A plausible CE for a rejected
20-year-old loan applicant, for example, might reveal that their
application would have been accepted, if only they were 20 years older.
Ignoring all other features, this complies with the definition of
plausibility if 40-year-old individuals are in fact more credit-worthy
on average than young adults. But of course this CE does not qualify for
providing actionable recourse to the applicant since \emph{age} is not a
mutable feature. For our intents and purposes, counterfactual training
aims at improving model explainability by aligning models with
counterfactuals that meet both desiderata, plausibility and
actionability. Formally, we define explainability as follows:

\begin{definition}[Model
Explainability]\protect\hypertarget{def-explainability}{}\label{def-explainability}

Let \(\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}\) denote a
supervised classification model that maps from the \(D\)-dimensional
input space \(\mathcal{X}\) to representations
\(\phi(\mathbf{x};\theta)\) and finally to the \(K\)-dimensional output
space \(\mathcal{Y}\). Assume that for any given input-output pair
\(\{\mathbf{x},\mathbf{y}\}_i\) there exists a counterfactual
\(\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})\)
where \(\mathbf{y}^{+}\) denotes some target output. We say that
\(\mathbf{M}_\theta\) is \textbf{explainable} to the extent that
faithfully generated counterfactuals are plausible (i.e.~consistent with
the data) and actionable. Formally, we define these properties as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Plausibility)
  \(\int^{A} p(\mathbf{x}|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is some small region around \(\mathbf{x}^{\prime}\).
\item
  (Actionability) Permutations \(\Delta\) are subject to actionability
  constraints.
\end{enumerate}

We consider counterfactuals as faithful to the extent that they are
consistent with what the model has learned about the input data. Let
\(p_\theta(\mathbf{x}|\mathbf{y}^{+})\) denote the conditional posterior
over inputs, then formally:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  (Faithfulness)
  \(\int^{A} p_\theta(\mathbf{x}|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is defined as above.
\end{enumerate}

\end{definition}

The definitions of faithfulness and plausibility in
Definition~\ref{def-explainability} are the same as in
\cite{altmeyer2024faithful}, with adapted notation. Actionability
constraints in Definition~\ref{def-explainability} vary and depend on
the context in which \(\mathbf{M}_\theta\) is deployed. In this work, we
focus on domain and mutability constraints for individual features
\(x_d\) for \(d=1,...,D\). We limit ourselves to classification tasks
for reasons discussed in Section~\ref{sec-discussion}.

\subsection{Our Proposed Objective}\label{our-proposed-objective}

To train models with high explainability as defined in
Definition~\ref{def-explainability}, we propose the following objective,

\begin{equation}\phantomsection\label{eq-obj}{
\text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x},\mathbf{x^\prime},y;\theta) + \lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x^\prime}),\mathbf{y})
}\end{equation}

where \(\text{yloss}(\cdot)\) denotes any conventional classification
loss function (e.g.~cross-entropy) that induces discriminative
(predictive) performance. The two additional components in
Equation~\ref{eq-obj} are explained in more detail below. For now, they
can be sufficiently described as inducing explainability directly and
indirectly by penalizing: 1) the contrastive divergence,
\(\text{div}(\cdot)\), between counterfactuals \(x^\prime\) and observed
samples \(x\) and, 2) the adversarial loss, \(\text{advloss}(.)\), with
respect to counterfactuals. The tradeoff between the different
components can be governed by adjusting the strengths of the penalties
\(\lambda_{\text{div}}\) and \(\lambda_{\text{adv}}\).

\subsubsection{Directly Inducing Explainability through Contrastive
Divergence}\label{directly-inducing-explainability-through-contrastive-divergence}

\cite{grathwohl2020your} observe that any classifier can be
re-interpreted as a joint energy-based model (JEM) that learns to
discriminate output classes conditional on inputs and generate inputs.
They show that JEMs can be trained to perform well at both tasks by
directly maximizing the joint log-likelihood factorized as
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\).
The first factor can be optimized using conventional cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\)
\cite{grathwohl2020your} minimize the contrastive divergence between
samples drawn from \(p_\theta(\mathbf{x})\) and training observations,
i.e.~samples from \(p(\mathbf{x})\).

A key empirical finding in \cite{altmeyer2024faithful} was that JEMs
tend to do well with respect to the plausibility objective in
Definition~\ref{def-explainability}. If we consider samples drawn from
\(p_\theta(\mathbf{x})\) as counterfactuals, this is an expected
finding, because the JEM objective effectively minimizes the divergence
between the conditional posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\).
To generate samples, \cite{grathwohl2020your} rely on Stochastic
Gradient Langevin Dynamics (SGLD) using an uninformative prior for
initialization. This is where we depart from their methodology: instead
of generating samples through SGLD, we propose using counterfactual
explainers to generate counterfactuals for observed training samples.
Specifically, we have

\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x},\mathbf{x^\prime},y;\theta) = \mathcal{E}_\theta(\mathbf{x},y) - \mathcal{E}_\theta(\mathbf{x}^\prime,y)
}\end{equation}

where \(\mathcal{E}_\theta(\cdot)\) denotes the energy function. We
generate samples \(\mathbf{x}^\prime\) by first randomly sampling the
target class \(y^+ \sim p(y)\) and then generating a counterfactual
explanation for that target, similar to how conditional sampling is used
to draw from \(p_\theta(\mathbf{x})\) in \cite{grathwohl2020your}. In
particular, we set
\(\mathcal{E}_\theta(\mathbf{x},\mathbf{y})=-\mathbf{M}_\theta(\mathbf{x})[y^+]\)
where \(y^+\) denotes the index of the target class.

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while at same time
increasing the energy of counterfactuals (negative samples)
\cite{du2019implicit}. As the generated counterfactuals get more
plausible (Definition~\ref{def-explainability}) over the cause of
training, these two opposing effects gradually balance each out
\cite{lippe2024uvadlc}.

The departure from SGLD allows us to tap into the vast repertoire of
explainers that have been proposed in the literature to meet different
desiderata. Typically, these methods facilitate the imposition of domain
and mutability constraints, for example. In principle, any existing
approach for generating counterfactual explanations is viable, so long
as it does not violate the faithfulness condition. Like JEMs
\cite{murphy2022probabilistic}, counterfactual training can be
considered as a form of contrastive representation learning.

\subsubsection{Indirectly Inducing Explainability through Adversarial
Robustness}\label{indirectly-inducing-explainability-through-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
\cite{luu2023counterfactual} or adversarial examples
\cite{freiesleben2022intriguing, pawelczyk2022exploring}. This leaves
some flexibility with respect to the exact choice for
\(\text{advloss}(\cdot)\) in Equation~\ref{eq-obj}. An intuitive
functional form to use, though likely not the only reasonable choice, is
inspired by adversarial training:

\begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x^\prime}),\mathbf{y};\varepsilon)&=\begin{cases}
\text{yloss}(\mathbf{M}_\theta(\mathbf{x^\prime}),\mathbf{y}) & \text{if} \ ||\Delta||_\infty \leq \varepsilon \\
0 & \text{otherwise.}
\end{cases}
\end{aligned}
}\end{equation}

Under this choice we treat the counterfactual \(\mathbf{x}^\prime\) as
an adversarial example iff it is imperceptible, i.e.~the magnitude of
the perturbation of any individual feature is upper-bounded at
\(\varepsilon\).

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any counterfactual explainer that relies on gradient
descent in the feature space for optimization
\cite{altmeyer2023explaining}. In this context, domain constraints can
be imposed by simply projecting counterfactuals back to the specified
domain, if the previous gradient step resulted in updated feature values
that were out-of-domain. Mutability constraints can similarly be
enforced by setting partial derivatives to zero to ensure that features
are only mutated in the allowed direction, if at all.

Since actionability constraints are binding at test time, we should also
impose them when generating \(\mathbf{x}^\prime\) during each training
iteration to align model representations with user requirements. Through
their effect on \(\mathbf{x}^\prime\), both types of constraints
influence model outcomes through Equation~\ref{eq-div}. Here it is
crucial that we avoid penalizing implausibility that arises due to
mutability constraints. For any mutability-constrained feature \(d\)
this can be achieved by enforcing
\(\mathbf{x}[d] - \mathbf{x}^\prime[d]:=0\) whenever perturbing
\(\mathbf{x}^\prime[d]\) in the direction of \(\mathbf{x}[d]\) would
violate mutability constraints. Specifically, we set
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) if

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  We have \(\mathbf{x}[d]>\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be decreased in practice.
\item
  We have \(\mathbf{x}[d]<\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be increased in practice.
\end{enumerate}

From a Bayesian perspective, setting
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) can be understood as assuming
a point mass prior for \(p(\mathbf{x})\) with respect to feature \(d\).
Intuitively, we think of this simply in terms ignoring implausibility
costs with respect to immutable features, which effectively forces the
model to instead seek plausibility with respect to the remaining
features. This in turn results in lower overall sensitivity to immutable
features, which we demonstrate empirically for different classifiers in
Section~\ref{sec-experiments}. Under certain conditions, this results
holds theoretically{[}For the proof, see the supplementary appendix.{]}:

\begin{theorem}[Protecting Immutable
Features]\protect\hypertarget{thm-mtblty}{}\label{thm-mtblty}

Let
\(f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})\)
denote a linear classifier with softmax activation \(\mathcal{S}\)
(i.e.~\emph{multinomial logistic regression}) where
\(y\in\{1,...,K\}=\mathcal{K}\) and \(\mathbf{x} \in \mathbb{R}^D\). If
we assume multivariate Gaussian class densities with common diagonal
covariance matrix \(\Sigma_k=\Sigma\) for all \(k \in \mathcal{K}\),
then protecting an immutable feature from the contrastive divergence
penalty (Equation~\ref{eq-div}) will result in lower classifier
sensitivity to that feature relative to the remaining features, provided
that at least one of those is mutable.

\end{theorem}

It is worth highlighting that Theorem~\ref{thm-mtblty} assumes
independence of features. This raises a valid concern about the effect
of protecting immutable features in the presence of proxy features that
remain unprotected. We discuss this limitation in
Section~\ref{sec-discussion}.

\subsection{Illustration}\label{illustration}

To better convey the intuition underlying our proposed method, we
illustrate different model outcomes in Example~\ref{exm-grad}.

\begin{example}[Prediction of Consumer Credit
Default]\protect\hypertarget{exm-grad}{}\label{exm-grad}

Suppose we are interested in predicting the likelihood that loan
applicants default on their credit. We have access to historical data on
previous loan takers comprised of a binary outcome variable
(\(y\in\{1=\text{default},2=\text{no default}\}\)) two input features:
1) the subjects' \emph{age}, which we define as immutable, and 2) the
subjects' existing level of \emph{debt}, which we define as mutable.

We have simulated this scenario using synthetic data with independent
features and Gaussian class-conditional densities in
Figure~\ref{fig-poc}. The four panels in Figure~\ref{fig-poc} show the
outcomes for different training procedures using the same model
architecture each time (a linear classifier). In each case, we show the
linear decision boundary (green) and the training data colored according
to their ground-truth label: orange points belong to the target class,
\(y^+=2\), blue points belong to the non-target class, \(y^-=1\). Stars
indicate counterfactuals in the target class generated at test time
using generic gradient descent for a fixed number of iterations.

In panel (a), we have trained our model conventionally, and we do not
impose mutability constraints at test time. The generated
counterfactuals are all valid, but not plausible: they are clearly
distinguishable from the ground-truth data. In panel (b), we have
trained our model with counterfactual training, once again not imposing
mutability constraints at test time. We observe that the counterfactuals
are clearly plausible, therefore meeting the first objective of
Definition~\ref{def-explainability}.

In panel (c), we have used conventional training again, this time
imposing the mutability constraint on \emph{age} at test time.
Counterfactuals are valid but involve some substantial reductions in
\emph{debt} for some individuals. By comparison, counterfactual paths
are shorter on average in panel (d), where we have used counterfactual
training and protected immutable features as described in
Section~\ref{sec-constraints}. The counterfactuals are also plausible
with respect to the mutable feature. Thus, we consider the model in
panel (d) as the most explainable according to
Definition~\ref{def-explainability}.

\end{example}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{body_ecml_files/mediabag/../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Visual illustration of how counterfactual
training improves explainability. See Example~\ref{exm-grad} for
details.}

\end{figure}%

\section{Experiments}\label{sec-experiments}

In this section, we present experiments that we have conducted in order
to answer the following research questions:

\begin{corollary}[Plausibility]\protect\hypertarget{cor-plaus}{}\label{cor-plaus}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) induce models to learn plausible explanations?

\end{corollary}

\begin{corollary}[Actionability]\protect\hypertarget{cor-action}{}\label{cor-action}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) yield more favorable algorithmic recourse
outcomes in the presence of actionability constraints?

\end{corollary}

Beyond this, we are also interested in understanding how robust our
answers to RQ~\ref{cor-plaus} and RQ~\ref{cor-action} are:

\begin{corollary}[Hyperparameters]\protect\hypertarget{cor-hyper}{}\label{cor-hyper}

What are the effects of different hyperparameter choices with respect to
Equation~\ref{eq-obj}?

\end{corollary}

\subsection{Experimental Setup}\label{experimental-setup}

\subsection{Experimental Results}\label{experimental-results}

\section{Discussion}\label{sec-discussion}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Limited to classification models.
\item
  Proxy attributes of immutable features.
\end{enumerate}

\section{Conclusion}\label{sec-conclusion}
