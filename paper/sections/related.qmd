# Related Literature {#sec-lit}

To make the desiderata for our framework more concrete, we follow @augustin2020adversarial in tying the concept of explainability to the quality of CEs that can be generated for a given model. The authors show that CEs---understood as minimal input perturbations that yield some desired model prediction---tend to be more meaningful if the underlying model is more robust to adversarial examples. We can make intuitive sense of this finding when looking at adversarial training (AT) through the lens of representation learning with high degrees of freedom. As argued before, learned representations may be sensitive to producing implausible explanations and mispredicting for worst-case counterfactuals (i.e., AEs). Thus, by inducing models to "unlearn" susceptiblity to such examples, adversarial training can effectively remove implausible explanations from the solution space.

## Adversarial Examples are Counterfactual Explanations

This interpretation of the link between explainability through counterfactuals on one side and robustness to adversarial examples on the other is backed by empirical evidence. @sauer2021counterfactual demonstrates that using counterfactual images during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argues that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, demonstrate that augmenting the training data of image classifiers can improve generalization. Finally, @teney2020learning proposes an approach using counterfactuals in training that does not rely on data augmentation: they argue that counterfactual pairs typically already exist in training datasets. Specifically, their approach relies on identifying similar input samples with different annotations and ensuring that the gradient of the classifier aligns with the vector between such pairs of counterfactual inputs using the cosine distance as the loss function. 

In the natural language processing (NLP) domain, CEs have also been used to improve models through data augmentation. @wu2021polyjuice2 proposes *Polyjuice*, a general-purpose counterfactual generator for language models. The authors empirically demonstrate that the augmentation of training data with their method improves robustness in a number of NLP tasks. @balashankar2023improving similarly uses *Polyjuice* to augment NLP datasets through diverse counterfactuals and show that classifier robustness improves by up to 20%. Finally, @luu2023counterfactual introduces Counterfactual Adversarial Training (CAT) that also aims to improve generalization and robustness of language models through a three-step procedure: the authors identify training samples that are subject to high predictive uncertainty, generate CEs for them, and fine-tune the language model on a dataset augmented with the CEs. 

There have also been several attempts at formalizing the relationship between counterfactual explanations and adversarial examples. Pointing to clear similarities in how CEs and AEs are generated, @freiesleben2022intriguing makes the case for jointly studying the opaqueness and robustness problems in representation learning. Formally, AEs can be seen as the subset of CEs for which misclassification is achieved [@freiesleben2022intriguing]. Similarly, @pawelczyk2022exploring shows that CEs and AEs are equivalent under certain conditions. 

Two recent works are closely related to ours in that they use counterfactuals during training with the explicit goal of affecting certain properties of the post-hoc counterfactual explanations. Firstly, @ross2021learning proposes a way to train models that guarantee individual recourse to some positive target class with high probability. Their approach builds on adversarial training by explicitly inducing susceptibility to targeted adversarial examples for the positive class. Additionally, the proposed method allows for imposing a set of actionability constraints ex-ante. For example, users can specify that certain features are immutable. Secondly, @guo2023counternet is the first to propose an end-to-end training pipeline that includes CEs as part of the training procedure. In particular, they propose a specific network architecture that includes a predictor and CE generator network, where the parameters of the CE generator network are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor network. In contrast to @guo2023counternet, we impose no restrictions on the ANN architecture at all. 

## Aligning Representations with Plausible Explanations 

Improving the adversarial robustness of models is not the only path towards aligning representations with plausible explanations. In a work closely related to this one, @altmeyer2024faithful shows that explainability can be improved through model averaging and refined model objectives. The authors propose a way to generate counterfactuals that are maximally faithful to the model in that they are consistent with what the model has learned about the underlying data. Formally, they rely on tools from energy-based modelling [@teh2003energy] to minimize the divergence between the distribution of counterfactuals and the conditional posterior over inputs learned by the model. Their proposed counterfactual explainer, *ECCCo*, yields plausible explanations if and only if the underlying model has learned representations that align with them. The authors find that both deep ensembles [@lakshminarayanan2016simple] and joint energy-based models (JEMs) [@grathwohl2020your] tend to do well in this regard. 

Once again it helps to look at these findings through the lens of representation learning with high degrees of freedom. Deep ensembles are approximate Bayesian model averages, which are most called for when models are underspecified by the available data [@wilson2020case]. Averaging across solutions mitigates the aforementioned risk of relying on a single locally optimal representations that corresponds to semantically meaningless explanations for the data. Previous work of @schut2021generating similarly found that generating plausible ("interpretable") CEs is almost trivial for deep ensembles that have also undergone adversarial training. The case for JEMs is even clearer: they involve a hybrid objective that induces both high predictive performance and generative capacity [@grathwohl2020your]. This is closely related to the idea of aligning models with plausible explanations and has inspired our CT objective.