{
  "hash": "ce3e3feeaddddc6681b6fc21729509c0",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  arxiv-pdf:\n    keep-tex: true  \n    linenumbers: true\n    doublespacing: false\n    runninghead: \"counterfactual training (A Preprint)\"\n    authorcols: true\n    include-in-header:\n      text: |\n        \\usepackage{amsthm}\n        \\theoremstyle{plain}\n        \\newtheorem{proposition}{Proposition}[section]\n        \\theoremstyle{definition}\n        \\newtheorem{definition}{Definition}[section]\n        \\theoremstyle{definition}\n        \\newtheorem{example}{Example}[section]\n        \\theoremstyle{plain}\n---\n\n\n# Abstract\n\nWe propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-word decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the stakeholder requirements. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals ad-hoc during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable explanations while maintaining high predictive performance.\n\n\n\n# Introduction\n\nToday's prominence of artificial intelligence (AI) has largely been driven by **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern machine learning (ML) models are tasked with learning representations directly from data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such models with ever-growing degrees of freedom to achieve that task, which frequently allows them to outperform traditionally more parsimonious models. Unfortunately, in doing so, models learn increasingly complex and highly sensitive representations that humans can no longer easily interpret.\n\nThe trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are sensitive to adversarial examples: perturbed versions of data instances that yield vastly different model predictions despite being \"imperceptible\" in that they are semantically indifferent from their factual counterparts. Even though some partially effective mitigation strategies have been proposed---most notably **adversarial training** [@goodfellow2014explaining]---truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. \n\nPart of the problem is that the high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case].^[We follow the standard ML convention, where \"degrees of freedom\" refer to the number of parameters estimated from data.] Indeed, recent work on the so-called \"lottery ticket hypothesis\" suggests that modern neural networks can be pruned by up to 90% while preserving their predictive performance [@frankle2018lottery] and generalizability [@morcos2019success]. Similarly, @zhang2021understanding showed that state-of-the-art neural networks are so expressive that they can fit randomly labeled data. Thus, looking at the predictive performance alone, the solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two related challenges. Firstly, there is no dependable way to verify if such complex representations correspond to meaningful and plausible explanations. Secondly, even if we could resolve the first challenge, it remains undecided how to ensure that models can *only* learn valuable explanations. \n\nThe first challenge has attracted an abundance of research on **explainable AI** (XAI), a paradigm that focuses on the development of tools to derive (post-hoc) explanations from complex model representations. Such explanations should mitigate a scenario in which practitioners deploy opaque models and blindly rely on their predictions. On countless occasions, this has happened in practice and caused real harms to people who were adversely and unfairly affected by automated decision-making (ADM) systems involving opaque models [@oneil2016weapons; @mcgregor2021preventing]. Effective XAI tools can aid us in monitoring models and providing recourse to individuals to turn negative outcomes (e.g., \"loan application rejected\") into positive ones (e.g., \"application accepted\"). Our work builds upon **counterfactual explanations** (CE) proposed by @wachter2017counterfactual as an effective approach to achieve this goal. CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce a desired output.\n\nTo our surprise, the second challenge has not yet attracted major research interest. Specifically, there has been no concerted effort towards improving the \"explanatory capacity\" of models, i.e., the degree to which learned representations correspond to explanations that are **interpretable** and deemed **plausible** by humans (see Def. \\ref{def-explainability}). Instead, the choice has generally been to improve the ability of XAI tools to identify the subset of explanations that are both plausible and valid for any given model, independent of whether the learned representations are also compatible with plausible explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that improved explanatory capacity can arise as a consequence of regularization techniques aimed at other training objectives such as robustness, generalization, and generative capacity [@schut2021generating; @augustin2020adversarial; @altmeyer2024faithful]. As further discussed in @sec-lit, our work consolidates these findings within a single objective.\n\n**Specifically, we introduce counterfactual training (CT)**: a novel training regime explicitly meant to align learned representations with plausible explanations that comply with user requirements. The remainder of this paper is structured as follows. @sec-lit presents related work, focusing in particular on the link between adversarial examples and counterfactual explanations. Then follow our main contributions:\n\n1. In @sec-method, we introduce our methodological framework and show theoretically that it can be used to enforce global actionability constraints.  \n2. Through extensive experiments we demonstrate that CT substantially improves explainability without sacrificing predictive performance (@sec-experiments).  \n\nWe discuss future research and challenges in @sec-discussion and conclude in @sec-conclusion that CT is a promising new approach towards making opaque models more trustworthy.\n\n\n\n\n# Related Literature {#sec-lit}\n\nTo the best of our knowledge, the proposed framework for counterfactual training represents the first attempt to use counterfactual explanations during training to improve model explainability. In high-level terms, we define model explainability as the extent to which valid explanations derived for an opaque model are also deemed plausible with respect to the underlying data and stakeholder requirements; the former means that the counterfactuals should comply with the distribution of the factual data, the latter means that they should respect arbitrary (global) actionability constraints. To make the desiderata for our framework more concrete, we follow @augustin2020adversarial in tying the concept of explainability to the quality of counterfactual explanations that we can generate for a given model. The authors show that CEs---understood here as minimal input perturbations that yield some desired model prediction---are generally more meaningful if the underlying model is more robust to adversarial examples. We can make intuitive sense of this finding when looking at adversarial training (AT) through the lens of representation learning with high degrees of freedom. As argued before, learned representations may be sensitive to producing implausible explanations and mispredicting for worst-case counterfactuals (i.e., adversarial examples). Thus, by inducing models to \"unlearn\" susceptiblity to such examples, AT can effectively remove implausible explanations from the solution space.\n\n## Adversarial Examples are Counterfactual Explanations\n\nThis interpretation of the link between explainability through counterfactuals on one side and robustness to adversarial examples on the other is backed by empirical evidence. @sauer2021counterfactual demonstrate that using counterfactual images during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, demonstrate that augmenting the training data of image classifiers can improve generalization. Finally, @teney2020learning propose an approach using counterfactuals in training that does not rely on data augmentation: they argue that counterfactual pairs typically already exist in training datasets. Specifically, their approach relies on identifying similar input samples with different annotations and ensuring that the gradient of the classifier aligns with the vector between such pairs of counterfactual inputs using the cosine distance as the loss function. \n\nIn the natural language processing (NLP) domain, counterfactuals have similarly been used to improve models through data augmentation. @wu2021polyjuice propose *Polyjuice*, a general-purpose counterfactual generator for language models. They demonstrate empirically that the augmentation of training data through *Polyjuice* counterfactuals improves robustness in a number of NLP tasks. @balashankar2023improving similarly use *Polyjuice* to augment NLP datasets through diverse counterfactuals and show that classifier robustness improves by up to 20%. Finally, @luu2023counterfactual introduce Counterfactual Adversarial Training (CAT), which also aims at improving generalization and robustness of language models through a three-step procedure. First, the authors identify training samples that are subject to high predictive uncertainty. Second, they generate counterfactual explanations for those samples. Finally, they fine-tune the given language model on the augmented dataset that includes the generated counterfactuals. \n\nThere have also been several attempts at formalizing the relationship between counterfactual explanations and adversarial examples (AE). Pointing to clear similarities in how CEs and AEs are generated, @freiesleben2022intriguing makes the case for jointly studying the opaqueness and robustness problems in representation learning. Formally, AEs can be seen as the subset of CEs for which misclassification is achieved [@freiesleben2022intriguing]. Similarly, @pawelczyk2022exploring show that CEs and AEs are equivalent under certain conditions and derive theoretical upper bounds on distances between them. \n\nTwo recent works are closely related to ours in that they use counterfactuals during training with the explicit goal of affecting certain properties of the post-hoc counterfactual explanations. Firstly, @ross2021learning propose a way to train models that guarantee individual recourse to some positive target class with high probability. Their approach builds on adversarial training by explicitly inducing susceptibility to targeted adversarial examples for the positive class. Additionally, the proposed method allows for imposing a set of actionability constraints ex-ante. For example, users can specify that certain features (e.g., *age*, *gender*) are immutable. Secondly, @guo2023counternet are the first to propose an end-to-end training pipeline that includes counterfactual explanations as part of the training procedure. In particular, they propose a specific network architecture that includes a predictor and CE generator network, where the parameters of the CE generator network are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor network. In contrast to @guo2023counternet, we impose no restrictions on the neural network architecture at all. \n\n## Beyond Robustness \n\nImproving the adversarial robustness of models is not the only path towards aligning representations with plausible explanations. In a work closely related to this one, @altmeyer2024faithful show that explainability can be improved through model averaging and refined model objectives. The authors propose a way to generate counterfactuals that are maximally faithful to the model in that they are consistent with what the model has learned about the underlying data. Formally, they rely on tools from energy-based modelling to minimize the divergence between the distribution of counterfactuals and the conditional posterior over inputs learned by the model. Their proposed counterfactual explainer, *ECCCo*, yields plausible explanations if and only if the underlying model has learned representations that align with them. The authors find that both deep ensembles [@lakshminarayanan2016simple] and joint energy-based models (JEMs) [@grathwohl2020your] tend to do well in this regard. \n\nOnce again it helps to look at these findings through the lens of representation learning with high degrees of freedom. Deep ensembles are approximate Bayesian model averages, which are most called for when models are underspecified by the available data [@wilson2020case]. Averaging across solutions mitigates the aforementioned risk of relying on a single locally optimal representations that corresponds to semantically meaningless explanations for the data. Previous work by @schut2021generating similarly found that generating plausible (\"interpretable\") counterfactual explanations is almost trivial for deep ensembles that have also undergone adversarial training. The case for JEMs is even clearer: they involve a hybrid objective that induces both high predictive performance and generative capacity [@grathwohl2020your]. This is closely related to the idea of aligning models with plausible explanations and has inspired our proposed CT objective, as we explain in @sec-method.\n\n\n\n# Counterfactual Training {#sec-method}\n\nCounterfactual training combines ideas from adversarial training, energy-based modelling and counterfactuals explanations with the explicit goal of aligning representations with plausible explanations that comply with user requirements. In the context of CEs, plausibility has broadly been defined as the degree to which counterfactuals comply with the underlying data-generating process [@poyiadzi2020face;@guidotti2022counterfactual;@altmeyer2024faithful]. Plausibility is a necessary but insufficient condition for using CEs to provide algorithmic recourse (AR) to individuals (negatively) affected by opaque models. For AR recommendations to be actionable, they need to not only result in plausible counterfactuals but also be attainable. A plausible CE for a rejected 20-year-old loan applicant, for example, might reveal that their application would have been accepted, if only they were 20 years older. Ignoring all other features, this would comply with the definition of plausibility if 40-year-old individuals were in fact more credit-worthy on average than young adults. But of course this CE does not qualify for providing actionable recourse to the applicant since *age* is not a (directly) mutable feature. CT aims to improve model explainability by aligning models with counterfactuals that meet both desiderata: plausibility and actionability. Formally, we define explainability as follows:\n\n\\begin{definition}[Model Explainability]\n\\label{def-explainability}\nLet $\\mathbf{M}_\\theta: \\mathcal{X} \\mapsto \\mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\\mathcal{X}$ to representations $\\phi(\\mathbf{x};\\theta)$ and finally to the $K$-dimensional output space $\\mathcal{Y}$. Assume that for any given input-output pair $\\{\\mathbf{x},\\mathbf{y}\\}_i$ there exists a counterfactual $\\mathbf{x}^{\\prime} = \\mathbf{x} + \\Delta: \\mathbf{M}_\\theta(\\mathbf{x}^{\\prime}) = \\mathbf{y}^{+} \\neq \\mathbf{y} = \\mathbf{M}_\\theta(\\mathbf{x})$ where $\\arg\\max_y{\\mathbf{y}^{+}}=y^+$ and $y^+$ denotes the index of the target class. \n\nWe say that $\\mathbf{M}_\\theta$ is \\textbf{explainable} to the extent that faithfully generated counterfactuals are plausible and actionable. Formally, we define these properties as follows,\n\n\\begin{enumerate}\n    \\item (Plausibility) $\\int^{A} p(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$ where $A$ is some small region around $\\mathbf{x}^{\\prime}$.\n    \\item (Actionability) Permutations $\\Delta$ are subject to some actionability constraints.\n    \\item (Faithfulness) $\\int^{A} p_\\theta(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$ where $A$ is defined as above.\n\\end{enumerate}\n\nwhere $p_\\theta(\\mathbf{x}|\\mathbf{y}^{+})$ denotes the conditional posterior over inputs. \n\\end{definition}\n\nThe characterization of faithfulness and plausibility in Def. \\ref{def-explainability} is the same as in @altmeyer2024faithful, with adapted notation. Intuitively, plausible counterfactuals are consistent with the data and faithful counterfactuals are consistent with what the model has learned about input data. Actionability constraints in Def. \\ref{def-explainability} vary and depend on the context in which $\\mathbf{M}_\\theta$ is deployed. In this work, we focus on domain and mutability constraints for individual features $x_d$ for $d=1,...,D$. We limit ourselves to classification tasks for reasons discussed in @sec-discussion.\n\n## Our Proposed Objective\n\nLet $\\mathbf{x}_t^\\prime$ for $t=0,...,T$ denote a counterfactual explanation generated through gradient descent over $T$ iterations as initially proposed by @wachter2017counterfactual. For our purposes, we let $T$ vary and consider the counterfactual search as converged as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$, where $\\mathcal{S}$ is the softmax function.^[For detailed background information on gradient-based counterfactual search and convergence see supplementary appendix.] \n\n<!-- @sec-app-ce -->\n\nTo train models with high explainability as defined in Def. \\ref{def-explainability}, we propose to leverage counterfactuals in the following objective:\n\n$$\n\\begin{split}\n\\min_\\theta \\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}),\\mathbf{y}) + \\lambda_{\\text{div}} \\text{div}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta) &+ \\lambda_{\\text{adv}} \\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y}) \\\\ &+ \\lambda_{\\text{reg}}\\text{ridge}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta)\n\\end{split}\n$$ {#eq-obj}\nwhere $\\text{yloss}(\\cdot)$ is a classification loss that induces discriminative performance (e.g., cross-entropy). The second and third terms in @eq-obj are explained in detail below. For now, they can be sufficiently described as inducing explainability directly and indirectly by penalizing: (1) the contrastive divergence, $\\text{div}(\\cdot)$, between mature counterfactuals $\\mathbf{x}_T^\\prime$ and observed samples $\\mathbf{x}^+\\in\\mathcal{X}^+=\\{\\mathbf{x}:y=y^+\\}$ in the target class $y^+$, and, (2) the adversarial loss, $\\text{advloss}(.)$, with respect to nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$. Finally, $\\text{ridge}(\\cdot)$ denotes a Ridge penalty ($\\ell_2$-norm) that regularizes the magnitude of the energy terms involved in $\\text{div}(\\cdot)$ [@du2019implicit]. The trade-off between the components can be governed by adjusting the strengths of the penalties $\\lambda_{\\text{div}}$, $\\lambda_{\\text{adv}}$ and $\\lambda_{\\text{reg}}$.\n\n## Directly Inducing Explainability with Contrastive Divergence\n\n@grathwohl2020your observe that any classifier can be re-interpreted as a joint energy-based model (JEM) that learns to discriminate output classes conditional on the observed (training) samples from $p(\\mathbf{x})$ and the generated samples from $p_\\theta(\\mathbf{x})$. The authors show that JEMs can be trained to perform well at both tasks by directly maximizing the joint log-likelihood factorized as $\\log p_\\theta(\\mathbf{x},\\mathbf{y})=\\log p_\\theta(\\mathbf{y}|\\mathbf{x}) + \\log p_\\theta(\\mathbf{x})$. The first term can be optimized using conventional cross-entropy as in @eq-obj. Then, to optimize $\\log p_\\theta(\\mathbf{x})$ @grathwohl2020your minimize the contrastive divergence between these observed samples from $p(\\mathbf{x})$ and generated samples from $p_\\theta(\\mathbf{x})$. \n\nA key empirical finding in @altmeyer2024faithful was that JEMs tend to do well with respect to the plausibility objective in Def. \\ref{def-explainability}. This follows directly if we consider samples drawn from $p_\\theta(\\mathbf{x})$ as counterfactuals because the JEM objective effectively minimizes the divergence between the conditional posterior and $p(\\mathbf{x}|\\mathbf{y}^{+})$. To generate samples, @grathwohl2020your rely on Stochastic Gradient Langevin Dynamics (SGLD) using an uninformative prior for initialization but we depart from their methodology. Instead of SGLD, we propose to use counterfactual explainers to generate counterfactuals of observed training samples. Specifically, we have:\n\n$$\n\\text{div}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta) = \\mathcal{E}_\\theta(\\mathbf{x}^+,y) - \\mathcal{E}_\\theta(\\mathbf{x}_T^\\prime,y)\n$$ {#eq-div}\nwhere $\\mathcal{E}_\\theta(\\cdot)$ denotes the energy function. We set $\\mathcal{E}_\\theta(\\mathbf{x},y)=-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]$ where $y^+$ denotes the index of the randomly drawn target class, $y^+ \\sim p(y)$. Conditional on the target class $y^+$, $\\mathbf{x}_T^\\prime$ denotes a mature counterfactual for a randomly sampled factual from a non-target class generated with a gradient-based CE generator for up to $T$ iterations. Mature counterfactuals are ones that have either reached convergence wrt. the decision threshold $\\tau$ or exhausted $T$.\n\nIntuitively, the gradient of @eq-div decreases the energy of observed training samples (positive samples) while increasing the energy of counterfactuals (negative samples) [@du2019implicit]. As the counterfactuals get more plausible (Def. \\ref{def-explainability}) during training, these opposing effects gradually balance each other out [@lippe2024uvadlc]. \n\nThe departure from SGLD allows us to tap into the vast repertoire of explainers that have been proposed in the literature to meet different desiderata. For example, many methods facilitate the imposition of domain and mutability constraints. In principle, any existing approach for generating counterfactual explanations is viable, so long as it does not violate the faithfulness condition. Like JEMs [@murphy2022probabilistic], CT can be considered a form of contrastive representation learning. \n\n## Indirectly Inducing Explainability with Adversarial Robustness\n\nBased on our analysis in @sec-lit, counterfactuals $\\mathbf{x}^\\prime$ can be repurposed as additional training samples [@luu2023counterfactual;@balashankar2023improving] or AEs [@freiesleben2022intriguing;@pawelczyk2022exploring]. This leaves some flexibility with respect to the choice for $\\text{advloss}(\\cdot)$ in @eq-obj. An intuitive functional form, but likely not the only sensible choice, is inspired by adversarial training:\n\n$$\n\\begin{aligned}\n\\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y};\\varepsilon)&=\\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t_\\varepsilon}^\\prime),\\mathbf{y}) \\\\\nt_\\varepsilon &= \\max_t \\{t: ||\\Delta_t||_\\infty < \\varepsilon\\}\n\\end{aligned}\n$$ {#eq-adv}\nUnder this choice, we consider nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$ as AEs as long as the magnitude of the perturbation to any single feature is at most $\\varepsilon$. This is closely aligned with @szegedy2013intriguing who define an adversarial attack as an \"imperceptible non-random perturbation\". Thus, we choose to work with a different distinction between CE and AE than @freiesleben2022intriguing who consider misclassification as the key distinguishing feature of AE. One of the key observations in this work is that we can leverage CEs during training and get adversarial examples essentially for free. \n\n## Encoding Actionability Constraints {#sec-constraints}\n\nMany existing counterfactual explainers support domain and mutability constraints out-of-the-box. In fact, both types of constraints can be implemented for any counterfactual explainer that relies on gradient descent in the feature space for optimization [@altmeyer2023explaining]. In this context, domain constraints can be imposed by simply projecting counterfactuals back to the specified domain, if the previous gradient step resulted in updated feature values that were out-of-domain. Mutability constraints can similarly be enforced by setting partial derivatives to zero to ensure that features are only perturbed in the allowed direction, if at all. \n\nSince such actionability constraints are binding at test time, we should also impose them when generating $\\mathbf{x}^\\prime$ during each training iteration to inform model representations. Through their effect on $\\mathbf{x}^\\prime$, both types of constraints influence model outcomes via @eq-div. Here it is crucial that we avoid penalizing implausibility that arises due to mutability constraints. For any mutability-constrained feature $d$ this can be achieved by enforcing $\\mathbf{x}^+[d] - \\mathbf{x}^\\prime[d]:=0$ whenever perturbing $\\mathbf{x}^\\prime[d]$ in the direction of $\\mathbf{x}^+[d]$ would violate mutability constraints. Specifically, we set $\\mathbf{x}^+[d] := \\mathbf{x}^\\prime[d]$ if:\n\n1. Feature $d$ is strictly immutable in practice.\n2. We have $\\mathbf{x}^+[d]>\\mathbf{x}^\\prime[d]$, but feature $d$ can only be decreased in practice.\n3. We have $\\mathbf{x}^+[d]<\\mathbf{x}^\\prime[d]$, but feature $d$ can only be increased in practice.\n\nFrom a Bayesian perspective, setting $\\mathbf{x}^+[d] := \\mathbf{x}^\\prime[d]$ can be understood as assuming a point mass prior for $p(\\mathbf{x}^+)$ with respect to feature $d$. Intuitively, we think of this simply in terms ignoring implausibility costs with respect to immutable features, which effectively forces the model to instead seek plausibility with respect to the remaining features. This in turn results in lower overall sensitivity to immutable features, which we demonstrate empirically for different classifiers in @sec-experiments. Under certain conditions, this results holds theoretically:^[For the proof, see the supplementary appendix.]\n\n\\begin{proposition}[Protecting Immutable Features]\n\\label{prp-mtblty}\nLet $f_\\theta(\\mathbf{x})=\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}))=\\mathcal{S}(\\Theta\\mathbf{x})$ denote a linear classifier with softmax activation $\\mathcal{S}$ where $y\\in\\{1,...,K\\}=\\mathcal{K}$ and $\\mathbf{x} \\in \\mathbb{R}^D$. If we assume multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.\n\\end{proposition}\n\nIt is worth highlighting that Prp.~\\ref{prp-mtblty} assumes independence of features. This raises a valid concern about the effect of protecting immutable features in the presence of proxies that remain unprotected. We address this in @sec-discussion.\n\n## Example (Prediction of Consumer Credit Default)\n\nSuppose we are interested in predicting the likelihood that loan applicants default on their credit. We have access to historical data on previous loan takers comprised of a binary outcome variable ($y\\in\\{1=\\text{default},2=\\text{no default}\\}$) with two input features: (1) the subjects' *age*, which we define as immutable, and (2) the subjects' existing level of *debt*, which we define as mutable. \n\nWe have simulated this scenario using synthetic data with two independent features and Gaussian class-conditional densities in @fig-poc. The four panels in @fig-poc show the outcomes for different training procedures using the same model architecture each time (a linear classifier). In each case, we show the decision boundary (in green) and the training data colored according to their ground-truth label: orange points belong to the target class, $y^+=2$, blue points belong to the non-target class, $y^-=1$. Stars indicate counterfactuals in the target class generated at test time using generic gradient descent until convergence.\n\nIn panel (a), we have trained our model conventionally, and we do not impose mutability constraints at test time. The generated counterfactuals are all valid, but not plausible: they do not comply with the distribution of the factual samples in the target class to the point where they are clearly distinguishable from the ground-truth data. In panel (b), we have trained our model with CT, once again without any mutability constraints. We observe that the counterfactuals are highly plausible, meeting the first objective of Def. \\ref{def-explainability}.\n\nIn panel (c), we have used conventional training again, this time imposing the mutability constraint on *age* at test time. Counterfactuals are valid but involve some substantial reductions in *debt* for some individuals (very young applicants). By comparison, counterfactual paths are shorter on average in panel (d), where we have used CT and protected the immutable feature as described in @sec-constraints. We observe that due to the classifier's lower sensitivity to *age*, recourse recommendations with respect to *debt* are much more homogenous and do not disproportionately punish younger individuals. The counterfactuals are also plausible with respect to the mutable feature. Thus, we consider the model in panel (d) as the most explainable according to Def. \\ref{def-explainability}.\n\n![Illustration of how CT improves model explainability.](/paper/figures/poc.svg){#fig-poc}\n\n\n\n# Experiments {#sec-experiments}\n\nIn this section, we present experiments that we have conducted in order to answer the following research questions:\n\n1. To what extent does our proposed counterfactual training objective (@eq-obj) induce models to learn plausible explanations?\n2. To what extent does our proposed counterfactual training objective (@eq-obj) yield more favorable algorithmic recourse outcomes in the presence of actionability constraints?\n3. What are the effects of hyperparameter selection with respect to @eq-obj?\n\n## Experimental Setup\n\n### Evaluation \n\nOur key outcome of interest is how well do models perform with respect to explainability (Def. \\ref{def-explainability}). To this end, we focus primarily on the plausibility and cost of faithfully generated counterfactuals at test time. To measure the cost of counterfactuals, we follow the standard convention of using distances ($\\ell_1$-norm) between factuals and counterfactuals as a proxy. For plausibility, we assess how similar counterfactuals are to observed samples in the target domain. We rely on the distance-based metric used by @altmeyer2024faithful,\n\n$$\n\\text{IP}(\\mathbf{x}^\\prime,\\mathbf{X}^+) = \\frac{1}{\\lvert\\mathbf{X}^+\\rvert}\\sum_{\\mathbf{x} \\in \\mathbf{X}^+} \\text{dist}(\\mathbf{x}^{\\prime},\\mathbf{x})\n$$ {#eq-impl-dist}\nand introduce a novel divergence metric,\n\n$$\n\\text{IP}^*(\\mathbf{X}^\\prime,\\mathbf{X}^+) = \\text{MMD}(\\mathbf{X}^\\prime,\\mathbf{X}^+)\n$$ {#eq-impl-div}\nwhere $\\mathbf{X}^\\prime$ denotes a set of multiple counterfactuals and $\\text{MMD}(\\cdot)$ is an unbiased estimate of the squared population maximum mean discrepancy [@gretton2012kernel]. The metric in @eq-impl-div is equal to zero iff the two distributions are the same, $\\mathbf{X}^\\prime=\\mathbf{X}^+$.\n\nIn addition to cost and plausibility, we also compute other standard metrics to evaluate counterfactuals at test time including validity and redundancy. Finally, we also assess the predictive performance of models using standard metrics.\n\nWe run the experiments with three gradient-based generators: *Generic* of @wachter2017counterfactual as a simple baseline approach, *REVISE* [@joshi2019realistic] that aims to generate plausible counterfactuals using a surrogate Variational Autoencoder (VAE), and *ECCo*---the generator of @altmeyer2023faithful but without the conformal prediction component---as a method that directly targets both faithfulness and plausibility of the CEs.\n\n## Experimental Results\n\n### Plausibility\n\n@tbl-main presents our main empirical findings. The top five rows show the percentage reduction in implausibility according to @eq-impl-dist for varying degrees of the energy penalty used for *ECCo* at test time. The following row shows the reduction in implausibility as measured by @eq-impl-div and aggregated across all test specifications of *ECCo*. The final two rows show the test accuracies for the model trained with CT and conventionally trained models (\"vanilla\"). \n\nWe observe that for all datasets except *OL* and across all test settings, the average distance of counterfactuals from observed samples in the target class is reduced, indicating improved plausibility. The magnitude of improvements varies by dataset: for the simple synthetic datasets, distance reductions range from around 20-40% (*LS*, *Moon*) to almost 60% (*Circ*). For the real-world tabular datasets, improvements are generally smaller but still substantial in many cases with around 10-15% for *CH*, 11-28% for *GMSC*, 7-8% for *Cred* and around 3% for *Adult*. For our only vision dataset (*MNIST*), distances are reduced by up to 9%. The results for our proposed divergence metric are qualitatively similar, but generally even more pronounced: for the *Circ* dataset, implausibility is reduced by almost 94% to virtually zero as we verified by looking at the absolute outcome. Improvements for other datasets range from 28% (*Moon*) to 78% (*GMSC*). For *OL* the reduction is negative, consistent with the distance-based metric. The only dataset, for which our proposed metric disagrees with the distance-based metric is *MNIST*.\n\nThese broad and substantial improvements in plausibility generally do not come at the cost of decreased predictive performance: test accuracy for CT is virtually identical to the baseline for *Adult*, *Circ*, *LS*, *Moon* and *OL*, and even slightly improved for *Cred*. Exceptions to this general pattern are *MNIST*, *CH* and *GMSC*, for which we observe reduction in test accuracy of 2, 5 and 15 percentage points, respectively. We note in this context, that we have not optimized our models for predictive performance at all and worked with very small networks. In summary, we find that CT can substantially improve the quality of explanations learned by models without generally sacrificing predictive accuracy. \n\n::: {#tbl-main}\n\n\n```{=latex}\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}} cccccc }\n\\input{tables/main.tex}\n\\end{tabular*}\n```\n\n\nKey plausibility and predictive performance metrics for all datasets. The top five rows show the percentage reduction in implausibility according to @eq-impl-dist for varying degrees of the energy penalty used for *ECCo* at test time. The following row shows the reduction in implausibility as measured by @eq-impl-div and aggregated across all test specifications of *ECCo*. The final two rows show the test accuracies for the model trained with CT and conventionally trained models (\"vanilla\").\n\n:::\n\n### Actionability\n\n### Impact of hyperparameter settings {#sec-hyperparameters}\nWe extensively test the impact of three types of hyperparameters on the proposed training regime. Our complete results are available in the technical appendix; this section focuses on the main findings. \n\n***Hyperparameters of the CE generators.*** First, we observe that CT is highly sensitive to hyperparameter settings but (a) there are manageable patterns and (b) we can typically identify settings that improve either plausibility or cost, and commonly both of them at the same time. Second, we note that the choice of a CE generator has a major impact on the results. For example, *REVISE* tends to perform the worst, most likely because it uses a surrogate VAE to generate counterfactuals which impedes faithfulness [@altmeyer2024faithful]. Third, increasing $T$, the maximum number of steps, generally yields better outcomes because more CEs can mature in each training epoch. Fourth, the impact of $\\tau$, the required decision threshold is more difficult to predict. On \"harder\" datasets it may be difficult to satisfy high $\\tau$ for any given sample (i.e., also factuals) and so increasing this threshold does not seem to correlate with better outcomes. In fact, we have generally found that a choice of $\\tau=0.5$ leads to optimal results because it is associated with high proportions of mature counterfactuals.\n\n***Hyperparameters for penalties.*** We find that the strength of the energy regularization, $\\lambda_{\\text{reg}}$ is highly impactful; energy must be sufficiently regularized to avoid poor performance in terms of decreased plausibility and increased costs. The sensitivity with respect to $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$ is much less evident. While high values of $\\lambda_{\\text{reg}}$ may increase the variability in outcomes when combined with high values of $\\lambda_{\\text{div}}$ or $\\lambda_{\\text{adv}}$, this effect is not very pronounced.\n\n***Other hyperparameters.*** We observe that the effectiveness and stability of CT is positively associated with the number of counterfactuals generated during each training epoch. We also confirm that a higher number of training epochs is beneficial. Interestingly, we find that it is not necessary to employ CT during the entire training phase to achieve the desired improvements in explainability. When training models conventionally during the first 50% of epochs before switching to CT for the next 50% of epochs, we observed positive results. Put differently, CT may be a way to improve the explainability of models in a fine-tuning manner. \n\n\n\n# Discussion {#sec-discussion}\n\nWe first address the direct extensions of CT in @sec-approach. Then, we look at its limitations and challenges in @sec-limitations. \n\n## Future Research {#sec-approach}\n\n***CT is defined only for classification settings.*** Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined, CT requires the output space $\\mathcal{Y}$ to be discrete. Thus, it does not apply to ML tasks where the change in outcome cannot be readily quantified. Focus on classification models is a common restriction in research on CEs and AR. Other settings have attracted some interest (e.g., regression in [@spooner2021counterfactual; @zhao2023counterfactual]), but there is little consensus how to robustly extend the notion of counterfactuals.\n\n***CT is subject to training instabilities.*** Joint energy-based models are susceptible to instabilities during training [@grathwohl2020your] and even though we depart from the SGLD-based sampling, we still encounter major variability in the outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term in @eq-div, and (2) the underlying counterfactual explainers. For example, @altmeyer2023faithful recognize this to be a challenge for *ECCCo* and so it may have downstream impacts on our proposed method. Still, we find that training instabilities can be successfully mitigated by regularizing energy ($\\lambda_{\\text{reg}}$), generating a sufficiently large number of counterfactuals during each training epoch, and including only mature counterfactuals for contrastive divergence.\n\n***CT is sensitive to hyperparameter selection.*** Our method benefits from tuning certain key hyperparameters (see @sec-hyperparameters). In this work, we have relied exclusively on grid search for this task. Future work on CT could benefit from investigating more sophisticated approaches towards hyperparameter tuning. Notably, CT is iterative which makes a variety of methods applicable, including Bayesian [e.g., @snoek2012practical] or gradient-based [e.g., @franceschi2017forward] optimization.\n\n## Limitations and Challenges {#sec-limitations}\n\n***CT increases the training time of models.*** Counterfactual training promotes explainability through CEs and robustness through AEs at the cost of longer training times compared to conventional training regimes. While higher numbers of iterations and counterfactuals per iteration positively impact the quality of found solutions, they also increase the required amount of computations. We find that relatively small grids with 270 settings can take almost four hours for more demanding datasets on a high-performance computing cluster with 34 2GB CPUs^[See supplementary appendix for computational details.]. However, there are three factors that attenuate the impact of this limitation. First, CT provides counterfactual explanations for the training samples essentially for free, which may be beneficial in many ADM systems. Second, we find that CT can retain its value when used as a \"fine-tuning\" training regime for conventionally-trained models. Third, in principle, CT yields itself to parallel execution, which we have leveraged for our own experiments.\n\n***Immutable features may have proxies.*** We propose an approach to protect immutable features and thus increase the actionability of the generated CEs. However, it requires that model owners define the mutability constraints for (all) features considered by the model. Even with sufficient domain knowledge to protect all immutable features, there may exist proxies that are theoretically mutable (and hence should not be protected) but preserve enough information about the principals to hinder the protections. As an example, consider the Adult dataset used in our experiments where the mutable education status is a proxy for the immutable age, in that the attainment of degrees is correlated with age. Delineating actionability is a major undecided challenge in the AR literature [see, e.g., @venkatasubramanian2020philosophical] impacting the capacity of CT to increase the explainability of the model.\n\n***Interventions on features may impact fairness downstream.***  Related to the point above, we provide a tool that allows practitioners to modify the sensitivity of a model with respect to certain features, which may have implication for the fair and equitable treatment of individuals subject to automated decisions. As protecting a set of features leads the model to assign higher relative importance to unprotected features, model owners could misuse our solution by enforcing explanations based on features that are more difficult to modify by some (group of) individuals. For example, consider again the Adult dataset where features such as workclass or education may be more difficult to change for underpriviledged groups. When applied irresponsibly, CT could result in an unfairly assigned burden of recourse [e.g., @sharma2020certifai], threatening the equality of opportunity in the system [@bell2024fairness] and potentially reinforcing social segregation [@gao2023impact]. Still, as the referenced publications indicate, such phenomena are not specific to CT; all types of ADM solutions without strong external protections have been recognized to promote harmful power dynamics [@maas2023machine].\n\n\n\n# Conclusion {#sec-conclusion}\nState-of-the-art machine learning models are prone to learning complex representations that cannot be interpreted by humans. Although post-hoc explainability approaches have attracted major research interest, these cannot guarantee that the explanations agree with the opaque model's learned representation of data. As a step towards addressing this challenge, we introduced counterfactual training, a novel training regime that incentivizes highly-explainable models. Our approach leads to explanations that are both plausible---compliant with the underlying data-generating process---and actionable---compliant with user-specified mutability constraints---and thus meaningful to their recipients. Through extensive experiments we demonstrate that CT satisfies its objectives while preserving the predictive performance of the trained models. We also find that our approach can be used to fine-tune conventionally-trained models and achieve similar gains in explainability. Finally, this work showcases that it is practical to improve models *and* their explanations at the same time.\n\n\n\n\n\n# References {-}\n\n::: {#refs}\n:::\n\n\n\n{{< pagebreak >}}\n\n\n\n\\FloatBarrier\n\n<!-- \\setcounter{section}{0} -->\n\\renewcommand{\\thesection}{\\Alph{section}}\n\n<!-- \\setcounter{table}{0} -->\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\n<!-- \\setcounter{figure}{0} -->\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\n<!-- # Supplementary Material {.appendix} -->\n\n# Notation {.appendix}\n\n- $y^+$: The target class and also the index of the target class.\n- $y^-$: The non-target class and also the index of non-the target class.\n- $\\mathbf{y}^+$: The one-hot encoded output vector for the target class. \n- $\\theta$: Model parameters (unspecified).\n- $\\Theta$: Matrix of parameters. \n\n## Other Technical Details\n\n$$\n\\begin{aligned}\nMMD({X}^\\prime,\\tilde{X}^\\prime) &= \\frac{1}{m(m-1)}\\sum_{i=1}^m\\sum_{j\\neq i}^m k(x_i,x_j) \\\\ &+ \\frac{1}{n(n-1)}\\sum_{i=1}^n\\sum_{j\\neq i}^n k(\\tilde{x}_i,\\tilde{x}_j) \\\\ &- \\frac{2}{mn}\\sum_{i=1}^m\\sum_{j=1}^n k(x_i,\\tilde{x}_j)\n\\end{aligned}\n$$ {#eq-mmd}\n\n\n\n# Technical Details of Our Approach {.appendix} \n\n## Generating Counterfactuals through Gradient Descent {#sec-app-ce}\n\nIn this section, we provide some background on gradient-based counterfactual generators (@sec-app-ce-background) and discuss how we define convergence in this context (@sec-app-conv).\n\n### Background {#sec-app-ce-background}\n\nGradient-based counterfactual search was originally proposed by @wachter2017counterfactual. It generally solves the following unconstrained objective,\n\n$$\n\\begin{aligned}\n\\min_{\\mathbf{z}^\\prime \\in \\mathcal{Z}^L} \\left\\{  {\\text{yloss}(\\mathbf{M}_{\\theta}(g(\\mathbf{z}^\\prime)),\\mathbf{y}^+)}+ \\lambda {\\text{cost}(g(\\mathbf{z}^\\prime)) }  \\right\\} \n\\end{aligned} \n$$\n\nwhere $g: \\mathcal{Z} \\mapsto \\mathcal{X}$ is an invertible function that maps from the $L$-dimensional counterfactual state space to the feature space and $\\text{cost}(\\cdot)$ denotes one or more penalties that are used to induce certain properties of the counterfactual outcome. As above, $\\mathbf{y}^+$ denotes the target output and $\\mathbf{M}_{\\theta}(\\mathbf{x})$ returns the logit predictions of the underlying classifier for $\\mathbf{x}=g(\\mathbf{z})$.\n\nFor all generators used in this work we use standard logit crossentropy loss for $\\text{yloss}(\\cdot)$. All generators also penalize the distance ($\\ell_1$-norm) of counterfactuals from their original factual state. For *Generic* and *ECCo*, we have $\\mathcal{Z}:=\\mathcal{X}$ and $g(\\mathbf{z})=g(\\mathbf{z})^{-1}=\\mathbf{z}$, that is counterfactual are searched directly in the feature space. Conversely, *REVISE* traverses the latent space of a variational autoencoder (VAE) fitted to the training data, where $g(\\cdot)$ corresponds to the decoder [@joshi2019realistic]. In addition to the distance penalty, *ECCo* uses an additional penalty component that regularizes the energy associated with the counterfactual, $\\mathbf{x}^\\prime$ [@altmeyer2024faithful]. \n\n### Convergence {#sec-app-conv}\n\nAn important consideration when generating counterfactual explanations using gradient-based methods is how to define convergence. Two common choices are to 1) perform gradient descent over a fixed number of iterations $T$, or 2) conclude the search as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$. We prefer the latter for our purposes, because it explicitly defines convergence in terms of the black-box model, $\\mathbf{M}(\\mathbf{x})$.\n\nDefining convergence in this way allows for a more intuitive interpretation of the resulting counterfactual outcomes than with fixed $T$. Specifically, it allows us to think of counterfactuals as explaining 'high-confidence' predictions by the model for the target class $y^+$. Depending on the context and application, different choices of $\\tau$ can be considered as representing 'high-confidence' predictions.\n\n\n\n\n\n\n\n\n## Protecting Mutability Constraints with Linear Classifiers {#sec-app-constraints}\n\nIn @sec-constraints we explain that to avoid penalizing implausibility that arises due to mutability constraints, we impose a point mass prior on $p(\\mathbf{x})$ for the corresponding feature. We argue in @sec-constraints that this approach induces models to be less sensitive to immutable features and demonstrate this empirically in @sec-experiments. Below we derive the analytical results in Prp.~\\ref{prp-mtblty}.\n\n::: {.proof}\n\nLet $d_{\\text{mtbl}}$ and $d_{\\text{immtbl}}$ denote some mutable and immutable feature, respectively. Suppose that $\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}}$ and $\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}}$, where $\\mu_{k,d}$ denotes the conditional sample mean of feature $d$ in class $k$. In words, we assume that the immutable feature tends to take lower values for samples in the non-target class $y^-$ than in the target class $y^+$. We assume the opposite to hold for the mutable feature.\n\nAssuming multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, we have for the log likelihood ratio between any two classes $k,m \\in \\mathcal{K}$ [@hastie2009elements]:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})}=\\mathbf{x}^\\intercal \\Sigma^{-1}(\\mu_{k}-\\mu_{m})  + \\text{const}\n$$ {#eq-loglike}\n\nBy independence of $x_1,...,x_D$, the full log-likelihood ratio decomposes into:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\frac{\\mu_{k,d}-\\mu_{m,d}}{\\sigma_{d}^2} x_{d} + \\text{const}\n$$ {#eq-loglike-decomp}\n\nBy the properties of our classifier (*multinomial logistic regression*), we have:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\left( \\theta_{k,d} - \\theta_{m,d} \\right)x_d + \\text{const}\n$$ {#eq-multi}\n\nwhere $\\theta_{k,d}=\\Theta[k,d]$ denotes the coefficient on feature $d$ for class $k$. \n\nBased on @eq-loglike-decomp and @eq-multi we can identify that $(\\mu_{k,d}-\\mu_{m,d}) \\propto (\\theta_{k,d} - \\theta_{m,d})$ under the assumptions we made above. Hence, we have that $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$ and $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$\n\nLet $\\mathbf{x}^\\prime$ denote some randomly chosen individual from class $y^-$ and let $y^+ \\sim p(y)$ denote the randomly chosen target class. Then the partial derivative of the contrastive divergence penalty [@eq-div] with respect to coefficient $\\theta_{y^+,d}$ is equal to \n\n$$\n\\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left(\\text{div}(\\mathbf{x},\\mathbf{x^\\prime},\\mathbf{y};\\theta)\\right) = \\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left( \\left(-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]\\right) - \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^\\prime)[y^+]\\right) \\right) = x_{d}^\\prime - x_{d}\n$$ {#eq-grad}\n\nand equal to zero everywhere else.\n\nSince $(\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}})$ we are more likely to have $(x_{d_{\\text{immtbl}}}^\\prime - x_{d_{\\text{immtbl}}}) < 0$ than vice versa at initialization. Similarly, we are more likely to have $(x_{d_{\\text{mtbl}}}^\\prime - x_{d_{\\text{mtbl}}}) > 0$ since $(\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}})$.\n\nThis implies that if we do not protect feature $d_{\\text{immtbl}}$, the contrastive divergence penalty will decrease $\\theta_{y^-,d_{\\text{immtbl}}}$ thereby exacerbating the existing effect $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$. In words, not protecting the immutable feature would have the undesirable effect of making the classifier more sensitive to this feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for lower values of $d_{\\text{immtbl}}$. \n\nBy the same rationale, the contrastive divergence penalty can generally be expected to increase $\\theta_{y^-,d_{\\text{mtbl}}}$ exacerbating $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$. In words, this has the effect of making the classifier more sensitive to the mutable feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for higher values of $d_{\\text{mtbl}}$.\n\nThus, our proposed approach of protecting feature $d_{\\text{immtbl}}$ has the net affect of decreasing the classifier's sensitivity to the immutable feature relative to the mutable feature (i.e. no change in sensitivity for $d_{\\text{immtbl}}$ relative to increased sensitivity for $d_{\\text{mtbl}}$).\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Domain Constraints\n\nWe apply domain constraints on counterfactuals during training and evaluation. There are at least two good reasons for doing so. Firstly, within the context of explainability and algorithmic recourse, real-world attributes are often domain constrained: the *age* feature, for example, is lower bounded by zero and upper bounded by the maximum human lifespan. Secondly, domain constraints help mitigate training instabilities commonly associated with energy-based modelling [@grathwohl2020your;@altmeyer2024faithful].\n\nFor our image datasets, features are pixel values and hence the domain is constrained by the lower and upper bound of values that pixels can take depending on how they are scaled (in our case $[-1,1]$). For all other features $d$ in our synthetic and tabular datasets, we automatically infer domain constraints $[x_d^{\\text{LB}},x_d^{\\text{UB}}]$  as follows,\n\n$$\n\\begin{aligned}\nx_d^{\\text{LB}} &= \\arg\\min_{x_d} \\{\\mu_d - n_{\\sigma_d}\\sigma_d, \\arg \\min_{x_d} x_d\\} \\\\\nx_d^{\\text{UB}} &= \\arg\\max_{x_d} \\{\\mu_d + n_{\\sigma_d}\\sigma_d, \\arg \\max_{x_d} x_d\\} \n\\end{aligned}\n$$ {#eq-domain}\n\nwhere $\\mu_d$ and $\\sigma_d$ denote the sample mean and standard deviation of feature $d$. We set $n_{\\sigma_d}=3$ across the board but higher values and hence wider bounds may be appropriate depending on the application.\n\n\n\n## Training Details {#sec-app-training}\n\nIn this section, we describe the training procedure in detail. While the details laid out here are not crucial for understanding our proposed approach, they are of importance to anyone looking to implement counterfactual training. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Details on Main Experiments {.appendix}\n\n## Final Hyperparameters\n\nAs discussed @sec-experiments, CT is sensitive to certain hyperparameter choices. We study the effect of many hyperparameters extensively in @sec-app-grid. For the main results, we tune a small set of key hyperparameters (@sec-app-tune). The final choices for the main results are presented for each data set in @tbl-final-params along with training, test and batch sizes.\n\n::: {#tbl-final-params}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{table}\n  \\begin{tabular}{cccccccc}\n    \\toprule\n    \\textbf{Data} & \\textbf{No. Train} & \\textbf{No. Test} & \\textbf{Batchsize} & \\textbf{Domain} & \\textbf{Decision Threshold} & \\textbf{No. Counterfactuals} & \\textbf{$\\lambda_{\\text{reg}}$} \\\\\\midrule\n    Adult & $2.6 \\cdot 10^{4}$ & $5.01 \\cdot 10^{3}$ & $1 \\cdot 10^{3}$ & none & 0.75 & $5 \\cdot 10^{3}$ & 0.25 \\\\\n    CH & $1.65 \\cdot 10^{4}$ & $3.1 \\cdot 10^{3}$ & $1 \\cdot 10^{3}$ & none & 0.5 & $5 \\cdot 10^{3}$ & 0.25 \\\\\n    Circ & $3.6 \\cdot 10^{3}$ & 600 & 30 & none & 0.5 & $1 \\cdot 10^{3}$ & 0.5 \\\\\n    Cred & $1.06 \\cdot 10^{4}$ & $1.92 \\cdot 10^{3}$ & $1 \\cdot 10^{3}$ & none & 0.5 & $5 \\cdot 10^{3}$ & 0.25 \\\\\n    GMSC & $1.34 \\cdot 10^{4}$ & $2.47 \\cdot 10^{3}$ & $1 \\cdot 10^{3}$ & none & 0.5 & $5 \\cdot 10^{3}$ & 0.5 \\\\\n    LS & $3.6 \\cdot 10^{3}$ & 600 & 30 & none & 0.5 & $1 \\cdot 10^{3}$ & 0.01 \\\\\n    MNIST & $1.1 \\cdot 10^{4}$ & $2 \\cdot 10^{3}$ & $1 \\cdot 10^{3}$ & (-1.0, 1.0) & 0.5 & $5 \\cdot 10^{3}$ & 0.01 \\\\\n    Moon & $3.6 \\cdot 10^{3}$ & 600 & 30 & none & 0.9 & $1 \\cdot 10^{3}$ & 0.25 \\\\\n    OL & $3.6 \\cdot 10^{3}$ & 600 & 30 & none & 0.5 & $1 \\cdot 10^{3}$ & 0.25 \\\\\\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\n\n\n:::\n\nFinal hyperparameters used for the main results for the different datasets.\n\n:::\n\n\n\n\n\n\n## Qualitative Findings for Image Data\n\n::: {.callout-note}\n\n@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla). In fact, this is not even using *ECCo+* and still showing better results than the best results we achieved in our AAAI paper for JEM ensembles.\n\n:::\n\n![Counterfactual images for *MLP* with counterfactual training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}\n\n![Counterfactual images for *MLP* with conventional training. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}\n\n\n\n# Grid Searches {#sec-app-grid}\n\n\n\n\n\n\n\n\n\nTo assess the hyperparameter sensitivity of our proposed training regime we ran multiple large grid searches for all of our synthetic datasets. We have grouped these grid searches into multiple categories: \n\n1. **Generator Parameters** (@sec-app-grid-gen): Investigates the effect of changing hyperparameters that affect the counterfactual outcomes during the training phase.\n2. **Penalty Strengths** (@sec-app-grid-pen): Investigates the effect of changing the penalty strengths in out proposed objective (@eq-obj).\n3. **Other Parameters** (@sec-app-grid-train): Investigates the effect of changing other training parameters, including the total number of generated counterfactuals in each epoch.\n\nWe begin by summarizing the high-level findings in @sec-app-grid-hl. For each of the categories, @sec-app-grid-gen to @sec-app-grid-train then present all details including the exact parameter grids, average predictive performance outcomes and key evaluation metrics for the generated counterfactuals. \n\n## Evaluation Details\n\nTo measure predictive performance, we compute the accuracy and F1-score for all models on test data (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). With respect to explanatory performance, we report here our findings for the (im)plausibility and cost of counterfactuals at test time. Since the computation of our proposed divergence metric (@eq-impl-div) is memory-intensive, we rely on the distance-based metric for the grid searches. For the counterfactual evaluation, we draw factual samples from the training data for the grid searches to avoid data leakage with respect to our final results reported in the body of the paper. Specifically, we want to avoid choosing our default hyperparameters based on results on the test data. Since we are optimizing for explainability, not predictive performance, we still present test accuracy and F1-scores. \n\n### Predictive Performance\n\nWe find that CT is associated with little to no decrease in average predictive performance for our synthetic datasets: test accuracy and F1-scores decrease by at most ~1 percentage point, but generally much less (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). Variation across hyperparameters is negligible as indicated by small standard deviations for these metrics across the board. \n\n### Counterfactual Outcomes {#sec-app-grid-hl}\n\nOverall, we find that counterfactual training (CT) achieves it key objectives consistently across all hyperparameter settings and also broadly across datasets: plausibility is improved by up to ~60 percent (%) for the *Circles* data (e.g. @fig-grid-gen_params-plaus-circles), ~25-30% for the *Moons* data (e.g. @fig-grid-gen_params-plaus-moons) and ~10-20% for the *Linearly Separable* data (e.g. @fig-grid-gen_params-plaus-lin_sep). At the same time, the average costs of faithful counterfactuals are reduced in many cases by around ~20-25% for  *Circles* (e.g. @fig-grid-gen_params-cost-circles) and up to ~50% for *Moons* (e.g. @fig-grid-gen_params-cost-moons). For the *Linearly Separable* data, costs are generally increased although typically by less than 10% (e.g. @fig-grid-gen_params-cost-lin_sep), which reflects a common tradeoff between costs and plausibility [@altmeyer2024faithful]. \n\nWe do observe strong sensitivity to certain hyperparameters, with clear an manageable patterns. Concerning generator parameters, we firstly find that using *REVISE* to generate counterfactuals during training typically yields the worst outcomes out of all generators, often leading to a substantial decrease in plausibility. This finding can be attributed to the fact that *REVISE* effectively assigns the task of learning plausible explanations from the model itself to a surrogate VAE. In other words, counterfactuals generated by *REVISE* are less faithful to the model that *ECCo* and *Generic*, and hence we would expect them to be a less effective and, in fact, potentially detrimental role in our training regime. Secondly, we observe that allowing for a higher number of maximum steps $T$ for the counterfactual search generally yields better outcomes. This is intuitive, because it allows more counterfactuals to reach maturity in any given iteration. Looking in particular at the results for *Linearly Separable*, it seems that higher values for $T$ in combination with higher decision thresholds ($\\tau$) yields the best results when using *ECCo*. But depending on the degree of class separability of the underlying data, a high decision-threshold can also affect results adversely, as evident from the results for the *Overlapping* data (@fig-grid-gen_params-plaus-over): here we find that CT generally fails to achieve its objective because only a tiny proportion of counterfactuals ever reaches maturity.\n\nRegarding penalty strengths, we find that the strength of the energy regularization, $\\lambda_{\\text{reg}}$ is a key hyperparameter, while sensitivity with respect to $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$ is much less evident. In particular, we observe that not regularizing energy enough or at all typically leads to poor performance in terms of decreased plausibility and increased costs, in particular for *Circles* (@fig-grid-pen-plaus-circles), *Linearly Separable* (@fig-grid-pen-plaus-lin_sep) and *Overlapping* (@fig-grid-pen-plaus-over). High values of $\\lambda_{\\text{reg}}$ can increase the variability in outcomes, in particular when combined with high values for $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$, but this effect is less pronounced.\n\nFinally, concerning other hyperparameters we observe that the effectiveness and stability of CT is positively associated with the number of counterfactuals generated during each training epoch, in particular for *Circles* (@fig-grid-train-plaus-circles) and *Moons* (@fig-grid-train-plaus-moons). We further find that a higher number of training epochs is beneficial as expected, where we tested training models for 50 and 100 epochs. Interestingly, we find that it is not necessary to employ CT during the entire training phase to achieve the desired improvements in explainability: specifically, we have tested training models conventionally during the first half of training before switching to CT after this initial burn-in period. \n\n## Generator Parameters {#sec-app-grid-gen}\n\n\n\n\n\n\nThe hyperparameter grid with varying generator parameters during training is shown in @nte-gen-params-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-gen-params-final-run-eval.\n\n::: {#nte-gen-params-final-run-train .callout-note}\n\n## Training Phase\n\n\n- Generator Parameters:\n    - Decision Threshold: `0.75, 0.9, 0.95`\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 5.0, 10.0, 20.0`\n    - Maximum Iterations: `5, 25, 50`\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n::: {#nte-gen-params-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n\n:::\n\n### Accuracy\n\n::: {#tbl-acc-gen}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 0.997 & 0.00309 \\\\\n  Circ & Accuracy & Vanilla & 0.998 & 0.000557 \\\\\n  Circ & F1-score & Full & 0.997 & 0.00309 \\\\\n  Circ & F1-score & Vanilla & 0.998 & 0.000558 \\\\\n  LS & Accuracy & Full & 0.999 & 0.00201 \\\\\n  LS & Accuracy & Vanilla & 1 & 0 \\\\\n  LS & F1-score & Full & 0.999 & 0.00201 \\\\\n  LS & F1-score & Vanilla & 1 & 0 \\\\\n  Moon & Accuracy & Full & 0.999 & 0.000696 \\\\\n  Moon & Accuracy & Vanilla & 1 & 0.00111 \\\\\n  Moon & F1-score & Full & 0.999 & 0.000696 \\\\\n  Moon & F1-score & Vanilla & 1 & 0.00111 \\\\\n  OL & Accuracy & Full & 0.915 & 0.00477 \\\\\n  OL & Accuracy & Vanilla & 0.917 & 0.00123 \\\\\n  OL & F1-score & Full & 0.915 & 0.00478 \\\\\n  OL & F1-score & Vanilla & 0.917 & 0.00124 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-gen-params-final-run-train) and evaluation-phase parameters (@nte-gen-params-final-run-eval).\n\n:::\n\n### Plausibility\n\n\nThe results with respect to the plausibility measure are shown in @fig-grid-gen_params-plaus-circles to @fig-grid-gen_params-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-circles}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-lin_sep}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-moons}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-over}\n\n\n\n\n### Cost\n\n\nThe results with respect to the cost measure are shown in @fig-grid-gen_params-cost-circles to @fig-grid-gen_params-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-circles}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-lin_sep}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-moons}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-over}\n\n\n\n\n## Penalty Strengths {#sec-app-grid-pen}\n\n\n\n\n\n\nThe hyperparameter grid with varying penalty strengths during training is shown in @nte-pen-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-pen-final-run-eval.\n\n::: {#nte-pen-final-run-train .callout-note}\n\n## Training Phase\n\n\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{adv}}$: `0.1, 0.25, 1.0`\n    - $\\lambda_{\\text{div}}$: `0.01, 0.1, 1.0`\n    - $\\lambda_{\\text{reg}}$: `0.0, 0.01, 0.1, 0.25, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n::: {#nte-pen-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n\n:::\n\n### Accuracy\n\n::: {#tbl-acc-pen}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 0.994 & 0.0144 \\\\\n  Circ & Accuracy & Vanilla & 0.998 & 0.000875 \\\\\n  Circ & F1-score & Full & 0.994 & 0.0145 \\\\\n  Circ & F1-score & Vanilla & 0.998 & 0.000875 \\\\\n  LS & Accuracy & Full & 0.998 & 0.00772 \\\\\n  LS & Accuracy & Vanilla & 1 & 0 \\\\\n  LS & F1-score & Full & 0.998 & 0.00773 \\\\\n  LS & F1-score & Vanilla & 1 & 0 \\\\\n  Moon & Accuracy & Full & 0.987 & 0.0351 \\\\\n  Moon & Accuracy & Vanilla & 0.998 & 0.0101 \\\\\n  Moon & F1-score & Full & 0.987 & 0.0352 \\\\\n  Moon & F1-score & Vanilla & 0.998 & 0.0102 \\\\\n  OL & Accuracy & Full & 0.911 & 0.0217 \\\\\n  OL & Accuracy & Vanilla & 0.916 & 0.00236 \\\\\n  OL & F1-score & Full & 0.911 & 0.0219 \\\\\n  OL & F1-score & Vanilla & 0.916 & 0.00236 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-pen-final-run-train) and evaluation-phase parameters (@nte-pen-final-run-eval).\n\n:::\n\n### Plausibility\n\n\nThe results with respect to the plausibility measure are shown in @fig-grid-pen-plaus-circles to @fig-grid-pen-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-circles}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-lin_sep}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-moons}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-over}\n\n\n\n\n### Cost\n\n\nThe results with respect to the cost measure are shown in @fig-grid-pen-cost-circles to @fig-grid-pen-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-circles}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-lin_sep}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-moons}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-over}\n\n\n\n\n## Other Parameters {#sec-app-grid-train}\n\n\n\n\n\n\nThe hyperparameter grid with other varying training parameters is shown in @nte-train-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-train-final-run-eval.\n\n::: {#nte-train-final-run-train .callout-note}\n\n## Training Phase\n\n\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - Burnin: `0.0, 0.5`\n    - No. Counterfactuals: `100, 1000`\n    - No. Epochs: `50, 100`\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n::: {#nte-train-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n\n:::\n\n### Accuracy\n\n::: {#tbl-acc-train}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 0.995 & 0.00431 \\\\\n  Circ & Accuracy & Vanilla & 0.998 & 0.000566 \\\\\n  Circ & F1-score & Full & 0.995 & 0.00432 \\\\\n  Circ & F1-score & Vanilla & 0.998 & 0.000566 \\\\\n  LS & Accuracy & Full & 0.999 & 0.00231 \\\\\n  LS & Accuracy & Vanilla & 1 & 0 \\\\\n  LS & F1-score & Full & 0.999 & 0.00231 \\\\\n  LS & F1-score & Vanilla & 1 & 0 \\\\\n  Moon & Accuracy & Full & 0.996 & 0.0136 \\\\\n  Moon & Accuracy & Vanilla & 0.988 & 0.022 \\\\\n  Moon & F1-score & Full & 0.996 & 0.0136 \\\\\n  Moon & F1-score & Vanilla & 0.988 & 0.022 \\\\\n  OL & Accuracy & Full & 0.914 & 0.00563 \\\\\n  OL & Accuracy & Vanilla & 0.918 & 0.00116 \\\\\n  OL & F1-score & Full & 0.914 & 0.0057 \\\\\n  OL & F1-score & Vanilla & 0.918 & 0.00116 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-train-final-run-train) and evaluation-phase parameters (@nte-train-final-run-eval).\n\n:::\n\n### Plausibility\n\n\nThe results with respect to the plausibility measure are shown in @fig-grid-train-plaus-circles to @fig-grid-train-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-circles}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-lin_sep}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-moons}\n\n![Average outcomes for the plausibility measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-over}\n\n\n\n\n### Cost\n\n\nThe results with respect to the cost measure are shown in @fig-grid-train-cost-circles to @fig-grid-train-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. Data: Circles.](/paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-circles}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-lin_sep}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Moons.](/paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-moons}\n\n![Average outcomes for the cost measure across hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-over}\n\n\n\n\n\n\n# Tuning Key Parameters {#sec-app-tune}\n\n\n\n\n\n\n\n\n\nBased on the findings from our initial large grid searches (@sec-app-grid), we tune selected hyperparameters for all datasets: namely, the decision threshold $\\tau$ and the strength of the energy regularization $\\lambda_{\\text{reg}}$. The final hyperparameter choices for each dataset are presented in **ADD TABLE**. Detailed results for each data set are shown in @fig-tune-plaus-adult to @fig-tune-mat-over. From **ADD TABLE**, we notice that the same decision threshold of $\\tau=0.5$ is optimal for all but on dataset. We attribute this to the fact that a low decision threshold results in a higher share of mature counterfactuals and hence more opportunities for the model to learn from examples (@fig-tune-mat-adult to @fig-tune-mat-over). This has played a role in particular for our real-world tabular datasets and MNIST, which suffered from low levels of maturity for higher decision thresholds. In cases where maturity is not an issue, as for *Moons*, higher decision thresholds lead to better outcomes, which may have to do with the fact that the resulting counterfactuals are more faithful to the model. Concerning the regularization strength, we find somewhat high variation across datasets. Most notably, we find that relatively low levels of regularization are optimal for MNIST. We hypothesize that this finding may be attributed to the uniform scaling of all input features (digits). \n\nFinally, to increase the proportion of mature counterfactuals for some datasets, we have also investigated the effect on the learning rate $\\eta$ for the counterfactual search and even smaller regularization strengths for a fixed decision threshold of 0.5 (@fig-tune_lr-plaus-adult to @fig-tune_lr-plaus-over). For the given low decision threshold, we find that the learning rate has no discernable impact on the proportion of mature counterfactuals (@fig-tune_lr-mat-adult to @fig-tune_lr-mat-over). We do notice, however, that the results for MNIST are much improved when using a low value $\\lambda_{\\text{reg}}$, the strength for the engery regularization: plausibility is increased by up to ~10% (@fig-tune_lr-plaus-mnist) and the proportion of mature counterfactuals reaches 100%. \n\nOne consideration worth exploring is to combine high decision thresholds with high learning rates, which we have not investigated here. \n\n::: {.callout-warning}\n\n## Package Version (Reproducibility)\n\nTuning was run using `v1.1.3` of `TaijaData`. The follow-up version `v1.1.4` introduced an option to split real-world tabular datasets into train and test set, ensuring that pre-processing steps like standardization is fit on the training set only. If you are rerunning the tuning experiments with a version of `TaijaData` that is higher than `v1.1.3`, than for the default parameters specified in the configuration files, you may end up with slightly different results, although we would not expect any changes in terms of qualitative findings. For exact reproducibility, please use `v1.1.3`.\n\n:::\n\n## Key Parameters {#sec-app-tune-key}\n\n\n\n\n\n\nThe hyperparameter grid for tuning key parameters is shown in @nte-tune-train. The corresponding evaluation grid used for these experiments is shown in @nte-tune-eval.\n\n::: {#nte-tune-train .callout-note}\n\n## Training Phase\n\n\n- Generator Parameters:\n    - Decision Threshold: `0.5, 0.75, 0.9`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{reg}}$: `0.1, 0.25, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n::: {#nte-tune-eval .callout-note}\n\n## Evaluation Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n\n:::\n\n### Plausibility\n\n\nThe results with respect to the plausibility measure are shown in @fig-tune-plaus-adult to @fig-tune-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Adult.](/paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-adult}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: California Housing.](/paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-cali}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Circles.](/paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-circles}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Credit.](/paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-credit}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: GMSC.](/paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-gmsc}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-lin_sep}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: MNIST.](/paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-mnist}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Moons.](/paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-moons}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/tune/mlp/over/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-over}\n\n\n\n\n### Proportion of Mature CE\n\n\nThe results with respect to the proportion of mature counterfactuals in each epoch are shown in @fig-tune-mat-adult to @fig-tune-mat-over.\n\n\n\n![Proportion of mature counterfactuals in each epoch. Data: Adult.](/paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-adult}\n\n![Proportion of mature counterfactuals in each epoch. Data: California Housing.](/paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-cali}\n\n![Proportion of mature counterfactuals in each epoch. Data: Circles.](/paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-circles}\n\n![Proportion of mature counterfactuals in each epoch. Data: Credit.](/paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-credit}\n\n![Proportion of mature counterfactuals in each epoch. Data: GMSC.](/paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-gmsc}\n\n![Proportion of mature counterfactuals in each epoch. Data: Linearly Separable.](/paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-lin_sep}\n\n![Proportion of mature counterfactuals in each epoch. Data: MNIST.](/paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-mnist}\n\n![Proportion of mature counterfactuals in each epoch. Data: Moons.](/paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-moons}\n\n![Proportion of mature counterfactuals in each epoch. Data: Overlapping.](/paper/experiments/output/final_run/tune/mlp/over/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-over}\n\n\n\n\n## Learning Rate {#sec-app-tune-lr}\n\n\n\n\n\n\nThe hyperparameter grid for tuning the learning rate is shown in @nte-tune_lr-train. The corresponding evaluation grid used for these experiments is shown in @nte-tune_lr-eval.\n\n::: {#nte-tune_lr-train .callout-note}\n\n## Training Phase\n\n\n- Generator Parameters:\n    - Learning Rate: `0.1, 0.5, 1.0`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{reg}}$: `0.01, 0.1, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n\n:::\n\n::: {#nte-tune_lr-eval .callout-note}\n\n## Evaluation Phase\n\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n\n:::\n\n### Plausibility\n\n\nThe results with respect to the plausibility measure are shown in @fig-tune_lr-plaus-adult to @fig-tune_lr-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Adult.](/paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-adult}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Credit.](/paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-credit}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: GMSC.](/paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-gmsc}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Linearly Separable.](/paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-lin_sep}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: MNIST.](/paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-mnist}\n\n![Average outcomes for the plausibility measure across key hyperparameters. Data: Overlapping.](/paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-over}\n\n\n\n\n### Proportion of Mature CE\n\n\nThe results with respect to the proportion of mature counterfactuals in each epoch are shown in @fig-tune_lr-mat-adult to @fig-tune_lr-mat-over.\n\n\n\n![Proportion of mature counterfactuals in each epoch. Data: Adult.](/paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-adult}\n\n![Proportion of mature counterfactuals in each epoch. Data: Credit.](/paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-credit}\n\n![Proportion of mature counterfactuals in each epoch. Data: GMSC.](/paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-gmsc}\n\n![Proportion of mature counterfactuals in each epoch. Data: Linearly Separable.](/paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-lin_sep}\n\n![Proportion of mature counterfactuals in each epoch. Data: MNIST.](/paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-mnist}\n\n![Proportion of mature counterfactuals in each epoch. Data: Overlapping.](/paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-over}\n\n\n\n\n\n\n# Computation Details {.appendix}\n\n<!-- [@DHPC2022] -->\n## Hardware {#sec-app-hardware}\n\nWe performed our experiments on a high-performance cluster. Details about the cluster will be disclosed upon publication to avoid revealing information that might interfere with the double-blind review process. Since our experiments involve highly parallel tasks and rather small models by today's standard, we have relied on distributed computing across multiple central processing units (CPU). Graphical processing units (GPU) were not required. \n\n### Grid Searches\n\nModel training for the largest grid searches with 270 unique parameter combinations was parallelized across 34 CPUs with 2GB memory each. The time to completion varied by dataset for reasons discussed in @sec-discussion: 0h49m (*Moons*), 1h4m (*Linearly Separable*), 1h49m (*Circles*), 3h52m (*Overlapping*). Model evaluations for large grid searches were parallelized across 20 CPUs with 3GB memory each. Evaluations for all data sets took less than one hour (<1h) to complete. \n\n### Tuning\n\nFor tuning of selected hyperparameters, we distributed the task of generating counterfactuals during training across 40 CPUs with 2GB memory each for all tabular datasets. Except for the *Adult* dataset, all training runs were completed in less that half an hour (<0h30m). The *Adult* dataset took around 0h35m to complete. Evaluations across 20 CPUs with 3GB memory each generally took less than 0h30m to complete. For *MNIST*, we relied on 100 CPUs with 2GB memory each. For the *MLP*, training of all models could be completed in 1h30m, while the evaluation across 20 CPUs (6GB memory) took 4h12m. For the *CNN*, training of all models took ~8h, with conventionally trained models taking ~0h15m each and model with CT taking ~0h30m-0h45m each.\n\n## Software\n\nAll computations were performed in the Julia Programming Language [@bezanson2017julia]. We have developed a package for counterfactual training that leverages and extends the functionality provided by several existing packages, most notably [CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) [@altmeyer2023explaining] and the [Flux.jl](https://fluxml.ai/Flux.jl/v0.16/) library for deep learning [@innes2018fashionable;@innes2018flux]. For data-wrangling and presentation-ready tables we relied on [DataFrames.jl](https://dataframes.juliadata.org/v1.7/) [@milan2023dataframes] and [PrettyTables.jl](https://ronisbr.github.io/PrettyTables.jl/v2.4/) [@chagas2024pretty], respectively. For plots and visualizations we used both [Plots.jl](https://docs.juliaplots.org/v1.40/) [@PlotsJL] and [Makie.jl](https://docs.makie.org/v0.22/) [@danisch2021makie], in particular [AlgebraOfGraphics.jl](https://aog.makie.org/v0.9.3/). To distribute computational tasks across multiple processors, we have relied on [MPI.jl](https://juliaparallel.org/MPI.jl/v0.20/) [@byrne2021mpi].\n\n\n\n\n",
    "supporting": [
      "paper_files"
    ],
    "filters": []
  }
}