# Introduction

Today's prominence of artificial intelligence (AI) has largely been driven by **representation learning**: instead of relying on features and rules hand-crafted by humans, modern machine learning (ML) models are tasked with learning representations directly from the data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Advances in computing have made it possible to provide these models with ever-growing degrees of freedom to achieve this task, which often allows them to outperform traditionally parsimonious models. Unfortunately, in doing so, models learn increasingly complex, sensitive representations that humans can no longer easily interpret.

The trend towards complexity for the sake of performance has come under scrutiny in recent years. At the very cusp of the deep learning (DL) revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are susceptible to adversarial examples (AEs): perturbed versions of data instances that yield vastly different model predictions despite being semantically indistinguishable from their factual counterparts. Some partial mitigation strategies have been proposed---most notably **adversarial training** [@goodfellow2014explaining]---but truly robust deep learning remains unattainable even for models that are considered "shallow" by today's standards [@kolter2023keynote]. 

Part of the problem is that the high degrees of freedom---high number of parameters estimated from data---provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case]. As one example, research on the "lottery ticket hypothesis" suggests that modern neural networks can be pruned by up to 90% without losing predictive performance [@frankle2018lottery]. Thus, looking at the predictive performance alone, found solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative and semantically meaningless patterns. This poses two related challenges. Firstly, there is no dependable way to verify if learned representations correspond to meaningful, plausible explanations. Secondly, even if we resolve this challenge, it remains undecided how to ensure that machine learning models can *only* learn valuable explanations. 

The first challenge has attracted an abundance of work on **explainable AI** (XAI), a paradigm that focuses on the development of tools to derive (post-hoc) explanations from complex model representations, aiming to mitigate scenarios in which practitioners deploy opaque models and have to blindly rely on their predictions. On many occasions, this has happened in practice, causing harms to people who were adversely and unfairly affected by automated decision-making (ADM) systems involving opaque models; see, e.g., @oneil2016weapons. Effective XAI tools can also aid in monitoring models and providing recourse, empowering people to turn negative outcomes (e.g., "loan application rejected") into positive ones (e.g., "loan application accepted"). In line with this, our work builds upon **counterfactual explanations** (CE) proposed by @wachter2017counterfactual; CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce an alternative, more desirable output.

To our surprise, the second challenge has not yet attracted major research interest. In particular, there has been no concerted effort towards improving the degree to which learned representations promote explanations that are both **interpretable** to and deemed **plausible** by humans. Instead, the typical choice has been to improve the ability of XAI tools to identify the subset of explanations that are plausible and valid for any given model, independent of whether these explanations are compatible with the learned representations [@altmeyer2024faithful]. Fortunately, recent findings indicate that improved "explanatory capacity" of a model can arise as a consequence of regularization techniques aimed at other training objectives such as generative capacity, generalization, or robustness [@altmeyer2024faithful; @augustin2020adversarial; @schut2021generating]. Our contribution consolidates these findings within a unified framework.

Specifically, **we propose Counterfactual Training (CT)**: a novel training regime explicitly geared towards improving the explanatory capacity of models that, in high-level terms, we define as the extent to which valid explanations derived for a model can be deemed plausible with respect to the underlying data and global actionability constraints (we refine this notion in Def. \ref{def-explainability}). For simplicity, we refer to models with high explanatory capacity as *explainable*. To the best of our knowledge, Counterfactual Training represents the first attempt to achieve more explainable models by employing counterfactual explanations already in the training phase.

The remainder of this manuscript is structured as follows. @sec-lit presents related work, focusing on the link between AEs and CEs. Then follow our two principal contributions. In @sec-method, we introduce our methodological framework and show theoretically that it can be employed to enforce global actionability constraints. In @sec-experiments, through extensive experiments, we empirically demonstrate that CT substantially improves explainability and positively contributes to the robustness of trained models without sacrificing predictive performance. In @sec-discussion, we discuss open challenges and, in @sec-conclusion, conclude that CT is a promising approach towards making opaque models more trustworthy.

