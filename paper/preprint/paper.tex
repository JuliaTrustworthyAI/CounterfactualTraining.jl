% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Latin Modern Roman}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage[title]{appendix}
\usepackage{placeins}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{placeins}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\newcounter{quartocalloutnteno}
\newcommand{\quartocalloutnte}[1]{\refstepcounter{quartocalloutnteno}\label{#1}}

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Counterfactual Training: Teaching Models Plausible and Actionable Explanations},
  pdfauthor={Patrick Altmeyer; Aleksander Buszydlik; Arie van Deursen; Cynthia C. S. Liem},
  pdfkeywords={Counterfactual Training, Counterfactual
Explanations, Algorithmic Recourse, Explainable AI, Representation
Learning},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\usepackage{lineno}
\linenumbers
\newcommand{\runninghead}{A Preprint }
\renewcommand{\runninghead}{Counterfactual Training (A Preprint) }
\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}
\def\asep{\\\\\\ } % default: all authors on same column
\def\asep{\And }
\author{\textbf{Patrick
Altmeyer}~\orcidlink{0000-0003-4726-8613}\\Faculty of Electrical
Engineering, Mathematics and Computer Science\\Delft University of
Technology\\\\\href{mailto:p.altmeyer@tudelft.nl}{p.altmeyer@tudelft.nl}\asep\textbf{Aleksander
Buszydlik}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Arie van
Deursen}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Cynthia C. S.
Liem}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be (1) plausible with
respect to the underlying data and (2) actionable with respect to the
user-defined mutability constraints. Much existing research has
therefore focused on developing post-hoc methods to generate
counterfactuals that meet these desiderata. In this work, we instead
hold models directly accountable for the desired end goal:
counterfactual training employs counterfactuals ad-hoc during the
training phase to minimize the divergence between learned
representations and plausible, actionable explanations. We demonstrate
empirically and theoretically that our proposed method facilitates
training models that deliver inherently desirable explanations while
maintaining high predictive performance.
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
Counterfactual Training \sep Counterfactual
Explanations \sep Algorithmic Recourse \sep Explainable AI \sep 
Representation Learning



\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by \textbf{representation learning}: instead of relying on
features and rules that are carefully hand-crafted by humans, modern
machine learning (ML) models are tasked with learning representations
directly from data, guided by narrow objectives such as predictive
accuracy (\citeproc{ref-goodfellow2016deep}{Goodfellow, Bengio, and
Courville 2016}). Modern advances in computing have made it possible to
provide such models with ever-growing degrees of freedom to achieve that
task, which frequently allows them to outperform traditionally more
parsimonious models. Unfortunately, in doing so, models learn
increasingly complex and highly sensitive representations that humans
can no longer easily interpret.

The trend towards complexity for the sake of performance has come under
serious scrutiny in recent years. At the very cusp of the deep learning
(DL) revolution, Szegedy et al.
(\citeproc{ref-szegedy2013intriguing}{2014}) showed that artificial
neural networks (ANN) are sensitive to adversarial examples (AEs):
perturbed versions of data instances that yield vastly different model
predictions despite being ``imperceptible'' in that they are
semantically indifferent from their factual counterparts. Even though
some partially effective mitigation strategies have been proposed---most
notably \textbf{adversarial training}
(\citeproc{ref-goodfellow2014explaining}{Goodfellow, Shlens, and Szegedy
2015})---truly robust deep learning remains unattainable even for models
that are considered ``shallow'' by today's standards
(\citeproc{ref-kolter2023keynote}{Kolter 2023}).

Part of the problem is that the high degrees of freedom provide room for
many solutions that are locally optimal with respect to narrow
objectives (\citeproc{ref-wilson2020case}{Wilson 2020}).\footnote{We
  follow the standard ML convention, where ``degrees of freedom'' refer
  to the number of parameters estimated from data.} Indeed, recent work
on the so-called ``lottery ticket hypothesis'' suggests that modern
neural networks can be pruned by up to 90\% while preserving their
predictive performance (\citeproc{ref-frankle2018lottery}{Frankle and
Carbin 2019}). Similarly, Zhang et al.
(\citeproc{ref-zhang2021understanding}{2021}) showed that
state-of-the-art neural networks are expressive enough to fit randomly
labeled data. Thus, looking at the predictive performance alone, the
solutions may seem to provide compelling explanations for the data, when
in fact they are based on purely associative, semantically meaningless
patterns. This poses two challenges. Firstly, there is no dependable way
to verify if representations correspond to meaningful, plausible
explanations. Secondly, even if we could resolve the first challenge, it
remains undecided how to ensure that models can \emph{only} learn
valuable explanations.

The first challenge has attracted an abundance of research on
\textbf{explainable AI} (XAI), a paradigm that focuses on the
development of tools to derive (post-hoc) explanations from complex
model representations. Such explanations should mitigate a scenario in
which practitioners deploy opaque models and blindly rely on their
predictions. On countless occasions, this has happened in practice and
caused real harms to people who were adversely and unfairly affected by
automated decision-making (ADM) systems involving opaque models
(\citeproc{ref-mcgregor2021preventing}{McGregor 2021};
\citeproc{ref-oneil2016weapons}{O'Neil 2016}). Effective XAI tools can
aid us in monitoring models and providing recourse to individuals to
turn negative outcomes (e.g., ``loan application rejected'') into
positive ones (e.g., ``application accepted''). In line with this, our
work builds upon \textbf{counterfactual explanations} (CE) proposed by
Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) as an effective
approach to achieve this goal. CEs prescribe minimal changes for factual
inputs that, if implemented, would prompt some fitted model to produce a
desired output.

To our surprise, the second challenge has not yet attracted major
research interest. Specifically, there has been no concerted effort
towards improving the ``explanatory capacity'' of models, i.e., the
degree to which learned representations correspond to explanations that
are \textbf{interpretable} and deemed \textbf{plausible} by humans (see
Def. \ref{def-explainability}). Instead, the choice has generally been
to improve the ability of XAI tools to identify the subset of
explanations that are both plausible and valid for any given model,
independent of whether the learned representations are in fact
compatible with plausible explanations
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).
Fortunately, recent findings indicate that improved explanatory capacity
can arise as a consequence of regularization techniques aimed at other
training objectives such as generative capacity, generalization, or
robustness (\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024};
\citeproc{ref-augustin2020adversarial}{Augustin, Meinke, and Hein 2020};
\citeproc{ref-schut2021generating}{Schut et al. 2021}). As further
discussed in Section~\ref{sec-lit}, our work consolidates these findings
within a single objective.

\textbf{Specifically, we introduce Counterfactual Training (CT)}: a
novel training regime explicitly geared towards improving the
explainability of models. In high-level terms, we define this concept as
as the extent to which valid explanations derived for an opaque model
are also deemed plausible with respect to the underlying data and the
global actionability constraints. To the best of our knowledge, our
framework represents the first attempt to address this challenge by
employing counterfactual explanations already in the training phase.

The remainder of this manuscript is structured as follows.
Section~\ref{sec-lit} presents related work, focusing on the link
between AEs and CEs. Then follow our two principal contributions. In
Section~\ref{sec-method}, we introduce our methodological framework and
show theoretically that it can be employed to enforce global
actionability constraints. In Section~\ref{sec-experiments}, through
extensive experiments, we demonstrate that CT substantially improves
explainability without sacrificing predictive performance. We discuss
the challenges in Section~\ref{sec-discussion} and conclude in
Section~\ref{sec-conclusion} that CT is a promising approach towards
making opaque models more trustworthy.

\section{Related Literature}\label{sec-lit}

To make the desiderata for our framework more concrete, we follow
Augustin, Meinke, and Hein
(\citeproc{ref-augustin2020adversarial}{2020}) in tying the concept of
explainability to the quality of CEs that can be generated for a given
model. The authors show that CEs---understood as minimal input
perturbations that yield some desired model prediction---tend to be more
meaningful if the underlying model is more robust to adversarial
examples. We can make intuitive sense of this finding when looking at
adversarial training (AT) through the lens of representation learning
with high degrees of freedom. As argued before, learned representations
may be sensitive to producing implausible explanations and mispredicting
for worst-case counterfactuals (i.e., AEs). Thus, by inducing models to
``unlearn'' susceptiblity to such examples, adversarial training can
effectively remove implausible explanations from the solution space.

\subsection{Adversarial Examples are Counterfactual
Explanations}\label{adversarial-examples-are-counterfactual-explanations}

This interpretation of the link between explainability through
counterfactuals on one side and robustness to adversarial examples on
the other is backed by empirical evidence. Sauer and Geiger
(\citeproc{ref-sauer2021counterfactual}{2021}) demonstrates that using
counterfactual images during classifier training improves model
robustness. Similarly, Abbasnejad et al.
(\citeproc{ref-abbasnejad2020counterfactual}{2020}) argue that
counterfactuals represent potentially useful training data in machine
learning, especially in supervised settings where inputs may be
reasonably mapped to multiple outputs. They, too, demonstrate that
augmenting the training data of image classifiers can improve
generalization. Finally, Teney, Abbasnedjad, and Hengel
(\citeproc{ref-teney2020learning}{2020}) propose an approach using
counterfactuals in training that does not rely on data augmentation:
they argue that counterfactual pairs typically already exist in training
datasets. Specifically, their approach relies on identifying similar
input samples with different annotations and ensuring that the gradient
of the classifier aligns with the vector between such pairs of
counterfactual inputs using the cosine distance as the loss function.

In the natural language processing (NLP) domain, CEs have also been used
to improve models through data augmentation. Wu et al.
(\citeproc{ref-wu2021polyjuice2}{2021}) proposes \emph{Polyjuice}, a
general-purpose counterfactual generator for language models. The
authors empirically demonstrate that the augmentation of training data
with their method improves robustness in a number of NLP tasks.
Balashankar et al. (\citeproc{ref-balashankar2023improving}{2023})
similarly uses \emph{Polyjuice} to augment NLP datasets through diverse
counterfactuals and show that classifier robustness improves by up to
20\%. Finally, Luu and Inoue
(\citeproc{ref-luu2023counterfactual}{2023}) introduces Counterfactual
Adversarial Training (CAT) that also aims to improve generalization and
robustness of language models through a three-step procedure: the
authors identify training samples that are subject to high predictive
uncertainty, generate CEs for them, and fine-tune the language model on
a dataset augmented with the CEs.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples. Pointing
to clear similarities in how CEs and AEs are generated, Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}) makes the case for
jointly studying the opaqueness and robustness problems in
representation learning. Formally, AEs can be seen as the subset of CEs
for which misclassification is achieved
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022}). Similarly,
Pawelczyk et al. (\citeproc{ref-pawelczyk2022exploring}{2022}) shows
that CEs and AEs are equivalent under certain conditions.

Two recent works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of the post-hoc counterfactual explanations. Firstly,
Ross, Lakkaraju, and Bastani (\citeproc{ref-ross2021learning}{2024})
proposes a way to train models that guarantee individual recourse to
some positive target class with high probability. Their approach builds
on adversarial training by explicitly inducing susceptibility to
targeted adversarial examples for the positive class. Additionally, the
proposed method allows for imposing a set of actionability constraints
ex-ante. For example, users can specify that certain features are
immutable. Secondly, Guo, Nguyen, and Yadav
(\citeproc{ref-guo2023counternet}{2023}) is the first to propose an
end-to-end training pipeline that includes CEs as part of the training
procedure. In particular, they propose a specific network architecture
that includes a predictor and CE generator network, where the parameters
of the CE generator network are learnable. Counterfactuals are generated
during each training iteration and fed back to the predictor network. In
contrast to Guo, Nguyen, and Yadav
(\citeproc{ref-guo2023counternet}{2023}), we impose no restrictions on
the ANN architecture at all.

\subsection{Aligning Representations with Plausible
Explanations}\label{aligning-representations-with-plausible-explanations}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a work
closely related to this one, Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) shows that explainability
can be improved through model averaging and refined model objectives.
The authors propose a way to generate counterfactuals that are maximally
faithful to the model in that they are consistent with what the model
has learned about the underlying data. Formally, they rely on tools from
energy-based modelling (\citeproc{ref-teh2003energy}{Teh et al. 2003})
to minimize the divergence between the distribution of counterfactuals
and the conditional posterior over inputs learned by the model. Their
proposed counterfactual explainer, \emph{ECCCo}, yields plausible
explanations if and only if the underlying model has learned
representations that align with them. The authors find that both deep
ensembles (\citeproc{ref-lakshminarayanan2016simple}{Lakshminarayanan,
Pritzel, and Blundell 2017}) and joint energy-based models (JEMs)
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}) tend to do
well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are most called for when
models are underspecified by the available data
(\citeproc{ref-wilson2020case}{Wilson 2020}). Averaging across solutions
mitigates the aforementioned risk of relying on a single locally optimal
representations that corresponds to semantically meaningless
explanations for the data. Previous work of Schut et al.
(\citeproc{ref-schut2021generating}{2021}) similarly found that
generating plausible (``interpretable'') CEs is almost trivial for deep
ensembles that have also undergone adversarial training. The case for
JEMs is even clearer: they involve a hybrid objective that induces both
high predictive performance and generative capacity
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}). This is
closely related to the idea of aligning models with plausible
explanations and has inspired our CT objective.

\section{Counterfactual Training}\label{sec-method}

In this section we propose our novel counterfactual training objective.
In CT, we combine ideas from adversarial training, counterfactual
explanations, and energy-based modelling with the explicit goal of
aligning representations with plausible explanations that comply with
user-defined actionability constraints.

In the context of counterfactual explanations, plausibility has broadly
been defined as the degree to which counterfactuals comply with the
underlying data-generating process
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024};
\citeproc{ref-guidotti2022counterfactual}{Guidotti 2022};
\citeproc{ref-poyiadzi2020face}{Poyiadzi et al. 2020}). Plausibility is
a necessary but insufficient condition for using CEs to provide
algorithmic recourse (AR) to individuals (negatively) affected by opaque
models. To be actionable, AR recommendations must also be attainable. A
plausible CE for a rejected 20-year-old loan applicant, for example,
might reveal that their application would have been accepted, if only
they were 20 years older. Ignoring all other features, this would comply
with the definition of plausibility if 40-year-old individuals were in
fact more credit-worthy on average than young adults. But of course this
CE does not qualify for providing actionable recourse to the applicant
since \emph{age} is not a (directly) mutable feature. Counterfactual
training aims to improve model explainability by aligning models with
counterfactuals that meet both desiderata: plausibility and
actionability. Formally, we define explainability as follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$ where $\arg\max_y{\mathbf{y}^{+}}=y^+$ and $y^+$ denotes the index of the target class. 

We say that $\mathbf{M}_\theta$ is \textbf{explainable} to the extent that faithfully generated counterfactuals are plausible and actionable. We define these properties as follows:

\begin{enumerate}
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$ where $A$ is some small region around $\mathbf{x}^{\prime}$.
    \item (Actionability) Permutations $\Delta$ are subject to some actionability constraints.
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$ where $A$ is defined as above.
\end{enumerate}
where $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior over inputs. 
\end{definition}

\noindent The characterization of faithfulness and plausibility in Def.
\ref{def-explainability} is the same as in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}), with adapted notation.
Intuitively, plausible counterfactuals are consistent with the data and
faithful counterfactuals are consistent with what the model has learned
about input data. Actionability constraints in Def.
\ref{def-explainability} vary and depend on the context in which
\(\mathbf{M}_\theta\) is deployed. In this work, we focus on domain and
mutability constraints for individual features \(x_d\) for
\(d=1,...,D\). We limit ourselves to classification tasks for reasons
discussed in Section~\ref{sec-discussion}.

\subsection{Our Proposed Objective}\label{our-proposed-objective}

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
explanation generated through gradient descent over \(T\) iterations as
initially proposed by Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). For our purposes, we
let \(T\) vary and consider the counterfactual search as converged as
soon as the predicted probability for the target class has reached a
pre-determined threshold, \(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.\footnote{For detailed
  background information on gradient-based counterfactual search and
  convergence see supplementary appendix.}

To train models with high explainability as defined in Def.
\ref{def-explainability}, we propose to leverage counterfactuals in the
following objective:

\begin{equation}\phantomsection\label{eq-obj}{
\begin{split}
\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) &+ \lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) \\ &+ \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{split}
}\end{equation} where \(\text{yloss}(\cdot)\) is a classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms are explained in detail below. For now, they can
be summarized as inducing explainability directly and indirectly by
penalizing: (1) the contrastive divergence, \(\text{div}(\cdot)\),
between mature counterfactuals \(\mathbf{x}_T^\prime\) and observed
samples \(\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}\) in the
target class \(y^+\), and, (2) the adversarial loss,
\(\text{advloss}(.)\), with respect to nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\). Finally, \(\text{ridge}(\cdot)\)
denotes a Ridge penalty (\(\ell_2\)-norm) that regularizes the magnitude
of the energy terms involved in \(\text{div}(\cdot)\)
(\citeproc{ref-du2019implicit}{Du and Mordatch 2020}). The trade-off
between the components can governed through penalties
\(\lambda_{\text{div}}\), \(\lambda_{\text{adv}}\) and
\(\lambda_{\text{reg}}\).

\subsection{Directly Inducing Explainability with Contrastive
Divergence}\label{directly-inducing-explainability-with-contrastive-divergence}

As observed by Grathwohl et al.
(\citeproc{ref-grathwohl2020your}{2020}), any classifier can be
re-interpreted as a joint energy-based model (JEM) that learns to
discriminate output classes conditional on the observed (training)
samples from \(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). The authors show that JEMs can be trained to
perform well at both tasks by directly maximizing the joint
log-likelihood factorized as
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\).
The first term can be optimized using conventional cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\),
Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) minimizes the
contrastive divergence between the observed samples from
\(p(\mathbf{x})\) and generated samples from \(p_\theta(\mathbf{x})\).

A key empirical finding in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) was that JEMs tend to do
well with respect to the plausibility objective in Def.
\ref{def-explainability}. This follows directly if we consider samples
drawn from \(p_\theta(\mathbf{x})\) as counterfactuals because the JEM
objective effectively minimizes the divergence between the conditional
posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\). To generate samples,
Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) relies on
Stochastic Gradient Langevin Dynamics (SGLD) using an uninformative
prior for initialization but we depart from their methodology. Instead
of SGLD, we propose to use counterfactual explainers to generate
counterfactuals of observed training samples. Specifically, we have:

\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation} where \(\mathcal{E}_\theta(\cdot)\) denotes the energy
function defined as
\(\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]\)
where \(y^+\) denotes the index of the randomly drawn target class,
\(y^+ \sim p(y)\). Conditional on the target class \(y^+\),
\(\mathbf{x}_T^\prime\) denotes a mature counterfactual for a randomly
sampled factual from a non-target class generated with a gradient-based
CE generator for up to \(T\) iterations. Mature counterfactuals are ones
that have either reached convergence wrt. the decision threshold
\(\tau\) or exhausted \(T\).

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while increasing the
energy of counterfactuals (negative samples)
(\citeproc{ref-du2019implicit}{Du and Mordatch 2020}). As the
counterfactuals get more plausible (Def. \ref{def-explainability})
during training, these opposing effects gradually balance each other out
(\citeproc{ref-lippe2024uvadlc}{Lippe 2024}).

The departure from SGLD of (\citeproc{ref-grathwohl2020your}{Grathwohl
et al. 2020}) allows us to tap into the vast repertoire of explainers
that have been proposed in the literature to meet different desiderata.
For example, many methods facilitate the imposition of domain and
mutability constraints. In principle, any existing approach for
generating counterfactual explanations is viable, so long as it does not
violate the faithfulness condition. Like JEMs
(\citeproc{ref-murphy2022probabilistic}{Murphy 2022}), CT can be
considered a form of contrastive representation learning.

\subsection{Indirectly Inducing Explainability with Adversarial
Robustness}\label{indirectly-inducing-explainability-with-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
(\citeproc{ref-balashankar2023improving}{Balashankar et al. 2023};
\citeproc{ref-luu2023counterfactual}{Luu and Inoue 2023}) or adversarial
examples (\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022};
\citeproc{ref-pawelczyk2022exploring}{Pawelczyk et al. 2022}). This
leaves some flexibility wrt. the choice for \(\text{advloss}(\cdot)\) in
Equation~\ref{eq-obj}. An intuitive functional form, but likely not the
only sensible choice, is inspired by adversarial training:

\begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation} Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as AEs as long as the magnitude of the
perturbation to any single feature is at most \(\varepsilon\). This is
closely aligned with Szegedy et al.
(\citeproc{ref-szegedy2013intriguing}{2014}) that defines an adversarial
attack as an ``imperceptible non-random perturbation''. Thus, we choose
to work with a different distinction between CE and AE than Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}) that considers
misclassification as the key distinguishing feature of AE. One of the
key observations of this work is that we can leverage CEs during
training and get adversarial examples essentially for free, which can be
used to reap the aforementioned benefits of adversarial training.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any counterfactual explainer that relies on gradient
descent in the feature space for optimization
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, and Liem
2023}). In this context, domain constraints can be imposed by simply
projecting counterfactuals back to the specified domain, if the previous
gradient step resulted in updated feature values that were
out-of-domain. Mutability constraints can similarly be enforced by
setting partial derivatives to zero to ensure that features are only
perturbed in the allowed direction, if at all.

Since such actionability constraints are binding at test time, we should
also impose them when generating \(\mathbf{x}^\prime\) during each
training iteration to inform model representations. Through their effect
on \(\mathbf{x}^\prime\), both types of constraints influence model
outcomes via Equation~\ref{eq-div}. Here it is crucial that we avoid
penalizing implausibility that arises due to mutability constraints. For
any mutability-constrained feature \(d\) this can be achieved by
enforcing \(\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0\) whenever
perturbing \(\mathbf{x}^\prime[d]\) in the direction of
\(\mathbf{x}^+[d]\) would violate mutability constraints. Specifically,
we set \(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  We have \(\mathbf{x}^+[d]>\mathbf{x}^\prime[d]\), but feature \(d\)
  can only be decreased in practice.
\item
  We have \(\mathbf{x}^+[d]<\mathbf{x}^\prime[d]\), but feature \(d\)
  can only be increased in practice.
\end{enumerate}

\noindent From a Bayesian perspective, setting
\(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) can be understood as
assuming a point mass prior for \(p(\mathbf{x}^+)\) with respect to
feature \(d\). Intuitively, we think of this simply in terms ignoring
implausibility costs with respect to immutable features, which
effectively forces the model to instead seek plausibility with respect
to the remaining features. This in turn results in lower overall
sensitivity to immutable features, which we demonstrate empirically for
different classifiers in Section~\ref{sec-experiments}. Under certain
conditions, this result holds theoretically:\footnote{For the proof, see
  the supplementary appendix.}

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. If we assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}

\noindent It is worth highlighting that
\mbox{Proposition \ref{prp-mtblty}} assumes independence of features.
This raises a valid concern about the effect of protecting immutable
features in the presence of proxies that remain unprotected. We address
this in Section~\ref{sec-discussion}.

\subsection{Example (Prediction of Consumer Credit
Default)}\label{example-prediction-of-consumer-credit-default}

Suppose we are interested in predicting the likelihood that loan
applicants default on their credit. We have access to historical data on
previous loan takers comprised of a binary outcome variable
(\(y\in\{1=\text{default},2=\text{no default}\}\)) with two input
features: (1) the subjects' \emph{age}, which we define as immutable,
and (2) the subjects' existing level of \emph{debt}, which we define as
mutable.

We have simulated this scenario using synthetic data with independent
\emph{age} and \emph{debt}, and Gaussian class-conditional densities in
Figure~\ref{fig-poc}. The four panels show the outcomes for different
training procedures using the same model architectures (a linear
classifier). In panels (a) and (c) we have trained the models
conventionally, while in panels (b) and (d) we used CT.

In all cases, all counterfactuals (stars) are valid---they have cross
the decision boundary (green)---but their quality differs. In panel (a),
they are not plausible: they do not comply with the distribution of the
factuals in \(y^+\) to the point where they form a clearly
distinguishable cluster. In panel (b), they are highly plausible,
meeting the first objective of Def. \ref{def-explainability}. In panel
(c), the CEs involve substantial reductions in \emph{debt} for younger
applicants. By comparison, counterfactual paths are shorter on average
in panel (d) where we have protected the immutable \emph{age} as
described in Section~\ref{sec-constraints}. Due to the classifier's
lower sensitivity to \emph{age}, recommendations with respect to
\emph{debt} are much more homogenous and do not unfairly punish younger
individuals. These counterfactuals are also plausible with respect to
the mutable feature. Thus, we consider the model in panel (d) as the
most explainable according to Def. \ref{def-explainability}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Illustration of how CT improves model
explainability: (a) conventional training, all mutable; (b) CT, all
mutable; (c) conventional, \emph{age} immutable; (d) CT, \emph{age}
immutable. The decision boundary is shown in green along with training
data colored according to their ground-truth label: \(y^+=2\) (orange)
and \(y^-=1\) (blue). Stars indicate CEs for the target class.}

\end{figure}%

\section{Experiments}\label{sec-experiments}

In our experiments we seek to answer the following three research
questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To what extent does the counterfactual training objective as it is
  defined in Equation~\ref{eq-obj} induce models to learn plausible
  explanations?
\item
  To what extent does the CT objective produce more favorable
  algorithmic recourse outcomes in the presence of actionability
  constraints?
\item
  What are the effects of hyperparameter selection wrt. the CT
  objective?
\end{enumerate}

\subsection{Experimental Setup}\label{experimental-setup}

Our key outcome of interest is improved explainability (Def.
\ref{def-explainability}). To this end, we focus primarily on the
plausibility and cost of faithfully generated counterfactuals at test
time. To measure the cost of counterfactuals, we follow the standard
convention of using distances (\(\ell_1\)-norm) between factuals and
counterfactuals as a proxy. For plausibility, we assess how similar
counterfactuals are to observed samples in the target domain,
\(\mathbf{X}^+\subset\mathcal{X}^+\). We rely on the distance-based
metric used in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}),

\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{IP}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation} and introduce a novel divergence metric,

\begin{equation}\phantomsection\label{eq-impl-div}{
\text{IP}^*(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation} where \(\mathbf{X}^\prime\) denotes a collection of
counterfactuals and \(\text{MMD}(\cdot)\) is an unbiased estimate of the
squared population maximum mean discrepancy
(\citeproc{ref-gretton2012kernel}{Gretton et al. 2012}). The metric in
Equation~\ref{eq-impl-div} is equal to zero iff the two distributions
are the same, \(\mathbf{X}^\prime=\mathbf{X}^+\).

In addition to cost and plausibility, we compute other standard metrics
to evaluate counterfactuals including validity and redundancy. Finally,
we also assess the predictive performance of models using standard
metrics, including robust accuracy estimated on adversarially perturbed
data using FGSM (\citeproc{ref-goodfellow2014explaining}{Goodfellow,
Shlens, and Szegedy 2015}).

We run the experiments with three gradient-based generators:
\emph{Generic} of Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) as a simple baseline
approach, \emph{REVISE} (\citeproc{ref-joshi2019realistic}{Joshi et al.
2019}) that aims to generate plausible counterfactuals using a surrogate
Variational Autoencoder (VAE), and \emph{ECCo}---the generator of
Altmeyer et al. (\citeproc{ref-altmeyer2024faithful}{2024}) but without
the conformal prediction component---as a method that directly targets
both faithfulness and plausibility of the counterfactuals.

We make use of nine classification datasets common in the CE/AR
literature. Four of them are synthetic with two classes and different
characteristics: linearly separable clusters (\emph{LS}), overlapping
clusters (\emph{OL}), concentric circles (\emph{Circ}), and interlocking
moons (\emph{Moon}). These datasets are generated using the library of
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, and Liem
2023}) and we present them in the supplementary appendix. Next, we have
four real-world binary tabular datasets from the domain of economics:
\emph{Adult} (a.k.a. Census data) of
(\citeproc{ref-becker1996adult2}{Becker and Kohavi 1996}), California
housing (\emph{CH}) of (\citeproc{ref-pace1997sparse}{Pace and Barry
1997}), Default of Credit Card Clients (\emph{Cred}) of
(\citeproc{ref-yeh2016default}{Yeh 2016}), and Give Me Some Credit
(\emph{GMSC}) of (\citeproc{ref-kaggle2011give2}{Kaggle 2011}). Finally,
for the convenience of illustration, we use of the 10-class \emph{MNIST}
vision dataset (\citeproc{ref-lecun1998mnist}{LeCun 1998}).

To assess our proposed training regime, we investigate the improvements
in performance metrics when using CT on top of a weak baseline (BL):
specifically, a multilayer perceptron (\emph{MLP}). This is the best way
to get a clear picture of how effective CT is and consistent with how
assessment is done in the related literature
(\citeproc{ref-ross2021learning}{Ross, Lakkaraju, and Bastani 2024};
\citeproc{ref-teney2020learning}{Teney, Abbasnedjad, and Hengel 2020};
\citeproc{ref-goodfellow2014explaining}{Goodfellow, Shlens, and Szegedy
2015}).

\subsection{Experimental Results}\label{experimental-results}

\subsubsection{Plausibility}\label{sec-plaus}

Table~\ref{tbl-main} presents our main empirical findings. For all
datasets except \emph{OL} and across all test settings, the average
distance of CEs from observed samples in the target class is reduced,
indicating improved plausibility. The magnitude of improvements varies.
For the simple synthetic datasets, distance reductions range from around
20-40\% (\emph{LS}, \emph{Moon}) to almost 60\% (\emph{Circ}). For the
real-world tabular datasets, improvements tend to be smaller but still
substantial, with around 10-15\% for \emph{CH}, 11-28\% for \emph{GMSC},
7-8\% for \emph{Cred}, and around 3\% for \emph{Adult}. For our only
vision dataset (\emph{MNIST}), distances are reduced by up to 9\%. The
results for our proposed divergence metric are qualitatively similar,
but generally even more pronounced: for the \emph{Circ} dataset,
implausibility is reduced by almost 94\% to virtually zero as we
verified by the absolute outcome. Improvements for other datasets range
from 28\% (\emph{Moon}) to 78\% (\emph{GMSC}). For \emph{OL} the
reduction is negative, consistent with the distance-based metric.
\emph{MNIST} is the only dataset for which the distance and divergence
metrics disagree. Upon visual inspection of the image counterfactuals we
find that CT clearly improves plausibility (see appendix).

\subsubsection{Predictive Performance}\label{sec-pred}

Test accuracy for CT is virtually identical to the baseline for
\emph{Adult}, \emph{Circ}, \emph{LS}, \emph{Moon}, and \emph{OL}, and
even slightly improved for \emph{Cred}. Exceptions to this general
pattern are \emph{MNIST}, \emph{CH}, and \emph{GMSC}, for which we
observe a reduction in test accuracy of 2, 5, and 15 percentage points
respectively. When looking at robust test accuracies (Acc.\(^*\)) for
these datasets in particular, we find that CT strongly outperforms the
baseline. In fact, we find that CT improves adversarial robustness on
all datasets.

\begin{table}

\caption{\label{tbl-main}Key performance metrics across all datasets.
\textbf{Plausibility}: Columns 2-6 show the percentage reduction in
implausibility (\(\text{IP}\)) for varying degrees of the energy penalty
used for \emph{ECCo} at test time; column 7 shows the reduction in
\(\text{IP}^*\) (\(\text{MMD}\)), aggregated across all test
specifications. \textbf{Accuracy} (columns 8-11): test accuracies and
robust accuracies (\(\text{Acc}^*\)) for CT and the baseline (BL).
\textbf{Actionability} (column 9): average reduction in costs when
imposing mutability constraints.}

\centering{

\input{tables/main.tex}

}

\end{table}%

\subsubsection{Actionability}\label{sec-act}

In Section~\ref{sec-method}, we show that our proposed way for encoding
mutability constraints leads to lower classifier sensitivity wrt.
immutable features for linear models, tilting the decision boundary in
favour of mutable features instead. For binding constraints at test
time, this leads to shorter counterfactual paths and hence smaller
average costs (\(\ell_1\)-norm) to individuals. To extend this to the
non-linear case, we test the effect of imposing mutability constraints
empirically for our synthetic data using the same evaluation scheme as
above. The final row in Table~\ref{tbl-main} reports the average
reduction in costs for CT compared to the ``vanilla'' baseline, when
imposing that either the first or the second feature is immutable. In
all cases, costs are reduced substantially, indicating that classifiers
trained with CT are indeed more sensitive to mutable features.

\subsubsection{Impact of hyperparameter
settings.}\label{sec-hyperparameters}

We test the impact of three types of hyperparameters; our complete
results are in the supplementary appendix.\\
\indent We note that CT is highly sensitive to the choice of a CE
generator and its hyperparameters but (a) there are manageable patterns
and (b) we can typically identify settings that improve either
plausibility or cost, and commonly both of them at the same time. For
example, \emph{REVISE} tends to perform the worst, most likely because
it uses a surrogate VAE to generate counterfactuals which impedes
faithfulness (\citeproc{ref-altmeyer2024faithful}{Altmeyer et al.
2024}). Increasing \(T\), the maximum number of steps, generally yields
better outcomes because more CEs can mature in each training epoch. The
impact of \(\tau\), the required decision threshold is more difficult to
predict. On ``harder'' datasets it may be difficult to satisfy high
\(\tau\) for any given sample (i.e., also factuals) and so increasing
this threshold does not seem to correlate with better outcomes. In fact,
the choice of \(\tau=0.5\) generally leads to optimal results because it
is associated with high proportions of mature counterfactuals.\\
\indent The strength of the energy regularization,
\(\lambda_{\text{reg}}\) is highly impactful and leads to poor
performance in terms of decreased plausibility and increased costs if
insufficiently high. The sensitivity with respect to
\(\lambda_{\text{div}}\) and \(\lambda_{\text{adv}}\) is much less
evident. While high values of \(\lambda_{\text{reg}}\) may increase the
variability in outcomes when combined with high values of
\(\lambda_{\text{div}}\) or \(\lambda_{\text{adv}}\), this effect is not
very pronounced.\\
\indent The effectiveness and stability of CT is positively associated
with the number of counterfactuals generated during each training epoch.
We also confirm that a higher number of training epochs is beneficial.
Interestingly, we observed desired improvements when CT was combined
with conventional training and applied only for the final 50\% of epochs
of the complete training process. Put differently, CT can improve the
explainability of models in a fine-tuning manner.

\section{Discussion}\label{sec-discussion}

As our results indicate, counterfactual training produces models that
are more explainable. Nonetheless, it brings about three important
limitations.\\
\indent \emph{CT increases the training time of models.} CT can be more
time-consuming than conventional training regimes. While higher numbers
of CEs per iteration positively impact the quality of solutions, they
also increase the amount of computations. Relatively small grids with
270 settings can take almost four hours for more demanding datasets on a
high-performance computing cluster with 34 2GB CPUs.\footnote{See
  supplementary appendix for computational details.} Three factors
attenuate this effect. First, CT amortizes the cost of CEs for the
training samples. Second, we find that it can retain its value when used
as a ``fine-tuning'' technique for conventionally-trained models. Third,
it yields itself to parallel execution, which we have leveraged for our
own experiments.\\
\indent \emph{Immutable features may have proxies.} We propose an
approach to protect immutable features and thus increase the
actionability of the generated CEs. However, it requires that model
owners define the mutability constraints for (all) features considered
by the model. Even if all immutable features are protected, there may
exist proxies that are mutable (and hence should not be protected) but
preserve enough information about the principals to hinder the
protections. Delineating actionability is a major undecided challenge in
the AR literature (see, e.g.,
(\citeproc{ref-venkatasubramanian2020philosophical}{Venkatasubramanian
and Alfano 2020})) impacting the capacity of CT to fulfill its intended
goal.\\
\indent \emph{Interventions on features may impact fairness.} We provide
a tool that allows practitioners to modify the sensitivity of a model
with respect to certain features, which may have implication for the
fair and equitable treatment of decision subjects. As protecting a set
of features leads the model to assign higher relative importance to
unprotected features, model owners could misuse our solution by
enforcing explanations based on features that are more difficult to
modify by some (group of) individuals. For example, consider the Adult
dataset used in our experiments, where \emph{workclass} or
\emph{education} may be more difficult to change for underpriviledged
groups. When applied irresponsibly, CT could result in an unfairly
assigned burden of recourse (\citeproc{ref-sharma2020certifai}{Sharma,
Henderson, and Ghosh 2020}), threatening the equality of opportunity in
the system (\citeproc{ref-bell2024fairness}{Bell et al. 2024}).
Nonetheless, these phenomena are not specific to CT.

We also highlight several important directions for future research.
Firstly, it is an interesting challenge to extend CT beyond
classification settings. Our formulation relies on the distinction
between non-target class(es) \(y^{-}\) and target class(es) \(y^{+}\) to
generate counterfactuals through Equation~\ref{eq-obj}. While \(y^{-}\)
and \(y^{+}\) can be arbitrarily defined, CT requires the output space
\(\mathcal{Y}\) to be discrete. Thus, it does not apply to ML tasks
where the change in outcome cannot be readily quantified. Focus on
classification models is a common restriction in research on CEs and AR.
Other settings have attracted some interest (e.g., regression in
(\citeproc{ref-spooner2021counterfactual}{Spooner et al. 2021})), but
there is little consensus how to robustly extend the notion of CEs.\\
\indent Secondly, our approach is susceptible to training instabilities.
This problem has been recognized for JEMs
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}) and even
though we depart from the SGLD-based sampling, we still encounter
considerable variability in the outcomes. CT is exposed to two potential
sources of instabilities: (1) the energy-based contrastive divergence
term in Equation~\ref{eq-div}, and (2) the underlying counterfactual
explainers. We find several promising ways to mitigate this problem:
regularizing energy (\(\lambda_{\text{reg}}\)), generating sufficiently
many counterfactuals during each epoch, and including only mature
counterfactuals for contrastive divergence.\\
\indent Finally, we believe that it is possible to substantially improve
hyperparameter selection procedures. Our method benefits from the tuning
of certain key hyperparameters (see Section~\ref{sec-hyperparameters}).
In this work, we have relied exclusively on grid search for this task.
Future work on CT could benefit from investigating more sophisticated
approaches. Notably, CT is iterative which makes methods such as
Bayesian or gradient-based optimization applicable (see, e.g.,
(\citeproc{ref-bischl2023hyperparameter}{Bischl et al. 2023})).

\section{Conclusion}\label{sec-conclusion}

State-of-the-art machine learning models are prone to learning complex
representations that cannot be interpreted by humans and existing
post-hoc explainability approaches cannot guarantee that the
explanations agree with the model's learned representation of data. As a
step towards addressing this challenge, we introduced counterfactual
training, a novel training regime that incentivizes highly-explainable
models. Our approach leads to explanations that are both
plausible---compliant with the underlying data-generating process---and
actionable---compliant with user-specified mutability constraints---and
thus meaningful to their recipients. Through extensive experiments we
demonstrate that CT satisfies its objectives while preserving the
predictive performance of the models. Our approach can also be used to
fine-tune conventionally-trained models and achieve similar gains in
explainability. Finally, this work showcases that it is practical to
improve models \emph{and} their explanations at the same time.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abbasnejad2020counterfactual}
Abbasnejad, Ehsan, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van
den Hengel. 2020. {``{Counterfactual Vision and Language Learning}.''}
In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 10041--51.
\url{https://doi.org/10.1109/CVPR42600.2020.01006}.

\bibitem[\citeproctext]{ref-altmeyer2023explaining}
Altmeyer, Patrick, Arie van Deursen, and Cynthia C. S. Liem. 2023.
{``{Explaining Black-Box Models through Counterfactuals}.''} In
\emph{Proceedings of the JuliaCon Conferences}, 1:130.

\bibitem[\citeproctext]{ref-altmeyer2024faithful}
Altmeyer, Patrick, Mojtaba Farmanbar, Arie van Deursen, and Cynthia C.
S. Liem. 2024. {``{Faithful Model Explanations through
Energy-Constrained Conformal Counterfactuals}.''} In \emph{Proceedings
of the Thirty-Eighth AAAI Conference on Artificial Intelligence},
38:10829--37. 10. \url{https://doi.org/10.1609/aaai.v38i10.28956}.

\bibitem[\citeproctext]{ref-augustin2020adversarial}
Augustin, Maximilian, Alexander Meinke, and Matthias Hein. 2020.
{``{Adversarial Robustness on In- and Out-Distribution Improves
Explainability}.''} In \emph{Computer Vision -- ECCV 2020}, edited by
Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
228--45. Cham: Springer.

\bibitem[\citeproctext]{ref-balashankar2023improving}
Balashankar, Ananth, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed
Chi, Jilin Chen, and Alex Beutel. 2023. {``{Improving Classifier
Robustness through Active Generative Counterfactual Data
Augmentation}.''} In \emph{Findings of the Association for Computational
Linguistics: EMNLP 2023}, 127--39. ACL.
\url{https://doi.org/10.18653/v1/2023.findings-emnlp.10}.

\bibitem[\citeproctext]{ref-becker1996adult2}
Becker, Barry, and Ronny Kohavi. 1996. {``{Adult}.''} UCI Machine
Learning Repository.

\bibitem[\citeproctext]{ref-bell2024fairness}
Bell, Andrew, Joao Fonseca, Carlo Abrate, Francesco Bonchi, and Julia
Stoyanovich. 2024. {``{Fairness in Algorithmic Recourse Through the Lens
of Substantive Equality of Opportunity}.''}
\url{https://arxiv.org/abs/2401.16088}.

\bibitem[\citeproctext]{ref-bezanson2017julia}
Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017.
{``Julia: A Fresh Approach to Numerical Computing.''} \emph{SIAM Review}
59 (1): 65--98. \url{https://doi.org/10.1137/141000671}.

\bibitem[\citeproctext]{ref-bischl2023hyperparameter}
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter,
Stefan Coors, Janek Thomas, et al. 2023. {``{Hyperparameter
optimization: Foundations, algorithms, best practices, and open
challenges}.''} \emph{WIREs Data Mining and Knowledge Discovery} 13 (2):
e1484. https://doi.org/\url{https://doi.org/10.1002/widm.1484}.

\bibitem[\citeproctext]{ref-milan2023dataframes}
Bouchet-Valat, Milan, and Bogumi Kamiski. 2023. {``DataFrames.jl:
Flexible and Fast Tabular Data in Julia.''} \emph{Journal of Statistical
Software} 107 (4): 1--32. \url{https://doi.org/10.18637/jss.v107.i04}.

\bibitem[\citeproctext]{ref-byrne2021mpi}
Byrne, Simon, Lucas C. Wilcox, and Valentin Churavy. 2021. {``MPI.jl:
Julia Bindings for the Message Passing Interface.''} \emph{Proceedings
of the JuliaCon Conferences} 1 (1): 68.
\url{https://doi.org/10.21105/jcon.00068}.

\bibitem[\citeproctext]{ref-chagas2024pretty}
Chagas, Ronan Arraes Jardim, Ben Baumgold, Glen Hertz, Hendrik Ranocha,
Mark Wells, Nathan Boyer, Nicholas Ritchie, et al. 2024.
{``Ronisbr/PrettyTables.jl: V2.4.0.''} Zenodo.
\url{https://doi.org/10.5281/zenodo.13835553}.

\bibitem[\citeproctext]{ref-PlotsJL}
Christ, Simon, Daniel Schwabeneder, Christopher Rackauckas, Michael
Krabbe Borregaard, and Thomas Breloff. 2023. {``Plots.jl -- a User
Extendable Plotting API for the Julia Programming Language.''}
https://doi.org/\url{https://doi.org/10.5334/jors.431}.

\bibitem[\citeproctext]{ref-danisch2021makie}
Danisch, Simon, and Julius Krumbiegel. 2021. {``{Makie.jl}: Flexible
High-Performance Data Visualization for {Julia}.''} \emph{Journal of
Open Source Software} 6 (65): 3349.
\url{https://doi.org/10.21105/joss.03349}.

\bibitem[\citeproctext]{ref-du2019implicit}
Du, Yilun, and Igor Mordatch. 2020. {``{Implicit Generation and
Generalization in Energy-Based Models}.''}
\url{https://arxiv.org/abs/1903.08689}.

\bibitem[\citeproctext]{ref-frankle2018lottery}
Frankle, Jonathan, and Michael Carbin. 2019. {``{The Lottery Ticket
Hypothesis: Finding Sparse, Trainable Neural Networks}.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-freiesleben2022intriguing}
Freiesleben, Timo. 2022. {``{The Intriguing Relation Between
Counterfactual Explanations and Adversarial Examples}.''} \emph{Minds
and Machines} 32 (1): 77--109.

\bibitem[\citeproctext]{ref-goodfellow2016deep}
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \emph{Deep
{Learning}}. {MIT Press}.

\bibitem[\citeproctext]{ref-goodfellow2014explaining}
Goodfellow, Ian, Jonathon Shlens, and Christian Szegedy. 2015.
{``{Explaining and Harnessing Adversarial Examples}.''}
\url{https://arxiv.org/abs/1412.6572}.

\bibitem[\citeproctext]{ref-grathwohl2020your}
Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
Mohammad Norouzi, and Kevin Swersky. 2020. {``Your Classifier Is
Secretly an Energy Based Model and You Should Treat It Like One.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-gretton2012kernel}
Gretton, Arthur, Karsten M Borgwardt, Malte J Rasch, Bernhard Schlkopf,
and Alexander Smola. 2012. {``A Kernel Two-Sample Test.''} \emph{The
Journal of Machine Learning Research} 13 (1): 723--73.

\bibitem[\citeproctext]{ref-guidotti2022counterfactual}
Guidotti, Riccardo. 2022. {``{Counterfactual Explanations and How to
Find Them: Literature Review and Benchmarking}.''} \emph{Data Mining and
Knowledge Discovery} 38 (5): 2770--2824.
\url{https://doi.org/10.1007/s10618-022-00831-6}.

\bibitem[\citeproctext]{ref-guo2023counternet}
Guo, Hangzhi, Thanh H. Nguyen, and Amulya Yadav. 2023. {``{CounterNet:
End-to-End Training of Prediction Aware Counterfactual Explanations}.''}
In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining}, 577-\/-589. KDD '23. New York, NY, USA:
Association for Computing Machinery.
\url{https://doi.org/10.1145/3580305.3599290}.

\bibitem[\citeproctext]{ref-hastie2009elements}
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning}. Springer New York.
\url{https://doi.org/10.1007/978-0-387-84858-7}.

\bibitem[\citeproctext]{ref-innes2018fashionable}
Innes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco
Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and
Viral Shah. 2018. {``Fashionable Modelling with Flux.''}
\url{https://arxiv.org/abs/1811.01457}.

\bibitem[\citeproctext]{ref-innes2018flux}
Innes, Mike. 2018. {``Flux: {Elegant} Machine Learning with {Julia}.''}
\emph{Journal of Open Source Software} 3 (25): 602.
\url{https://doi.org/10.21105/joss.00602}.

\bibitem[\citeproctext]{ref-joshi2019realistic}
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and
Joydeep Ghosh. 2019. {``{Towards Realistic Individual Recourse and
Actionable Explanations in Black-Box Decision Making Systems}.''}
\url{https://arxiv.org/abs/1907.09615}.

\bibitem[\citeproctext]{ref-kaggle2011give2}
Kaggle. 2011. {``Give Me Some Credit, {Improve} on the State of the Art
in Credit Scoring by Predicting the Probability That Somebody Will
Experience Financial Distress in the Next Two Years.''}
https://www.kaggle.com/c/GiveMeSomeCredit; {Kaggle}.

\bibitem[\citeproctext]{ref-kolter2023keynote}
Kolter, Zico. 2023.{``{Keynote Addresses: SaTML 2023 }.''} In \emph{2023
IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}. Los
Alamitos, CA, USA: IEEE Computer Society.
\url{https://doi.org/10.1109/SaTML54575.2023.00009}.

\bibitem[\citeproctext]{ref-lakshminarayanan2016simple}
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2017.
{``Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.''} In \emph{Proceedings of the 31st International Conference
on Neural Information Processing Systems}, 6405--16. NIPS'17. Red Hook,
NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-lecun1998mnist}
LeCun, Yann. 1998. {``{The MNIST database of handwritten digits}.''}
http://yann.lecun.com/exdb/mnist/.

\bibitem[\citeproctext]{ref-lippe2024uvadlc}
Lippe, Phillip. 2024. {``{UvA Deep Learning Tutorials}.''}
\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}.

\bibitem[\citeproctext]{ref-luu2023counterfactual}
Luu, Hoai Linh, and Naoya Inoue. 2023. {``{Counterfactual Adversarial
Training for Improving Robustness of Pre-trained Language Models}.''} In
\emph{Proceedings of the 37th Pacific Asia Conference on Language,
Information and Computation}, 881--88. ACL.
\url{https://aclanthology.org/2023.paclic-1.88/}.

\bibitem[\citeproctext]{ref-mcgregor2021preventing}
McGregor, Sean. 2021. {``{Preventing repeated real world AI failures by
cataloging incidents: The AI incident database}.''} In \emph{Proceedings
of the AAAI Conference on Artificial Intelligence}, 35:15458--63. 17.

\bibitem[\citeproctext]{ref-murphy2022probabilistic}
Murphy, Kevin P. 2022. \emph{Probabilistic {Machine Learning}: {An}
Introduction}. {MIT Press}.

\bibitem[\citeproctext]{ref-oneil2016weapons}
O'Neil, Cathy. 2016. \emph{Weapons of Math Destruction: {How} Big Data
Increases Inequality and Threatens Democracy}. {Crown}.

\bibitem[\citeproctext]{ref-pace1997sparse}
Pace, R Kelley, and Ronald Barry. 1997. {``Sparse Spatial
Autoregressions.''} \emph{Statistics \& Probability Letters} 33 (3):
291--97. \url{https://doi.org/10.1016/s0167-7152(96)00140-x}.

\bibitem[\citeproctext]{ref-pawelczyk2022exploring}
Pawelczyk, Martin, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and
Himabindu Lakkaraju. 2022. {``Exploring Counterfactual Explanations
Through the Lens of Adversarial Examples: A Theoretical and Empirical
Analysis.''} In \emph{Proceedings of the 25th International Conference
on Artificial Intelligence and Statistics}, edited by Gustau
Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, 151:4574--94.
Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v151/pawelczyk22a.html}.

\bibitem[\citeproctext]{ref-poyiadzi2020face}
Poyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and
Peter Flach. 2020. {``{FACE}: {Feasible} and Actionable Counterfactual
Explanations.''} In \emph{Proceedings of the {AAAI}/{ACM Conference} on
{AI}, {Ethics}, and {Society}}, 344--50.

\bibitem[\citeproctext]{ref-ross2021learning}
Ross, Alexis, Himabindu Lakkaraju, and Osbert Bastani. 2024.
{``{Learning Models for Actionable Recourse}.''} In \emph{Proceedings of
the 35th International Conference on Neural Information Processing
Systems}. NIPS '21. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-sauer2021counterfactual}
Sauer, Axel, and Andreas Geiger. 2021. {``{Counterfactual Generative
Networks}.''} \url{https://arxiv.org/abs/2101.06046}.

\bibitem[\citeproctext]{ref-schut2021generating}
Schut, Lisa, Oscar Key, Rory McGrath, Luca Costabello, Bogdan Sacaleanu,
Yarin Gal, et al. 2021. {``Generating {Interpretable Counterfactual
Explanations By Implicit Minimisation} of {Epistemic} and {Aleatoric
Uncertainties}.''} In \emph{International {Conference} on {Artificial
Intelligence} and {Statistics}}, 1756--64. {PMLR}.

\bibitem[\citeproctext]{ref-sharma2020certifai}
Sharma, Shubham, Jette Henderson, and Joydeep Ghosh. 2020. {``{CERTIFAI:
A Common Framework to Provide Explanations and Analyse the Fairness and
Robustness of Black-box Models}.''} In \emph{Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society}, 166--72. AIES '20. New York, NY,
USA: Association for Computing Machinery.
\url{https://doi.org/10.1145/3375627.3375812}.

\bibitem[\citeproctext]{ref-spooner2021counterfactual}
Spooner, Thomas, Danial Dervovic, Jason Long, Jon Shepard, Jiahao Chen,
and Daniele Magazzeni. 2021. {``{Counterfactual Explanations for
Arbitrary Regression Models}.''} \url{https://arxiv.org/abs/2106.15212}.

\bibitem[\citeproctext]{ref-szegedy2013intriguing}
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. {``Intriguing
Properties of Neural Networks.''} \url{https://arxiv.org/abs/1312.6199}.

\bibitem[\citeproctext]{ref-teh2003energy}
Teh, Yee Whye, Max Welling, Simon Osindero, and Geoffrey E. Hinton.
2003. {``Energy-Based Models for Sparse Overcomplete Representations.''}
\emph{J. Mach. Learn. Res.} 4 (null): 1235--60.

\bibitem[\citeproctext]{ref-teney2020learning}
Teney, Damien, Ehsan Abbasnedjad, and Anton van den Hengel. 2020.
{``Learning What Makes a Difference from Counterfactual Examples and
Gradient Supervision.''} In \emph{Computer Vision - ECCV 2020}, 580--99.
Berlin, Heidelberg: Springer-Verlag.
\url{https://doi.org/10.1007/978-3-030-58607-2_34}.

\bibitem[\citeproctext]{ref-venkatasubramanian2020philosophical}
Venkatasubramanian, Suresh, and Mark Alfano. 2020. {``The Philosophical
Basis of Algorithmic Recourse.''} In \emph{Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency}, 284--93. FAT*
'20. New York, NY, USA: Association for Computing Machinery.
\url{https://doi.org/10.1145/3351095.3372876}.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box:
{Automated} Decisions and the {GDPR}.''} \emph{Harv. JL \& Tech.} 31:
841. \url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wilson2020case}
Wilson, Andrew Gordon. 2020. {``{The Case for Bayesian Deep
Learning}.''} \url{https://arxiv.org/abs/2001.10995}.

\bibitem[\citeproctext]{ref-wu2021polyjuice2}
Wu, Tongshuang, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld.
2021. {``Polyjuice: Generating Counterfactuals for Explaining,
Evaluating, and Improving Models.''} In \emph{Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers)}, 6707--23. ACL.
\url{https://doi.org/10.18653/v1/2021.acl-long.523}.

\bibitem[\citeproctext]{ref-yeh2016default}
Yeh, I-Cheng. 2016. {``{Default of Credit Card Clients}.''} UCI Machine
Learning Repository.

\bibitem[\citeproctext]{ref-zhang2021understanding}
Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol
Vinyals. 2021. {``Understanding Deep Learning (Still) Requires
Rethinking Generalization.''} \emph{Commun. ACM} 64 (3): 107--15.
\url{https://doi.org/10.1145/3446776}.

\end{CSLReferences}

\newpage{}

\begin{appendices}

\FloatBarrier

\section{Notation}\label{notation}

\begin{itemize}
\tightlist
\item
  \(y^+\): The target class and also the index of the target class.
\item
  \(y^-\): The non-target class and also the index of non-the target
  class.
\item
  \(\mathbf{y}^+\): The one-hot encoded output vector for the target
  class.
\item
  \(\theta\): Model parameters (unspecified).
\item
  \(\Theta\): Matrix of parameters.
\end{itemize}

\subsection{Other Technical Details}\label{other-technical-details}

\begin{equation}\phantomsection\label{eq-mmd}{
\begin{aligned}
MMD({X}^\prime,\tilde{X}^\prime) &= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j)
\end{aligned}
}\end{equation}

In our implementation, Equation~\ref{eq-mmd} is by default applied to
the entire subset of the training data for which \(y=y^+\).

\section{Technical Details of Our
Approach}\label{technical-details-of-our-approach}

\subsection{Generating Counterfactuals through Gradient
Descent}\label{sec-app-ce}

In this section, we provide some background on gradient-based
counterfactual generators (Section~\ref{sec-app-ce-background}) and
discuss how we define convergence in this context
(Section~\ref{sec-app-conv}).

\subsubsection{Background}\label{sec-app-ce-background}

Gradient-based counterfactual search was originally proposed by Wachter,
Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). It generally solves
the following unconstrained objective,

\[
\begin{aligned}
\min_{\mathbf{z}^\prime \in \mathcal{Z}^L} \left\{  {\text{yloss}(\mathbf{M}_{\theta}(g(\mathbf{z}^\prime)),\mathbf{y}^+)}+ \lambda {\text{cost}(g(\mathbf{z}^\prime)) }  \right\} 
\end{aligned} 
\]

where \(g: \mathcal{Z} \mapsto \mathcal{X}\) is an invertible function
that maps from the \(L\)-dimensional counterfactual state space to the
feature space and \(\text{cost}(\cdot)\) denotes one or more penalties
that are used to induce certain properties of the counterfactual
outcome. As above, \(\mathbf{y}^+\) denotes the target output and
\(\mathbf{M}_{\theta}(\mathbf{x})\) returns the logit predictions of the
underlying classifier for \(\mathbf{x}=g(\mathbf{z})\).

For all generators used in this work we use standard logit crossentropy
loss for \(\text{yloss}(\cdot)\). All generators also penalize the
distance (\(\ell_1\)-norm) of counterfactuals from their original
factual state. For \emph{Generic} and \emph{ECCo}, we have
\(\mathcal{Z}:=\mathcal{X}\) and
\(g(\mathbf{z})=g(\mathbf{z})^{-1}=\mathbf{z}\), that is counterfactual
are searched directly in the feature space. Conversely, \emph{REVISE}
traverses the latent space of a variational autoencoder (VAE) fitted to
the training data, where \(g(\cdot)\) corresponds to the decoder
(\citeproc{ref-joshi2019realistic}{Joshi et al. 2019}). In addition to
the distance penalty, \emph{ECCo} uses an additional penalty component
that regularizes the energy associated with the counterfactual,
\(\mathbf{x}^\prime\) (\citeproc{ref-altmeyer2024faithful}{Altmeyer et
al. 2024}).

\subsubsection{Convergence}\label{sec-app-conv}

An important consideration when generating counterfactual explanations
using gradient-based methods is how to define convergence. Two common
choices are to 1) perform gradient descent over a fixed number of
iterations \(T\), or 2) conclude the search as soon as the predicted
probability for the target class has reached a pre-determined threshold,
\(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\). We
prefer the latter for our purposes, because it explicitly defines
convergence in terms of the black-box model, \(\mathbf{M}(\mathbf{x})\).

Defining convergence in this way allows for a more intuitive
interpretation of the resulting counterfactual outcomes than with fixed
\(T\). Specifically, it allows us to think of counterfactuals as
explaining `high-confidence' predictions by the model for the target
class \(y^+\). Depending on the context and application, different
choices of \(\tau\) can be considered as representing `high-confidence'
predictions.

\subsection{Protecting Mutability Constraints with Linear
Classifiers}\label{sec-app-constraints}

In Section~\ref{sec-constraints} we explain that to avoid penalizing
implausibility that arises due to mutability constraints, we impose a
point mass prior on \(p(\mathbf{x})\) for the corresponding feature. We
argue in Section~\ref{sec-constraints} that this approach induces models
to be less sensitive to immutable features and demonstrate this
empirically in Section~\ref{sec-experiments}. Below we derive the
analytical results in Prp.\textasciitilde{}\ref{prp-mtblty}.

\begin{proof}
Let \(d_{\text{mtbl}}\) and \(d_{\text{immtbl}}\) denote some mutable
and immutable feature, respectively. Suppose that
\(\mu_{y^-,d_{\text{immtbl}}} < \mu_{y^+,d_{\text{immtbl}}}\) and
\(\mu_{y^-,d_{\text{mtbl}}} > \mu_{y^+,d_{\text{mtbl}}}\), where
\(\mu_{k,d}\) denotes the conditional sample mean of feature \(d\) in
class \(k\). In words, we assume that the immutable feature tends to
take lower values for samples in the non-target class \(y^-\) than in
the target class \(y^+\). We assume the opposite to hold for the mutable
feature.

Assuming multivariate Gaussian class densities with common diagonal
covariance matrix \(\Sigma_k=\Sigma\) for all \(k \in \mathcal{K}\), we
have for the log likelihood ratio between any two classes
\(k,m \in \mathcal{K}\) (\citeproc{ref-hastie2009elements}{Hastie,
Tibshirani, and Friedman 2009}):

\begin{equation}\phantomsection\label{eq-loglike}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})}=\mathbf{x}^\intercal \Sigma^{-1}(\mu_{k}-\mu_{m})  + \text{const}
}\end{equation}

By independence of \(x_1,...,x_D\), the full log-likelihood ratio
decomposes into:

\begin{equation}\phantomsection\label{eq-loglike-decomp}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})} = \sum_{d=1}^D \frac{\mu_{k,d}-\mu_{m,d}}{\sigma_{d}^2} x_{d} + \text{const}
}\end{equation}

By the properties of our classifier (\emph{multinomial logistic
regression}), we have:

\begin{equation}\phantomsection\label{eq-multi}{
\log \frac{p(k|\mathbf{x})}{p(m|\mathbf{x})} = \sum_{d=1}^D \left( \theta_{k,d} - \theta_{m,d} \right)x_d + \text{const}
}\end{equation}

where \(\theta_{k,d}=\Theta[k,d]\) denotes the coefficient on feature
\(d\) for class \(k\).

Based on Equation~\ref{eq-loglike-decomp} and Equation~\ref{eq-multi} we
can identify that
\((\mu_{k,d}-\mu_{m,d}) \propto (\theta_{k,d} - \theta_{m,d})\) under
the assumptions we made above. Hence, we have that
\((\theta_{y^-,d_{\text{immtbl}}} - \theta_{y^+,d_{\text{immtbl}}}) < 0\)
and
\((\theta_{y^-,d_{\text{mtbl}}} - \theta_{y^+,d_{\text{mtbl}}}) > 0\)

Let \(\mathbf{x}^\prime\) denote some randomly chosen individual from
class \(y^-\) and let \(y^+ \sim p(y)\) denote the randomly chosen
target class. Then the partial derivative of the contrastive divergence
penalty Equation~\ref{eq-div} with respect to coefficient
\(\theta_{y^+,d}\) is equal to

\begin{equation}\phantomsection\label{eq-grad}{
\frac{\partial}{\partial\theta_{y^+,d}} \left(\text{div}(\mathbf{x}^+,\mathbf{x^\prime},\mathbf{y};\theta)\right) = \frac{\partial}{\partial\theta_{y^+,d}} \left( \left(-\mathbf{M}_\theta(\mathbf{x}^+)[y^+]\right) - \left(-\mathbf{M}_\theta(\mathbf{x}^\prime)[y^+]\right) \right) = x_{d}^\prime - x^+_{d}
}\end{equation}

and equal to zero everywhere else.

Since \((\mu_{y^-,d_{\text{immtbl}}} < \mu_{y^+,d_{\text{immtbl}}})\) we
are more likely to have
\((x_{d_{\text{immtbl}}}^\prime - x^+_{d_{\text{immtbl}}}) < 0\) than
vice versa at initialization. Similarly, we are more likely to have
\((x_{d_{\text{mtbl}}}^\prime - x^+_{d_{\text{mtbl}}}) > 0\) since
\((\mu_{y^-,d_{\text{mtbl}}} > \mu_{y^+,d_{\text{mtbl}}})\).

This implies that if we do not protect feature \(d_{\text{immtbl}}\),
the contrastive divergence penalty will decrease
\(\theta_{y^-,d_{\text{immtbl}}}\) thereby exacerbating the existing
effect
\((\theta_{y^-,d_{\text{immtbl}}} - \theta_{y^+,d_{\text{immtbl}}}) < 0\).
In words, not protecting the immutable feature would have the
undesirable effect of making the classifier more sensitive to this
feature, in that it would be more likely to predict class \(y^-\) as
opposed to \(y^+\) for lower values of \(d_{\text{immtbl}}\).

By the same rationale, the contrastive divergence penalty can generally
be expected to increase \(\theta_{y^-,d_{\text{mtbl}}}\) exacerbating
\((\theta_{y^-,d_{\text{mtbl}}} - \theta_{y^+,d_{\text{mtbl}}}) > 0\).
In words, this has the effect of making the classifier more sensitive to
the mutable feature, in that it would be more likely to predict class
\(y^-\) as opposed to \(y^+\) for higher values of \(d_{\text{mtbl}}\).

Thus, our proposed approach of protecting feature \(d_{\text{immtbl}}\)
has the net affect of decreasing the classifier's sensitivity to the
immutable feature relative to the mutable feature (i.e.~no change in
sensitivity for \(d_{\text{immtbl}}\) relative to increased sensitivity
for \(d_{\text{mtbl}}\)).
\end{proof}

\subsection{Domain Constraints}\label{domain-constraints}

We apply domain constraints on counterfactuals during training and
evaluation. There are at least two good reasons for doing so. Firstly,
within the context of explainability and algorithmic recourse,
real-world attributes are often domain constrained: the \emph{age}
feature, for example, is lower bounded by zero and upper bounded by the
maximum human lifespan. Secondly, domain constraints help mitigate
training instabilities commonly associated with energy-based modelling
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020};
\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).

For our image datasets, features are pixel values and hence the domain
is constrained by the lower and upper bound of values that pixels can
take depending on how they are scaled (in our case \([-1,1]\)). For all
other features \(d\) in our synthetic and tabular datasets, we
automatically infer domain constraints
\([x_d^{\text{LB}},x_d^{\text{UB}}]\) as follows,

\begin{equation}\phantomsection\label{eq-domain}{
\begin{aligned}
x_d^{\text{LB}} &= \arg\min_{x_d} \{\mu_d - n_{\sigma_d}\sigma_d, \arg \min_{x_d} x_d\} \\
x_d^{\text{UB}} &= \arg\max_{x_d} \{\mu_d + n_{\sigma_d}\sigma_d, \arg \max_{x_d} x_d\} 
\end{aligned}
}\end{equation}

where \(\mu_d\) and \(\sigma_d\) denote the sample mean and standard
deviation of feature \(d\). We set \(n_{\sigma_d}=3\) across the board
but higher values and hence wider bounds may be appropriate depending on
the application.

\subsection{Training Details}\label{sec-app-training}

In this section, we describe the training procedure in detail. While the
details laid out here are not crucial for understanding our proposed
approach, they are of importance to anyone looking to implement
counterfactual training.

\subsubsection{Default Hyperparameters}\label{default-hyperparameters}

Note~\ref{nte-train-default} presents the default hyperparameters used
during training.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-train-default}: Training Phase}]

\quartocalloutnte{nte-train-default} 

\begin{itemize}
\tightlist
\item
  Meta Parameters:

  \begin{itemize}
  \tightlist
  \item
    Generator: \texttt{ecco}
  \item
    Model: \texttt{mlp}
  \end{itemize}
\item
  Model:

  \begin{itemize}
  \tightlist
  \item
    Activation: \texttt{relu}
  \item
    No.~Hidden: \texttt{32}
  \item
    No.~Layers: \texttt{1}
  \end{itemize}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    Burnin: \texttt{0.0}
  \item
    Class Loss: \texttt{logitcrossentropy}
  \item
    Convergence: \texttt{threshold}
  \item
    Generator Parameters:

    \begin{itemize}
    \tightlist
    \item
      Decision Threshold: \texttt{0.75}
    \item
      \(\lambda_{\text{cst}}\): \texttt{0.001}
    \item
      \(\lambda_{\text{egy}}\): \texttt{5.0}
    \item
      Learning Rate: \texttt{0.25}
    \item
      Maximum Iterations: \texttt{30}
    \item
      Optimizer: \texttt{sgd}
    \item
      Type: \texttt{ECCo}
    \end{itemize}
  \item
    \(\lambda_{\text{adv}}\): \texttt{0.25}
  \item
    \(\lambda_{\text{clf}}\): \texttt{1.0}
  \item
    \(\lambda_{\text{div}}\): \texttt{0.5}
  \item
    \(\lambda_{\text{reg}}\): \texttt{0.1}
  \item
    Learning Rate: \texttt{0.001}
  \item
    No.~Counterfactuals: \texttt{1000}
  \item
    No.~Epochs: \texttt{100}
  \item
    Objective: \texttt{full}
  \item
    Optimizer: \texttt{adam}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsection{Evaluation Details}\label{sec-app-eval}

Note~\ref{nte-eval-default} presents the default hyperparameters used
during evaluation.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-eval-default}: Evaluation Phase}]

\quartocalloutnte{nte-eval-default} 

\begin{itemize}
\tightlist
\item
  Counterfactual Parameters:

  \begin{itemize}
  \tightlist
  \item
    Convergence: \texttt{threshold}
  \item
    Decision Threshold: \texttt{0.95}
  \item
    Generator Parameters:

    \begin{itemize}
    \tightlist
    \item
      Decision Threshold: \texttt{0.75}
    \item
      \(\lambda_{\text{cst}}\): \texttt{0.001}
    \item
      \(\lambda_{\text{egy}}\): \texttt{5.0}
    \item
      Learning Rate: \texttt{0.25}
    \item
      Maximum Iterations: \texttt{30}
    \item
      Optimizer: \texttt{sgd}
    \item
      Type: \texttt{ECCo}
    \end{itemize}
  \item
    Maximum Iterations: \texttt{50}
  \item
    No.~Individuals: \texttt{100}
  \item
    No.~Runs: \texttt{5}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\section{Details on Main Experiments}\label{sec-app-main}

\subsection{Final Hyperparameters}\label{final-hyperparameters}

As discussed Section~\ref{sec-experiments}, CT is sensitive to certain
hyperparameter choices. We study the effect of many hyperparameters
extensively in Section~\ref{sec-app-grid}. For the main results, we tune
a small set of key hyperparameters (Section~\ref{sec-app-tune}). The
final choices for the main results are presented for each data set in
Table~\ref{tbl-final-params} along with training, test and batch sizes.

\begin{table}

\caption{\label{tbl-final-params}Final hyperparameters used for the main
results preseneted in Section~\ref{sec-experiments}. Any hyperparameter
not shown here is set to its default value
(Note~\ref{nte-train-default}).}

\centering{

  \begin{tabular}{cccccccc}
    \toprule
    \textbf{Data} & \textbf{No. Train} & \textbf{No. Test} & \textbf{Batchsize} & \textbf{Domain} & \textbf{Decision Threshold} & \textbf{No. Counterfactuals} & \textbf{$\lambda_{\text{reg}}$} \\\midrule
    Adult & 26049 & 5010 & 1000 & none & 0.75 & 5000 & 0.25 \\
    CH & 16504 & 3101 & 1000 & none & 0.5 & 5000 & 0.25 \\
    Circ & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.5 \\
    Cred & 10617 & 1923 & 1000 & none & 0.5 & 5000 & 0.25 \\
    GMSC & 13371 & 2474 & 1000 & none & 0.5 & 5000 & 0.5 \\
    LS & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.01 \\
    MNIST & 11000 & 2000 & 1000 & (-1.0, 1.0) & 0.5 & 5000 & 0.01 \\
    Moon & 3600 & 600 & 30 & none & 0.9 & 1000 & 0.25 \\
    OL & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.25 \\\bottomrule
  \end{tabular}

}

\end{table}%

\subsection{Qualitative Findings for Image
Data}\label{qualitative-findings-for-image-data}

Figure~\ref{fig-mnist} shows much more plausible (faithful)
counterfactuals for a model with CT than the model with conventional
training (Figure~\ref{fig-mnist-vanilla}).

\begin{figure}

\begin{minipage}{0.46\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_mlp.png}}

}

\caption{\label{fig-mnist}Counterfactual images for \emph{MLP} with
counterfactual training. Factual images are shown on the diagonal, with
the corresponding counterfactual for each target class (columns) in that
same row. The underlying generator, \emph{ECCo}, aims to generate
counterfactuals that are faithful to the model
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).}

\end{figure}%

\end{minipage}%
%
\begin{minipage}{0.09\linewidth}
~\end{minipage}%
%
\begin{minipage}{0.46\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_mlp_vanilla.png}}

}

\caption{\label{fig-mnist-vanilla}The same setup, factuals, model
architecture and generator as in Figure~\ref{fig-mnist}, but the model
was trained with CT.}

\end{figure}%

\end{minipage}%

\end{figure}%

\FloatBarrier

\section{Grid Searches}\label{sec-app-grid}

To assess the hyperparameter sensitivity of our proposed training regime
we ran multiple large grid searches for all of our synthetic datasets.
We have grouped these grid searches into multiple categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Generator Parameters} (Section~\ref{sec-app-grid-gen}):
  Investigates the effect of changing hyperparameters that affect the
  counterfactual outcomes during the training phase.
\item
  \textbf{Penalty Strengths} (Section~\ref{sec-app-grid-pen}):
  Investigates the effect of changing the penalty strengths in out
  proposed objective (Equation~\ref{eq-obj}).
\item
  \textbf{Other Parameters} (Section~\ref{sec-app-grid-train}):
  Investigates the effect of changing other training parameters,
  including the total number of generated counterfactuals in each epoch.
\end{enumerate}

We begin by summarizing the high-level findings in
Section~\ref{sec-app-grid-hl}. For each of the categories,
Section~\ref{sec-app-grid-gen} to Section~\ref{sec-app-grid-train} then
present all details including the exact parameter grids, average
predictive performance outcomes and key evaluation metrics for the
generated counterfactuals.

\subsection{Evaluation Details}\label{evaluation-details}

To measure predictive performance, we compute the accuracy and F1-score
for all models on test data (Table~\ref{tbl-acc-gen},
Table~\ref{tbl-acc-pen}, Table~\ref{tbl-acc-train}). With respect to
explanatory performance, we report here our findings for the
(im)plausibility and cost of counterfactuals at test time. Since the
computation of our proposed divergence metric
(Equation~\ref{eq-impl-div}) is memory-intensive, we rely on the
distance-based metric for the grid searches. For the counterfactual
evaluation, we draw factual samples from the training data for the grid
searches to avoid data leakage with respect to our final results
reported in the body of the paper. Specifically, we want to avoid
choosing our default hyperparameters based on results on the test data.
Since we are optimizing for explainability, not predictive performance,
we still present test accuracy and F1-scores.

\subsubsection{Predictive Performance}\label{predictive-performance}

We find that CT is associated with little to no decrease in average
predictive performance for our synthetic datasets: test accuracy and
F1-scores decrease by at most \textasciitilde1 percentage point, but
generally much less (Table~\ref{tbl-acc-gen}, Table~\ref{tbl-acc-pen},
Table~\ref{tbl-acc-train}). Variation across hyperparameters is
negligible as indicated by small standard deviations for these metrics
across the board.

\subsubsection{Counterfactual Outcomes}\label{sec-app-grid-hl}

Overall, we find that counterfactual training (CT) achieves it key
objectives consistently across all hyperparameter settings and also
broadly across datasets: plausibility is improved by up to
\textasciitilde60 percent (\%) for the \emph{Circles} data (e.g.
Figure~\ref{fig-grid-gen_params-plaus-circles}), \textasciitilde25-30\%
for the \emph{Moons} data (e.g.
Figure~\ref{fig-grid-gen_params-plaus-moons}) and \textasciitilde10-20\%
for the \emph{Linearly Separable} data (e.g.
Figure~\ref{fig-grid-gen_params-plaus-lin_sep}). At the same time, the
average costs of faithful counterfactuals are reduced in many cases by
around \textasciitilde20-25\% for \emph{Circles} (e.g.
Figure~\ref{fig-grid-gen_params-cost-circles}) and up to
\textasciitilde50\% for \emph{Moons} (e.g.
Figure~\ref{fig-grid-gen_params-cost-moons}). For the \emph{Linearly
Separable} data, costs are generally increased although typically by
less than 10\% (e.g. Figure~\ref{fig-grid-gen_params-cost-lin_sep}),
which reflects a common tradeoff between costs and plausibility
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).

We do observe strong sensitivity to certain hyperparameters, with clear
an manageable patterns. Concerning generator parameters, we firstly find
that using \emph{REVISE} to generate counterfactuals during training
typically yields the worst outcomes out of all generators, often leading
to a substantial decrease in plausibility. This finding can be
attributed to the fact that \emph{REVISE} effectively assigns the task
of learning plausible explanations from the model itself to a surrogate
VAE. In other words, counterfactuals generated by \emph{REVISE} are less
faithful to the model that \emph{ECCo} and \emph{Generic}, and hence we
would expect them to be a less effective and, in fact, potentially
detrimental role in our training regime. Secondly, we observe that
allowing for a higher number of maximum steps \(T\) for the
counterfactual search generally yields better outcomes. This is
intuitive, because it allows more counterfactuals to reach maturity in
any given iteration. Looking in particular at the results for
\emph{Linearly Separable}, it seems that higher values for \(T\) in
combination with higher decision thresholds (\(\tau\)) yields the best
results when using \emph{ECCo}. But depending on the degree of class
separability of the underlying data, a high decision-threshold can also
affect results adversely, as evident from the results for the
\emph{Overlapping} data (Figure~\ref{fig-grid-gen_params-plaus-over}):
here we find that CT generally fails to achieve its objective because
only a tiny proportion of counterfactuals ever reaches maturity.

Regarding penalty strengths, we find that the strength of the energy
regularization, \(\lambda_{\text{reg}}\) is a key hyperparameter, while
sensitivity with respect to \(\lambda_{\text{div}}\) and
\(\lambda_{\text{adv}}\) is much less evident. In particular, we observe
that not regularizing energy enough or at all typically leads to poor
performance in terms of decreased plausibility and increased costs, in
particular for \emph{Circles} (Figure~\ref{fig-grid-pen-plaus-circles}),
\emph{Linearly Separable} (Figure~\ref{fig-grid-pen-plaus-lin_sep}) and
\emph{Overlapping} (Figure~\ref{fig-grid-pen-plaus-over}). High values
of \(\lambda_{\text{reg}}\) can increase the variability in outcomes, in
particular when combined with high values for \(\lambda_{\text{div}}\)
and \(\lambda_{\text{adv}}\), but this effect is less pronounced.

Finally, concerning other hyperparameters we observe that the
effectiveness and stability of CT is positively associated with the
number of counterfactuals generated during each training epoch, in
particular for \emph{Circles}
(Figure~\ref{fig-grid-train-plaus-circles}) and \emph{Moons}
(Figure~\ref{fig-grid-train-plaus-moons}). We further find that a higher
number of training epochs is beneficial as expected, where we tested
training models for 50 and 100 epochs. Interestingly, we find that it is
not necessary to employ CT during the entire training phase to achieve
the desired improvements in explainability: specifically, we have tested
training models conventionally during the first half of training before
switching to CT after this initial burn-in period.

\subsection{Generator Parameters}\label{sec-app-grid-gen}

The hyperparameter grid with varying generator parameters during
training is shown in Note~\ref{nte-gen-params-final-run-train}. The
corresponding evaluation grid used for these experiments is shown in
Note~\ref{nte-gen-params-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-gen-params-final-run-train}: Training Phase}]

\quartocalloutnte{nte-gen-params-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    Decision Threshold: \texttt{0.75,\ 0.9,\ 0.95}
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 5.0,\ 10.0,\ 20.0}
  \item
    Maximum Iterations: \texttt{5,\ 25,\ 50}
  \end{itemize}
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-gen-params-final-run-eval}: Evaluation Phase}]

\quartocalloutnte{nte-gen-params-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsubsection{Predictive Performance}\label{predictive-performance-1}

Predictive performance measures for this grid search are shown in
Table~\ref{tbl-acc-gen}.

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-gen}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-gen-params-final-run-train}) and evaluation-phase
parameters (Note~\ref{nte-gen-params-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circ & Accuracy & Full & 1.0 & 0.0 \\
  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\
  Circ & F1-score & Full & 1.0 & 0.0 \\
  Circ & F1-score & Vanilla & 1.0 & 0.0 \\
  LS & Accuracy & Full & 1.0 & 0.0 \\
  LS & Accuracy & Vanilla & 1.0 & 0.0 \\
  LS & F1-score & Full & 1.0 & 0.0 \\
  LS & F1-score & Vanilla & 1.0 & 0.0 \\
  Moon & Accuracy & Full & 1.0 & 0.0 \\
  Moon & Accuracy & Vanilla & 1.0 & 0.0 \\
  Moon & F1-score & Full & 1.0 & 0.0 \\
  Moon & F1-score & Vanilla & 1.0 & 0.0 \\
  OL & Accuracy & Full & 0.91 & 0.0 \\
  OL & Accuracy & Vanilla & 0.92 & 0.0 \\
  OL & F1-score & Full & 0.91 & 0.0 \\
  OL & F1-score & Vanilla & 0.92 & 0.0 \\\bottomrule

\end{longtable}

\subsubsection{Plausibility}\label{plausibility}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-gen_params-plaus-circles} to
Figure~\ref{fig-grid-gen_params-plaus-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-gen_params-plaus-circles}Average outcomes for
the plausibility measure across hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-gen_params-plaus-lin_sep}Average outcomes for
the plausibility measure across hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-gen_params-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-gen_params-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Overlapping.}

\end{figure}%

\subsubsection{Cost}\label{cost}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-gen_params-cost-circles} to
Figure~\ref{fig-grid-gen_params-cost-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}

}

\caption{\label{fig-grid-gen_params-cost-circles}Average outcomes for
the cost measure across hyperparameters. This shows the \% change from
the baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}

}

\caption{\label{fig-grid-gen_params-cost-lin_sep}Average outcomes for
the cost measure across hyperparameters. This shows the \% change from
the baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Linearly
Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}

}

\caption{\label{fig-grid-gen_params-cost-moons}Average outcomes for the
cost measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png}

}

\caption{\label{fig-grid-gen_params-cost-over}Average outcomes for the
cost measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data:
Overlapping.}

\end{figure}%

\subsection{Penalty Strengths}\label{sec-app-grid-pen}

The hyperparameter grid with varying penalty strengths during training
is shown in Note~\ref{nte-pen-final-run-train}. The corresponding
evaluation grid used for these experiments is shown in
Note~\ref{nte-pen-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-pen-final-run-train}: Training Phase}]

\quartocalloutnte{nte-pen-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{adv}}\): \texttt{0.1,\ 0.25,\ 1.0}
  \item
    \(\lambda_{\text{div}}\): \texttt{0.01,\ 0.1,\ 1.0}
  \item
    \(\lambda_{\text{reg}}\): \texttt{0.0,\ 0.01,\ 0.1,\ 0.25,\ 0.5}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-pen-final-run-eval}: Evaluation Phase}]

\quartocalloutnte{nte-pen-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsubsection{Predictive Performance}\label{predictive-performance-2}

Predictive performance measures for this grid search are shown in
Table~\ref{tbl-acc-pen}.

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-pen}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-pen-final-run-train}) and evaluation-phase parameters
(Note~\ref{nte-pen-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circ & Accuracy & Full & 0.99 & 0.01 \\
  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\
  Circ & F1-score & Full & 0.99 & 0.01 \\
  Circ & F1-score & Vanilla & 1.0 & 0.0 \\
  LS & Accuracy & Full & 1.0 & 0.01 \\
  LS & Accuracy & Vanilla & 1.0 & 0.0 \\
  LS & F1-score & Full & 1.0 & 0.01 \\
  LS & F1-score & Vanilla & 1.0 & 0.0 \\
  Moon & Accuracy & Full & 0.99 & 0.04 \\
  Moon & Accuracy & Vanilla & 1.0 & 0.01 \\
  Moon & F1-score & Full & 0.99 & 0.04 \\
  Moon & F1-score & Vanilla & 1.0 & 0.01 \\
  OL & Accuracy & Full & 0.91 & 0.02 \\
  OL & Accuracy & Vanilla & 0.92 & 0.0 \\
  OL & F1-score & Full & 0.91 & 0.02 \\
  OL & F1-score & Vanilla & 0.92 & 0.0 \\\bottomrule

\end{longtable}

\subsubsection{Plausibility}\label{plausibility-1}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-pen-plaus-circles} to
Figure~\ref{fig-grid-pen-plaus-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-pen-plaus-circles}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-pen-plaus-lin_sep}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-pen-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-pen-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Overlapping.}

\end{figure}%

\subsubsection{Cost}\label{cost-1}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-pen-cost-circles} to
Figure~\ref{fig-grid-pen-cost-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}

}

\caption{\label{fig-grid-pen-cost-circles}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}

}

\caption{\label{fig-grid-pen-cost-lin_sep}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Linearly
Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}

}

\caption{\label{fig-grid-pen-cost-moons}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png}

}

\caption{\label{fig-grid-pen-cost-over}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data:
Overlapping.}

\end{figure}%

\subsection{Other Parameters}\label{sec-app-grid-train}

The hyperparameter grid with other varying training parameters is shown
in Note~\ref{nte-train-final-run-train}. The corresponding evaluation
grid used for these experiments is shown in
Note~\ref{nte-train-final-run-eval}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-train-final-run-train}: Training Phase}]

\quartocalloutnte{nte-train-final-run-train} 

\begin{itemize}
\tightlist
\item
  Generator: \texttt{ecco,\ generic,\ revise}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    Burnin: \texttt{0.0,\ 0.5}
  \item
    No.~Counterfactuals: \texttt{100,\ 1000}
  \item
    No.~Epochs: \texttt{50,\ 100}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-train-final-run-eval}: Evaluation Phase}]

\quartocalloutnte{nte-train-final-run-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsubsection{Predictive Performance}\label{predictive-performance-3}

Predictive performance measures for this grid search are shown in
Table~\ref{tbl-acc-train}.

\begin{longtable}{ccccc}

\caption{\label{tbl-acc-train}Predictive performance measures by dataset
and objective averaged across training-phase parameters
(Note~\ref{nte-train-final-run-train}) and evaluation-phase parameters
(Note~\ref{nte-train-final-run-eval}).}

\tabularnewline

  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endfirsthead
  \toprule
  \textbf{Dataset} & \textbf{Variable} & \textbf{Objective} & \textbf{Mean} & \textbf{Std} \\\midrule
  \endhead
  \bottomrule
  \multicolumn{5}{r}{Continuing table below.}\\
  \bottomrule
  \endfoot
  \endlastfoot
  Circ & Accuracy & Full & 0.99 & 0.0 \\
  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\
  Circ & F1-score & Full & 0.99 & 0.0 \\
  Circ & F1-score & Vanilla & 1.0 & 0.0 \\
  LS & Accuracy & Full & 1.0 & 0.0 \\
  LS & Accuracy & Vanilla & 1.0 & 0.0 \\
  LS & F1-score & Full & 1.0 & 0.0 \\
  LS & F1-score & Vanilla & 1.0 & 0.0 \\
  Moon & Accuracy & Full & 1.0 & 0.01 \\
  Moon & Accuracy & Vanilla & 0.99 & 0.02 \\
  Moon & F1-score & Full & 1.0 & 0.01 \\
  Moon & F1-score & Vanilla & 0.99 & 0.02 \\
  OL & Accuracy & Full & 0.91 & 0.01 \\
  OL & Accuracy & Vanilla & 0.92 & 0.0 \\
  OL & F1-score & Full & 0.91 & 0.01 \\
  OL & F1-score & Vanilla & 0.92 & 0.0 \\\bottomrule

\end{longtable}

\subsubsection{Plausibility}\label{plausibility-2}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-grid-train-plaus-circles} to
Figure~\ref{fig-grid-train-plaus-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-train-plaus-circles}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-train-plaus-lin_sep}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-train-plaus-moons}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png}

}

\caption{\label{fig-grid-train-plaus-over}Average outcomes for the
plausibility measure across hyperparameters. This shows the \% change
from the baseline model for the distance-based implausibility metric
(Equation~\ref{eq-impl-dist}). Boxplots indicate the variation across
evaluation runs and test settings (varying parameters for \emph{ECCo}).
Data: Overlapping.}

\end{figure}%

\subsubsection{Cost}\label{cost-2}

The results with respect to the cost measure are shown in
Figure~\ref{fig-grid-train-cost-circles} to
Figure~\ref{fig-grid-train-cost-over}.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}

}

\caption{\label{fig-grid-train-cost-circles}Average outcomes for the
cost measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}

}

\caption{\label{fig-grid-train-cost-lin_sep}Average outcomes for the
cost measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Linearly
Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}

}

\caption{\label{fig-grid-train-cost-moons}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png}

}

\caption{\label{fig-grid-train-cost-over}Average outcomes for the cost
measure across hyperparameters. This shows the \% change from the
baseline model for the distance-based cost metric
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Boxplots indicate the variation across evaluation runs
and test settings (varying parameters for \emph{ECCo}). Data:
Overlapping.}

\end{figure}%

\FloatBarrier

\section{Tuning Key Parameters}\label{sec-app-tune}

Based on the findings from our initial large grid searches
(Section~\ref{sec-app-grid}), we tune selected hyperparameters for all
datasets: namely, the decision threshold \(\tau\) and the strength of
the energy regularization \(\lambda_{\text{reg}}\). The final
hyperparameter choices for each dataset are presented in
Table~\ref{tbl-final-params} in Section~\ref{sec-app-main}. Detailed
results for each data set are shown in Figure~\ref{fig-tune-plaus-adult}
to Figure~\ref{fig-tune-mat-over}. From Table~\ref{tbl-final-params}, we
notice that the same decision threshold of \(\tau=0.5\) is optimal for
all but on dataset. We attribute this to the fact that a low decision
threshold results in a higher share of mature counterfactuals and hence
more opportunities for the model to learn from examples
(Figure~\ref{fig-tune-mat-adult} to Figure~\ref{fig-tune-mat-over}).
This has played a role in particular for our real-world tabular datasets
and MNIST, which suffered from low levels of maturity for higher
decision thresholds. In cases where maturity is not an issue, as for
\emph{Moons}, higher decision thresholds lead to better outcomes, which
may have to do with the fact that the resulting counterfactuals are more
faithful to the model. Concerning the regularization strength, we find
somewhat high variation across datasets. Most notably, we find that
relatively low levels of regularization are optimal for MNIST. We
hypothesize that this finding may be attributed to the uniform scaling
of all input features (digits).

Finally, to increase the proportion of mature counterfactuals for some
datasets, we have also investigated the effect on the learning rate
\(\eta\) for the counterfactual search and even smaller regularization
strengths for a fixed decision threshold of 0.5
(Figure~\ref{fig-tune_lr-plaus-adult} to
Figure~\ref{fig-tune_lr-plaus-over}). For the given low decision
threshold, we find that the learning rate has no discernable impact on
the proportion of mature counterfactuals
(Figure~\ref{fig-tune_lr-mat-adult} to
Figure~\ref{fig-tune_lr-mat-over}). We do notice, however, that the
results for MNIST are much improved when using a low value
\(\lambda_{\text{reg}}\), the strength for the engery regularization:
plausibility is increased by up to \textasciitilde10\%
(Figure~\ref{fig-tune_lr-plaus-mnist}) and the proportion of mature
counterfactuals reaches 100\%.

One consideration worth exploring is to combine high decision thresholds
with high learning rates, which we have not investigated here.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-warning-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Package Version (Reproducibility)}]

Tuning was run using \texttt{v1.1.3} of \texttt{TaijaData}. The
follow-up version \texttt{v1.1.4} introduced an option to split
real-world tabular datasets into train and test set, ensuring that
pre-processing steps like standardization is fit on the training set
only. If you are rerunning the tuning experiments with a version of
\texttt{TaijaData} that is higher than \texttt{v1.1.3}, than for the
default parameters specified in the configuration files, you may end up
with slightly different results, although we would not expect any
changes in terms of qualitative findings. For exact reproducibility,
please use \texttt{v1.1.3}.

\end{tcolorbox}

\subsection{Key Parameters}\label{sec-app-tune-key}

The hyperparameter grid for tuning key parameters is shown in
Note~\ref{nte-tune-train}. The corresponding evaluation grid used for
these experiments is shown in Note~\ref{nte-tune-eval}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-tune-train}: Training Phase}]

\quartocalloutnte{nte-tune-train} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    Decision Threshold: \texttt{0.5,\ 0.75,\ 0.9}
  \end{itemize}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{reg}}\): \texttt{0.1,\ 0.25,\ 0.5}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-tune-eval}: Evaluation Phase}]

\quartocalloutnte{nte-tune-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsubsection{Plausibility}\label{plausibility-3}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-tune-plaus-adult} to Figure~\ref{fig-tune-plaus-over}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-adult}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Adult.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-cali}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: California Housing.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-circles}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-credit}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Credit.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-gmsc}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: GMSC.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-lin_sep}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-mnist}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: MNIST.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-moons}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/over/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune-plaus-over}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Overlapping.}

\end{figure}%

\subsubsection{Proportion of Mature CE}\label{proportion-of-mature-ce}

The results with respect to the proportion of mature counterfactuals in
each epoch are shown in Figure~\ref{fig-tune-mat-adult} to
Figure~\ref{fig-tune-mat-over}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-adult}Proportion of mature counterfactuals
in each epoch. Data: Adult.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-cali}Proportion of mature counterfactuals
in each epoch. Data: California Housing.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-circles}Proportion of mature
counterfactuals in each epoch. Data: Circles.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-credit}Proportion of mature counterfactuals
in each epoch. Data: Credit.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-gmsc}Proportion of mature counterfactuals
in each epoch. Data: GMSC.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-lin_sep}Proportion of mature
counterfactuals in each epoch. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-mnist}Proportion of mature counterfactuals
in each epoch. Data: MNIST.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-moons}Proportion of mature counterfactuals
in each epoch. Data: Moons.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune/mlp/over/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune-mat-over}Proportion of mature counterfactuals
in each epoch. Data: Overlapping.}

\end{figure}%

\subsection{Learning Rate}\label{sec-app-tune-lr}

The hyperparameter grid for tuning the learning rate is shown in
Note~\ref{nte-tune_lr-train}. The corresponding evaluation grid used for
these experiments is shown in Note~\ref{nte-tune_lr-eval}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-tune_lr-train}: Training Phase}]

\quartocalloutnte{nte-tune_lr-train} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    Learning Rate: \texttt{0.1,\ 0.5,\ 1.0}
  \end{itemize}
\item
  Model: \texttt{mlp}
\item
  Training Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{reg}}\): \texttt{0.01,\ 0.1,\ 0.5}
  \item
    Objective: \texttt{full,\ vanilla}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, colframe=quarto-callout-note-color-frame, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, rightrule=.15mm, breakable, titlerule=0mm, left=2mm, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black, toptitle=1mm, bottomtitle=1mm, title={Note \ref*{nte-tune_lr-eval}: Evaluation Phase}]

\quartocalloutnte{nte-tune_lr-eval} 

\begin{itemize}
\tightlist
\item
  Generator Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda_{\text{egy}}\): \texttt{0.1,\ 0.5,\ 1.0,\ 5.0,\ 10.0}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\subsubsection{Plausibility}\label{plausibility-4}

The results with respect to the plausibility measure are shown in
Figure~\ref{fig-tune_lr-plaus-adult} to
Figure~\ref{fig-tune_lr-plaus-over}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-adult}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Adult.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-credit}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Credit.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-gmsc}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: GMSC.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-lin_sep}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-mnist}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: MNIST.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png}

}

\caption{\label{fig-tune_lr-plaus-over}Average outcomes for the
plausibility measure across key hyperparameters. This shows the \%
change from the baseline model for the distance-based implausibility
metric (Equation~\ref{eq-impl-dist}). Boxplots indicate the variation
across evaluation runs and test settings (varying parameters for
\emph{ECCo}). Data: Overlapping.}

\end{figure}%

\subsubsection{Proportion of Mature CE}\label{proportion-of-mature-ce-1}

The results with respect to the proportion of mature counterfactuals in
each epoch are shown in Figure~\ref{fig-tune_lr-mat-adult} to
Figure~\ref{fig-tune_lr-mat-over}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-adult}Proportion of mature
counterfactuals in each epoch. Data: Adult.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-credit}Proportion of mature
counterfactuals in each epoch. Data: Credit.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-gmsc}Proportion of mature
counterfactuals in each epoch. Data: GMSC.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-lin_sep}Proportion of mature
counterfactuals in each epoch. Data: Linearly Separable.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-mnist}Proportion of mature
counterfactuals in each epoch. Data: MNIST.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{../../paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png}

}

\caption{\label{fig-tune_lr-mat-over}Proportion of mature
counterfactuals in each epoch. Data: Overlapping.}

\end{figure}%

\FloatBarrier

\section{Computation Details}\label{computation-details}

\subsection{Hardware}\label{sec-app-hardware}

We performed our experiments on a high-performance cluster. Details
about the cluster will be disclosed upon publication to avoid revealing
information that might interfere with the double-blind review process.
Since our experiments involve highly parallel tasks and rather small
models by today's standard, we have relied on distributed computing
across multiple central processing units (CPU). Graphical processing
units (GPU) were not required.

\subsubsection{Grid Searches}\label{grid-searches}

Model training for the largest grid searches with 270 unique parameter
combinations was parallelized across 34 CPUs with 2GB memory each. The
time to completion varied by dataset for reasons discussed in
Section~\ref{sec-discussion}: 0h49m (\emph{Moons}), 1h4m (\emph{Linearly
Separable}), 1h49m (\emph{Circles}), 3h52m (\emph{Overlapping}). Model
evaluations for large grid searches were parallelized across 20 CPUs
with 3GB memory each. Evaluations for all data sets took less than one
hour (\textless1h) to complete.

\subsubsection{Tuning}\label{tuning}

For tuning of selected hyperparameters, we distributed the task of
generating counterfactuals during training across 40 CPUs with 2GB
memory each for all tabular datasets. Except for the \emph{Adult}
dataset, all training runs were completed in less that half an hour
(\textless0h30m). The \emph{Adult} dataset took around 0h35m to
complete. Evaluations across 20 CPUs with 3GB memory each generally took
less than 0h30m to complete. For \emph{MNIST}, we relied on 100 CPUs
with 2GB memory each. For the \emph{MLP}, training of all models could
be completed in 1h30m, while the evaluation across 20 CPUs (6GB memory)
took 4h12m. For the \emph{CNN}, training of all models took
\textasciitilde8h, with conventionally trained models taking
\textasciitilde0h15m each and model with CT taking
\textasciitilde0h30m-0h45m each.

\subsection{Software}\label{software}

All computations were performed in the Julia Programming Language
(\citeproc{ref-bezanson2017julia}{Bezanson et al. 2017}). We have
developed a package for counterfactual training that leverages and
extends the functionality provided by several existing packages, most
notably
\href{https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl}{CounterfactualExplanations.jl}
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, and Liem
2023}) and the \href{https://fluxml.ai/Flux.jl/v0.16/}{Flux.jl} library
for deep learning (\citeproc{ref-innes2018fashionable}{Michael Innes et
al. 2018}; \citeproc{ref-innes2018flux}{Mike Innes 2018}). For
data-wrangling and presentation-ready tables we relied on
\href{https://dataframes.juliadata.org/v1.7/}{DataFrames.jl}
(\citeproc{ref-milan2023dataframes}{Bouchet-Valat and Kamiski 2023})
and
\href{https://ronisbr.github.io/PrettyTables.jl/v2.4/}{PrettyTables.jl}
(\citeproc{ref-chagas2024pretty}{Chagas et al. 2024}), respectively. For
plots and visualizations we used both
\href{https://docs.juliaplots.org/v1.40/}{Plots.jl}
(\citeproc{ref-PlotsJL}{Christ et al. 2023}) and
\href{https://docs.makie.org/v0.22/}{Makie.jl}
(\citeproc{ref-danisch2021makie}{Danisch and Krumbiegel 2021}), in
particular \href{https://aog.makie.org/v0.9.3/}{AlgebraOfGraphics.jl}.
To distribute computational tasks across multiple processors, we have
relied on \href{https://juliaparallel.org/MPI.jl/v0.20/}{MPI.jl}
(\citeproc{ref-byrne2021mpi}{Byrne, Wilcox, and Churavy 2021}).

\end{appendices}




\end{document}
