# Conclusions {#sec-conclusions}

State-of-the-art machine learning models are prone to learning complex representations that cannot be interpreted by humans. Existing explainability solutions cannot guarantee that explanations agree with these learned representation. As a step towards addressing this challenge, we introduce counterfactual training, a novel training regime that incentivizes highly-explainable models. Through extensive experiments, we demonstrate that CT satisfies this objective while promoting robustness and preserving the predictive performance of models. Explanations generated from CT-based models are both more plausible (compliant with the underlying data-generating process) and more actionable (compliant with user-specified mutability constraints), and thus meaningful to their recipients. We also show that CT can be used to fine-tune conventionally-trained models and achieve similar gains. Lastly, our work highlights the value of simultaneously improving models and their explanations.