# Discussion {#sec-discussion}

We first address the direct extensions of the counterfactual training approach in @sec-approach. Then, we look at its limitations and challenges in @sec-limitations. 

## Future research {#sec-approach}

***CT is defined only for classification settings.*** Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined by the user, CT requires the output space $\mathcal{Y}$ to be discrete. Thus, it applies to binary and multi-class classification but it is not well-defined for other ML tasks where the change in outcome with respect to a decision threshold $\tau$ cannot be readily quantified. In fact, this is a common restriction in research on CEs and AR that predominantly focuses on classification models. Although other settings have attracted some interest [e.g., regression in @spooner2021counterfactual; @zhao2023counterfactual], there is still no consensus on what constitutes a counterfactual in such settings.

***CT is subject to training instabilities.*** Joint energy-based models are susceptible to instabilities during training [@grathwohl2020your] and even though we depart from the SGLD-based sampling, we still encounter major variability in the outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term in @eq-div, and (2) the underlying counterfactual explainers. For example, @altmeyer2023faithful recognize this to be a challenge for *ECCCo* and so it may have downstream impacts on our proposed method. Still, we find that training instabilities can be successfully mitigated by regularizing energy ($\lambda_{\text{reg}}$), generating a sufficiently large number of counterfactuals during each training epoch and including only mature counterfactuals for contrastive divergence.

***CT is sensitive to hyperparameter selection.*** As discussed in @sec-hyperparameters, our method benefits from tuning certain key hyperparameters. In this work, we have relied exclusively on grid search for this task. Future work on CT could benefit from investigating more sophisticated approaches towards hyperparameter tuning. Notably, CT is iterative which makes a variety of methods applicable, including Bayesian [e.g., @snoek2012practical] or gradient-based [e.g., @franceschi2017forward] optimization.

## Limitations and challenges {#sec-limitations}

***CT increases the training time of models.*** Counterfactual training promotes explainability through CEs and robustness through AEs at the cost of longer training times compared to conventional training regimes. While higher numbers of iterations and counterfactuals per iteration positively impact the quality of found solutions, they also increase the required amount of computations. We find that relatively small grids with 270 settings can take almost four hours for more demanding datasets on a high-performance computing cluster with 34 2GB CPUs (see details in @sec-hardware). However, there are three factors that attenuate the impact of this limitation. First, CT provides counterfactual explanations for the training samples essentially for free, which may be beneficial in many ADM systems. Second, we find that CT can retain its value when used as a "fine-tuning" training regime for conventionally-trained models. Third, in principle, CT yields itself to parallel execution, which we have leveraged for our own experiments.

***Immutable features may have proxies.*** In @prp-mtblty we define an approach to protect immutable features and thus increase the actionability of the generated counterfactuals. However, this approach requires that model owners define the mutability constraints for (all) features considered by the model. Even with sufficient domain knowledge to protect all immutable features---ones that cannot be changed at all and ones that cannot be reasonably expected to change---there may exist proxies that are theoretically mutable (and hence should not be protected) but preserve enough information about the principals to counteract the protections. As one example, consider the Adult dataset used in our experiments where the mutable education status is a proxy for the immutable age, in that the attainment of degrees is correlated with age. Delineating actionability is a major undecided challenge in the AR literature [see, e.g., @venkatasubramanian2020philosophical] impacting the capacity of CT to increase the explainability of the model.

***Interventions on features may have downstream impacts on fairness.***  Related to the point above, we provide a tool that allows practitioners to modify the sensitivity of a model with respect to certain features, which may have implication for the fair and equitable treatment of individuals subject to automated decisions. As protecting a set of features leads the model to assign higher relative importance to unprotected features, model owners could misuse our solution by enforcing explanations based on features that are more difficult to modify by some (group of) individuals. For example, consider again the Adult dataset where features such as workclass or education may be more difficult to change for underpriviledged groups. When applied irresponsibly, counterfactual training could result in an unfairly assigned burden of recourse [e.g., @sharma2020certifai], threatening the equality of opportunity in the system [@bell2024fairness] and potentially reinforcing social segregation [@gao2023impact]. Still, as the referenced publications indicate, such phenomena are not specific to CT; all types of ADM solutions without strong external protections have been recognized to promote harmful power dynamics [@maas2023machine].