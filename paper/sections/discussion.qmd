# Discussion {#sec-discussion}

As our results indicate, counterfactual training produces models that are more explainable. Nonetheless, it brings about three important limitations.  
`\indent`{=latex} *CT increases the training time of models.* CT can be more time-consuming than conventional training regimes. While higher numbers of CEs per iteration positively impact the quality of solutions, they also increase the amount of computations. Relatively small grids with 270 settings can take almost four hours for more demanding datasets on a high-performance computing cluster with 34 2GB CPUs.^[See supplementary appendix for computational details.] Three factors attenuate this effect. First, CT amortizes the cost of CEs for the training samples. Second, we find that it can retain its value when used as a "fine-tuning" technique for conventionally-trained models. Third, it yields itself to parallel execution, which we have leveraged for our own experiments.  
`\indent`{=latex} *Immutable features may have proxies.* We propose an approach to protect immutable features and thus increase the actionability of the generated CEs. However, it requires that model owners define the mutability constraints for (all) features considered by the model. Even if all immutable features are protected, there may exist proxies that are mutable (and hence should not be protected) but preserve enough information about the principals to hinder the protections. Delineating actionability is a major undecided challenge in the AR literature [see, e.g., @venkatasubramanian2020philosophical] impacting the capacity of CT to fulfill its intended goal.  
`\indent`{=latex} *Interventions on features may impact fairness.* We provide a tool that allows practitioners to modify the sensitivity of a model with respect to certain features, which may have implication for the fair and equitable treatment of decision subjects. As protecting a set of features leads the model to assign higher relative importance to unprotected features, model owners could misuse our solution by enforcing explanations based on features that are more difficult to modify by some (group of) individuals. For example, consider the Adult dataset used in our experiments, where *workclass* or *education* may be more difficult to change for underpriviledged groups. When applied irresponsibly, CT could result in an unfairly assigned burden of recourse [@sharma2020certifai], threatening the equality of opportunity in the system [@bell2024fairness]. Nonetheless, these phenomena are not specific to CT.  
  
We also highlight several important directions for future research. Firstly, it is an interesting challenge to extend CT beyond classification settings. Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined, CT requires the output space $\mathcal{Y}$ to be discrete. Thus, it does not apply to ML tasks where the change in outcome cannot be readily quantified. Focus on classification models is a common restriction in research on CEs and AR. Other settings have attracted some interest (e.g., regression in [@spooner2021counterfactual]), but there is little consensus how to robustly extend the notion of CEs.  
`\indent`{=latex} Secondly, our approach is susceptible to training instabilities. This problem has been recognized for JEMs [@grathwohl2020your] and even though we depart from the SGLD-based sampling, we still encounter considerable variability in the outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term in @eq-div, and (2) the underlying counterfactual explainers. We find several promising ways to mitigate this problem: regularizing energy ($\lambda_{\text{reg}}$), generating sufficiently many counterfactuals during each epoch, and including only mature counterfactuals for contrastive divergence.  
`\indent`{=latex} Finally, we believe that it is possible to substantially improve hyperparameter selection procedures. Our method benefits from the tuning of certain key hyperparameters (see @sec-hyperparameters). In this work, we have relied exclusively on grid search for this task. Future work on CT could benefit from investigating more sophisticated approaches. Notably, CT is iterative which makes methods such as Bayesian or gradient-based optimization applicable (see, e.g., [@bischl2023hyperparameter]).