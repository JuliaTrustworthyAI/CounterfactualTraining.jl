% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[runningheads]{llncs}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[activate=false]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{2}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[misc]{ifsym}
\newcommand{\corr}{(\Letter)}
\usepackage{placeins}
\makeatletter

\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Fig.}
\else
  \newcommand\figurename{Fig.}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter

\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Counterfactual Training: Teaching Models Plausible and Actionable Explanations},
  pdfauthor={Patrick Altmeyer; Aleksander Buszydlik; Arie van Deursen; Cynthia C. S. Liem},
  pdfkeywords={Counterfactual Training, Counterfactual
Explanations, Algorithmic Recourse, Explainable AI, Representation
Learning},
  colorlinks=true,
  linkcolor={black},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\hypersetup{nolinks=true}

\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}

\titlerunning{Counterfactual Training}

\author{Author information scrubbed for double-blind reviewing}

\begin{document}
\maketitle

\begin{abstract}
We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be (1) plausible with
respect to the underlying data and (2) actionable with respect to the
user-defined mutability constraints. Much existing research has
therefore focused on developing post-hoc methods to generate
counterfactuals that meet these desiderata. In this work, we instead
hold models directly accountable for the desired end goal:
counterfactual training employs counterfactuals ad-hoc during the
training phase to minimize the divergence between learned
representations and plausible, actionable explanations. We demonstrate
empirically and theoretically that our proposed method facilitates
training models that deliver inherently desirable explanations while
maintaining high predictive performance.

\keywords{Counterfactual Training \and Counterfactual
Explanations \and Algorithmic Recourse \and Explainable
AI \and Representation Learning}

\end{abstract}



\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by \textbf{representation learning}: instead of relying on
features and rules that are carefully hand-crafted by humans, modern
machine learning (ML) models are tasked with learning representations
directly from data, guided by narrow objectives such as predictive
accuracy \cite{goodfellow2016deep}. Modern advances in computing have
made it possible to provide such models with ever-growing degrees of
freedom to achieve that task, which frequently allows them to outperform
traditionally more parsimonious models. Unfortunately, in doing so,
models learn increasingly complex and highly sensitive representations
that humans can no longer easily interpret.

The trend towards complexity for the sake of performance has come under
serious scrutiny in recent years. At the very cusp of the deep learning
(DL) revolution, \cite{szegedy2013intriguing} showed that artificial
neural networks (ANN) are sensitive to adversarial examples (AEs):
perturbed versions of data instances that yield vastly different model
predictions despite being ``imperceptible'' in that they are
semantically indifferent from their factual counterparts. Even though
some partially effective mitigation strategies have been proposed---most
notably \textbf{adversarial training}
\cite{goodfellow2014explaining}---truly robust deep learning remains
unattainable even for models that are considered ``shallow'' by today's
standards \cite{kolter2023keynote}.

Part of the problem is that the high degrees of freedom provide room for
many solutions that are locally optimal with respect to narrow
objectives \cite{wilson2020case}.\footnote{We follow the standard ML
  convention, where ``degrees of freedom'' refer to the number of
  parameters estimated from data.} Indeed, recent work on the so-called
``lottery ticket hypothesis'' suggests that modern neural networks can
be pruned by up to 90\% while preserving their predictive performance
\cite{frankle2018lottery}. Similarly, \cite{zhang2021understanding}
showed that state-of-the-art neural networks are expressive enough to
fit randomly labeled data. Thus, looking at the predictive performance
alone, the solutions may seem to provide compelling explanations for the
data, when in fact they are based on purely associative, semantically
meaningless patterns. This poses two challenges. Firstly, there is no
dependable way to verify if representations correspond to meaningful,
plausible explanations. Secondly, even if we could resolve the first
challenge, it remains undecided how to ensure that models can
\emph{only} learn valuable explanations.

The first challenge has attracted an abundance of research on
\textbf{explainable AI} (XAI), a paradigm that focuses on the
development of tools to derive (post-hoc) explanations from complex
model representations. Such explanations should mitigate a scenario in
which practitioners deploy opaque models and blindly rely on their
predictions. On countless occasions, this has happened in practice and
caused real harms to people who were adversely and unfairly affected by
automated decision-making (ADM) systems involving opaque models (see,
e.g., \cite{oneil2016weapons}). Effective XAI tools can aid us in
monitoring models and providing recourse to individuals to turn negative
outcomes (e.g., ``loan application rejected'') into positive ones (e.g.,
``application accepted''). In line with this, our work builds upon
\textbf{counterfactual explanations} (CE) proposed by
\cite{wachter2017counterfactual} as an effective approach to achieve
this goal. CEs prescribe minimal changes for factual inputs that, if
implemented, would prompt some fitted model to produce a desired output.

To our surprise, the second challenge has not yet attracted major
research interest. Specifically, there has been no concerted effort
towards improving the ``explanatory capacity'' of models, i.e., the
degree to which learned representations correspond to explanations that
are \textbf{interpretable} and deemed \textbf{plausible} by humans (see
Def. \ref{def-explainability}). Instead, the choice has generally been
to improve the ability of XAI tools to identify the subset of
explanations that are both plausible and valid for any given model,
independent of whether the learned representations are in fact
compatible with plausible explanations \cite{altmeyer2024faithful}.
Fortunately, recent findings indicate that improved explanatory capacity
can arise as a consequence of regularization techniques aimed at other
training objectives such as generative capacity, generalization, or
robustness
\cite{altmeyer2024faithful,augustin2020adversarial,schut2021generating}.
As further discussed in Section~\ref{sec-lit}, our work consolidates
these findings within a single objective.

\textbf{Specifically, we introduce Counterfactual Training (CT)}: a
novel training regime explicitly geared towards improving the
explainability of models. In high-level terms, we define this concept as
as the extent to which valid explanations derived for an opaque model
are also deemed plausible with respect to the underlying data and the
global actionability constraints. To the best of our knowledge, our
framework represents the first attempt to address this challenge by
employing counterfactual explanations already in the training phase.

The remainder of this manuscript is structured as follows.
Section~\ref{sec-lit} presents related work, focusing on the link
between AEs and CEs. Then follow our two principal contributions. In
Section~\ref{sec-method}, we introduce our methodological framework and
show theoretically that it can be employed to enforce global
actionability constraints. In Section~\ref{sec-experiments}, through
extensive experiments, we demonstrate that CT substantially improves
explainability and positively contributes to the robustness of trained
models without sacrificing predictive performance. Finally, in
Section~\ref{sec-discussion}, we discuss the challenges and conclude
that CT is a promising approach towards making opaque models more
trustworthy.

\section{Related Literature}\label{sec-lit}

To make the desiderata for our framework more concrete, we follow
\cite{augustin2020adversarial} in tying the concept of explainability to
the quality of CEs that can be generated for a given model. The authors
show that CEs---understood as minimal input perturbations that yield
some desired model prediction---tend to be more meaningful if the
underlying model is more robust to adversarial examples. We can make
intuitive sense of this finding when looking at adversarial training
(AT) through the lens of representation learning with high degrees of
freedom. As argued before, learned representations may be sensitive to
producing implausible explanations and mispredicting for worst-case
counterfactuals (i.e., AEs). Thus, by inducing models to ``unlearn''
susceptiblity to such examples, adversarial training can effectively
remove implausible explanations from the solution space.

\subsection{Adversarial Examples are Counterfactual
Explanations}\label{adversarial-examples-are-counterfactual-explanations}

This interpretation of the link between explainability through
counterfactuals on one side and robustness to adversarial examples on
the other is backed by empirical evidence.
\cite{sauer2021counterfactual} demonstrates that using counterfactual
images during classifier training improves model robustness. Similarly,
\cite{abbasnejad2020counterfactual} argue that counterfactuals represent
potentially useful training data in machine learning, especially in
supervised settings where inputs may be reasonably mapped to multiple
outputs. They, too, demonstrate that augmenting the training data of
image classifiers can improve generalization. Finally,
\cite{teney2020learning} propose an approach using counterfactuals in
training that does not rely on data augmentation: they argue that
counterfactual pairs typically already exist in training datasets.
Specifically, their approach relies on identifying similar input samples
with different annotations and ensuring that the gradient of the
classifier aligns with the vector between such pairs of counterfactual
inputs using the cosine distance as the loss function.

In the natural language processing (NLP) domain, CEs have also been used
to improve models through data augmentation. \cite{wu2021polyjuice2}
proposes \emph{Polyjuice}, a general-purpose counterfactual generator
for language models. The authors empirically demonstrate that the
augmentation of training data with their method improves robustness in a
number of NLP tasks. \cite{balashankar2023improving} similarly uses
\emph{Polyjuice} to augment NLP datasets through diverse counterfactuals
and show that classifier robustness improves by up to 20\%. Finally,
\cite{luu2023counterfactual} introduces Counterfactual Adversarial
Training (CAT) that also aims to improve generalization and robustness
of language models through a three-step procedure: the authors identify
training samples that are subject to high predictive uncertainty,
generate CEs for them, and fine-tune the language model on a dataset
augmented with the CEs.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples. Pointing
to clear similarities in how CEs and AEs are generated,
\cite{freiesleben2022intriguing} makes the case for jointly studying the
opaqueness and robustness problems in representation learning. Formally,
AEs can be seen as the subset of CEs for which misclassification is
achieved \cite{freiesleben2022intriguing}. Similarly,
\cite{pawelczyk2022exploring} shows that CEs and AEs are equivalent
under certain conditions.

Two recent works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of the post-hoc counterfactual explanations. Firstly,
\cite{ross2021learning} proposes a way to train models that guarantee
individual recourse to some positive target class with high probability.
Their approach builds on adversarial training by explicitly inducing
susceptibility to targeted adversarial examples for the positive class.
Additionally, the proposed method allows for imposing a set of
actionability constraints ex-ante. For example, users can specify that
certain features are immutable. Secondly, \cite{guo2023counternet} is
the first to propose an end-to-end training pipeline that includes CEs
as part of the training procedure. In particular, they propose a
specific network architecture that includes a predictor and CE generator
network, where the parameters of the CE generator network are learnable.
Counterfactuals are generated during each training iteration and fed
back to the predictor network. In contrast to \cite{guo2023counternet},
we impose no restrictions on the ANN architecture at all.

\subsection{Aligning Representations with Plausible
Explanations}\label{aligning-representations-with-plausible-explanations}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a work
closely related to this one, \cite{altmeyer2024faithful} shows that
explainability can be improved through model averaging and refined model
objectives. The authors propose a way to generate counterfactuals that
are maximally faithful to the model in that they are consistent with
what the model has learned about the underlying data. Formally, they
rely on tools from energy-based modelling \cite{teh2003energy} to
minimize the divergence between the distribution of counterfactuals and
the conditional posterior over inputs learned by the model. Their
proposed counterfactual explainer, \emph{ECCCo}, yields plausible
explanations if and only if the underlying model has learned
representations that align with them. The authors find that both deep
ensembles \cite{lakshminarayanan2016simple} and joint energy-based
models (JEMs) \cite{grathwohl2020your} tend to do well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are most called for when
models are underspecified by the available data \cite{wilson2020case}.
Averaging across solutions mitigates the aforementioned risk of relying
on a single locally optimal representations that corresponds to
semantically meaningless explanations for the data. Previous work of
\cite{schut2021generating} similarly found that generating plausible
(``interpretable'') CEs is almost trivial for deep ensembles that have
also undergone adversarial training. The case for JEMs is even clearer:
they involve a hybrid objective that induces both high predictive
performance and generative capacity \cite{grathwohl2020your}. This is
closely related to the idea of aligning models with plausible
explanations and has inspired our CT objective.

\section{Counterfactual Training}\label{sec-method}

In this section we propose our novel counterfactual training objective.
In CT, we combine ideas from adversarial training, counterfactual
explanations, and energy-based modelling with the explicit goal of
aligning representations with plausible explanations that comply with
user-defined actionability constraints.

In the context of counterfactual explanations, plausibility has broadly
been defined as the degree to which counterfactuals comply with the
underlying data-generating process
\cite{altmeyer2024faithful,guidotti2022counterfactual,poyiadzi2020face}.
Plausibility is a necessary but insufficient condition for using CEs to
provide algorithmic recourse (AR) to individuals (negatively) affected
by opaque models. To be actionable, AR recommendations must also be
attainable. A plausible CE for a rejected 20-year-old loan applicant,
for example, might reveal that their application would have been
accepted, if only they were 20 years older. Ignoring all other features,
this would comply with the definition of plausibility if 40-year-old
individuals were in fact more credit-worthy on average than young
adults. But of course this CE does not qualify for providing actionable
recourse to the applicant since \emph{age} is not a (directly) mutable
feature. Counterfactual training aims to improve model explainability by
aligning models with counterfactuals that meet both desiderata:
plausibility and actionability. Formally, we define explainability as
follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$ where $\arg\max_y{\mathbf{y}^{+}}=y^+$ and $y^+$ denotes the index of the target class. 

We say that $\mathbf{M}_\theta$ is \textbf{explainable} to the extent that faithfully generated counterfactuals are plausible and actionable. We define these properties as follows:

\begin{enumerate}
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$ where $A$ is some small region around $\mathbf{x}^{\prime}$.
    \item (Actionability) Permutations $\Delta$ are subject to some actionability constraints.
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$ where $A$ is defined as above.
\end{enumerate}
where $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior over inputs. 
\end{definition}

\noindent The characterization of faithfulness and plausibility in Def.
\ref{def-explainability} is the same as in \cite{altmeyer2024faithful},
with adapted notation. Intuitively, plausible counterfactuals are
consistent with the data and faithful counterfactuals are consistent
with what the model has learned about input data. Actionability
constraints in Def. \ref{def-explainability} vary and depend on the
context in which \(\mathbf{M}_\theta\) is deployed. In this work, we
focus on domain and mutability constraints for individual features
\(x_d\) for \(d=1,...,D\). We limit ourselves to classification tasks
for reasons discussed in Section~\ref{sec-discussion}.

\subsection{Our Proposed Objective}\label{our-proposed-objective}

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
explanation generated through gradient descent over \(T\) iterations as
initially proposed by \cite{wachter2017counterfactual}. For our
purposes, we let \(T\) vary and consider the counterfactual search as
converged as soon as the predicted probability for the target class has
reached a pre-determined threshold, \(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.\footnote{For detailed
  background information on gradient-based counterfactual search and
  convergence see supplementary appendix.}

To train models with high explainability as defined in Def.
\ref{def-explainability}, we propose to leverage counterfactuals in the
following objective:

\begin{equation}\phantomsection\label{eq-obj}{
\begin{split}
\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) &+ \lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) \\ &+ \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{split}
}\end{equation} where \(\text{yloss}(\cdot)\) is a classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms are explained in detail below. For now, they can
be summarized as inducing explainability directly and indirectly by
penalizing: (1) the contrastive divergence, \(\text{div}(\cdot)\),
between mature counterfactuals \(\mathbf{x}_T^\prime\) and observed
samples \(\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}\) in the
target class \(y^+\), and, (2) the adversarial loss,
\(\text{advloss}(.)\), with respect to nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\). Finally, \(\text{ridge}(\cdot)\)
denotes a Ridge penalty (\(\ell_2\)-norm) that regularizes the magnitude
of the energy terms involved in \(\text{div}(\cdot)\)
\cite{du2019implicit}. The trade-off between the components can governed
through penalties \(\lambda_{\text{div}}\), \(\lambda_{\text{adv}}\) and
\(\lambda_{\text{reg}}\).

\subsection{Directly Inducing Explainability with Contrastive
Divergence}\label{directly-inducing-explainability-with-contrastive-divergence}

As observed by \cite{grathwohl2020your}, any classifier can be
re-interpreted as a joint energy-based model (JEM) that learns to
discriminate output classes conditional on the observed (training)
samples from \(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). The authors show that JEMs can be trained to
perform well at both tasks by directly maximizing the joint
log-likelihood factorized as
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\).
The first term can be optimized using conventional cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\),
\cite{grathwohl2020your} minimizes the contrastive divergence between
the observed samples from \(p(\mathbf{x})\) and generated samples from
\(p_\theta(\mathbf{x})\).

A key empirical finding in \cite{altmeyer2024faithful} was that JEMs
tend to do well with respect to the plausibility objective in Def.
\ref{def-explainability}. This follows directly if we consider samples
drawn from \(p_\theta(\mathbf{x})\) as counterfactuals because the JEM
objective effectively minimizes the divergence between the conditional
posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\). To generate samples,
\cite{grathwohl2020your} relies on Stochastic Gradient Langevin Dynamics
(SGLD) using an uninformative prior for initialization but we depart
from their methodology. Instead of SGLD, we propose to use
counterfactual explainers to generate counterfactuals of observed
training samples. Specifically, we have:

\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation} where \(\mathcal{E}_\theta(\cdot)\) denotes the energy
function defined as
\(\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]\)
where \(y^+\) denotes the index of the randomly drawn target class,
\(y^+ \sim p(y)\). Conditional on the target class \(y^+\),
\(\mathbf{x}_T^\prime\) denotes a mature counterfactual for a randomly
sampled factual from a non-target class generated with a gradient-based
CE generator for up to \(T\) iterations. Mature counterfactuals are ones
that have either reached convergence wrt. the decision threshold
\(\tau\) or exhausted \(T\).

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while increasing the
energy of counterfactuals (negative samples) \cite{du2019implicit}. As
the counterfactuals get more plausible (Def. \ref{def-explainability})
during training, these opposing effects gradually balance each other out
\cite{lippe2024uvadlc}.

The departure from SGLD of \cite{grathwohl2020your} allows us to tap
into the vast repertoire of explainers that have been proposed in the
literature to meet different desiderata. For example, many methods
facilitate the imposition of domain and mutability constraints. In
principle, any existing approach for generating counterfactual
explanations is viable, so long as it does not violate the faithfulness
condition. Like JEMs \cite{murphy2022probabilistic}, CT can be
considered a form of contrastive representation learning.

\subsection{Indirectly Inducing Explainability with Adversarial
Robustness}\label{indirectly-inducing-explainability-with-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
\cite{balashankar2023improving,luu2023counterfactual} or adversarial
examples \cite{freiesleben2022intriguing,pawelczyk2022exploring}. This
leaves some flexibility wrt. the choice for \(\text{advloss}(\cdot)\) in
Equation~\ref{eq-obj}. An intuitive functional form, but likely not the
only sensible choice, is inspired by adversarial training:

\begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation} Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as AEs as long as the magnitude of the
perturbation to any single feature is at most \(\varepsilon\). This is
closely aligned with \cite{szegedy2013intriguing} that defines an
adversarial attack as an ``imperceptible non-random perturbation''.
Thus, we choose to work with a different distinction between CE and AE
than \cite{freiesleben2022intriguing} that considers misclassification
as the key distinguishing feature of AE. One of the key observations of
this work is that we can leverage CEs during training and get
adversarial examples essentially for free, which can be used to reap the
aforementioned benefits of adversarial training.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any counterfactual explainer that relies on gradient
descent in the feature space for optimization
\cite{altmeyer2023explaining}. In this context, domain constraints can
be imposed by simply projecting counterfactuals back to the specified
domain, if the previous gradient step resulted in updated feature values
that were out-of-domain. Mutability constraints can similarly be
enforced by setting partial derivatives to zero to ensure that features
are only perturbed in the allowed direction, if at all.

Since such actionability constraints are binding at test time, we should
also impose them when generating \(\mathbf{x}^\prime\) during each
training iteration to inform model representations. Through their effect
on \(\mathbf{x}^\prime\), both types of constraints influence model
outcomes via Equation~\ref{eq-div}. Here it is crucial that we avoid
penalizing implausibility that arises due to mutability constraints. For
any mutability-constrained feature \(d\) this can be achieved by
enforcing \(\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0\) whenever
perturbing \(\mathbf{x}^\prime[d]\) in the direction of
\(\mathbf{x}^+[d]\) would violate mutability constraints. Specifically,
we set \(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  We have \(\mathbf{x}^+[d]>\mathbf{x}^\prime[d]\), but feature \(d\)
  can only be decreased in practice.
\item
  We have \(\mathbf{x}^+[d]<\mathbf{x}^\prime[d]\), but feature \(d\)
  can only be increased in practice.
\end{enumerate}

\noindent From a Bayesian perspective, setting
\(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) can be understood as
assuming a point mass prior for \(p(\mathbf{x}^+)\) with respect to
feature \(d\). Intuitively, we think of this simply in terms ignoring
implausibility costs with respect to immutable features, which
effectively forces the model to instead seek plausibility with respect
to the remaining features. This in turn results in lower overall
sensitivity to immutable features, which we demonstrate empirically for
different classifiers in Section~\ref{sec-experiments}. Under certain
conditions, this result holds theoretically:\footnote{For the proof, see
  the supplementary appendix.}

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. If we assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}

\noindent It is worth highlighting that
\mbox{Proposition \ref{prp-mtblty}} assumes independence of features.
This raises a valid concern about the effect of protecting immutable
features in the presence of proxies that remain unprotected. We address
this in Section~\ref{sec-discussion}.

\subsection{Example (Prediction of Consumer Credit
Default)}\label{example-prediction-of-consumer-credit-default}

Suppose we are interested in predicting the likelihood that loan
applicants default on their credit. We have access to historical data on
previous loan takers comprised of a binary outcome variable
(\(y\in\{1=\text{default},2=\text{no default}\}\)) with two input
features: (1) the subjects' \emph{age}, which we define as immutable,
and (2) the subjects' existing level of \emph{debt}, which we define as
mutable.

We have simulated this scenario using synthetic data with independent
\emph{age} and \emph{debt}, and Gaussian class-conditional densities in
Figure~\ref{fig-poc}. The four panels show the outcomes for different
training procedures using the same model architectures (a linear
classifier). In panels (a) and (c) we have trained the models
conventionally, while in panels (b) and (d) we used CT.

In all cases, all counterfactuals (stars) are valid---they have cross
the decision boundary (green)---but their quality differs. In panel (a),
they are not plausible: they do not comply with the distribution of the
factuals in \(y^+\) to the point where they form a clearly
distinguishable cluster. In panel (b), they are highly plausible,
meeting the first objective of Def. \ref{def-explainability}. In panel
(c), the CEs involve substantial reductions in \emph{debt} for younger
applicants. By comparison, counterfactual paths are shorter on average
in panel (d) where we have protected the immutable \emph{age} as
described in Section~\ref{sec-constraints}. Due to the classifier's
lower sensitivity to \emph{age}, recommendations with respect to
\emph{debt} are much more homogenous and do not unfairly punish younger
individuals. These counterfactuals are also plausible with respect to
the mutable feature. Thus, we consider the model in panel (d) as the
most explainable according to Def. \ref{def-explainability}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Illustration of how CT improves model
explainability: (a) conventional training, all mutable; (b) CT, all
mutable; (c) conventional, \emph{age} immutable; (d) CT, \emph{age}
immutable. The linear decision boundary is shown in green along with
training data colored according to their ground-truth label: \(y^-=1\)
in blue and \(y^+=2\) in orange. Stars indicate counterfactuals in the
target class.}

\end{figure}%

\section{Experiments}\label{sec-experiments}

In our experiments we seek to answer the following three research
questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To what extent does the counterfactual training objective as it is
  defined in Equation~\ref{eq-obj} induce models to learn plausible
  explanations?
\item
  To what extent does the CT objective produce more favorable
  algorithmic recourse outcomes in the presence of actionability
  constraints?
\item
  What are the effects of hyperparameter selection wrt. the CT
  objective?
\end{enumerate}

\subsection{Experimental Setup}\label{experimental-setup}

Our key outcome of interest is improvement in explainability (Def.
\ref{def-explainability}). To this end, we focus primarily on the
plausibility and cost of faithfully generated counterfactuals at test
time. To measure the cost, we follow the standard convention of using
distances (\(\ell_1\)-norm) between factuals and counterfactuals as a
proxy. For plausibility, we assess how similar CEs are to the observed
samples in the target domain, \(\mathbf{X}^+\subset\mathcal{X}^+\). We
rely on the distance-based metric used in \cite{altmeyer2024faithful},

\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{IP}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation} and introduce a novel divergence metric,

\begin{equation}\phantomsection\label{eq-impl-div}{
\text{IP}^*(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation} where \(\mathbf{X}^\prime\) denotes a collection of
counterfactuals and \(\text{MMD}(\cdot)\) is an unbiased estimate of the
squared population maximum mean discrepancy \cite{gretton2012kernel}.
The metric in Equation~\ref{eq-impl-div} is equal to zero iff the two
distributions are the same, \(\mathbf{X}^\prime=\mathbf{X}^+\).

In addition to cost and plausibility, we compute other standard metrics
to evaluate counterfactuals including validity and redundancy. Finally,
we also assess the predictive performance of models using standard
metrics, including robust accuracy estimated on adversarially perturbed
data using FGSM \cite{goodfellow2014explaining}.

We run the experiments with three gradient-based generators:
\emph{Generic} of \cite{wachter2017counterfactual} as a simple baseline
approach, \emph{REVISE} \cite{joshi2019realistic} that aims to generate
plausible counterfactuals using a surrogate Variational Autoencoder
(VAE), and \emph{ECCo}---the generator of \cite{altmeyer2024faithful}
but without the conformal prediction component---as a method that
directly targets both faithfulness and plausibility of the
counterfactuals.

We make use of nine classification datasets common in the CE/AR
literature. Four of them are synthetic with two classes and different
characteristics: linearly separable clusters (\emph{LS}), overlapping
clusters (\emph{OL}), concentric circles (\emph{Circ}), and interlocking
moons (\emph{Moon}). These datasets are generated using the library of
\cite{altmeyer2023explaining} and we present them in the supplementary
appendix. Next, we have four real-world binary tabular datasets from the
domain of economics: \emph{Adult} (a.k.a. Census data) of
\cite{becker1996adult2}, California housing (\emph{CH}) of
\cite{pace1997sparse}, Default of Credit Card Clients (\emph{Cred}) of
\cite{yeh2016default}, and Give Me Some Credit (\emph{GMSC}) of
\cite{kaggle2011give2}. Finally, for the convenience of illustration, we
use of the 10-class \emph{MNIST} vision dataset \cite{lecun1998mnist}.

To assess CT, we investigate the improvements in performance metrics
when using it on top of a weak baseline (BL): a multilayer perceptron
(\emph{MLP}). This is the best way to get a clear picture of the
effectiveness of CT, and it is consistent with how assessment is done in
the related literature
\cite{goodfellow2014explaining,ross2021learning,teney2020learning}.

\subsection{Experimental Results}\label{experimental-results}

\subsubsection{Plausibility}\label{sec-plaus}

Table~\ref{tbl-main} presents our main empirical findings. For all
datasets except \emph{OL} and across all test settings, the average
distance of CEs from observed samples in the target class is reduced,
indicating improved plausibility. The magnitude of improvements varies.
For the simple synthetic datasets, distance reductions range from around
20-40\% (\emph{LS}, \emph{Moon}) to almost 60\% (\emph{Circ}). For the
real-world tabular datasets, improvements tend to be smaller but still
substantial, with around 10-15\% for \emph{CH}, 11-28\% for \emph{GMSC},
7-8\% for \emph{Cred}, and around 3\% for \emph{Adult}. For the vision
dataset (\emph{MNIST}), distances are reduced by up to 9\%. The results
for our proposed divergence metric are qualitatively similar, but
generally even more pronounced: for the \emph{Circ} dataset,
implausibility is reduced by almost 94\% to virtually zero as we
verified by the absolute outcome. Improvements for other datasets range
from 28\% (\emph{Moon}) up to 78\% (\emph{GMSC}). For \emph{OL} the
reduction is negative, consistent with the distance-based metric.
\emph{MNIST} is the only dataset for which the distance and divergence
metrics disagree. Upon visual inspection of the image counterfactuals we
find that CT clearly improves plausibility (see supplementary appendix).

\subsubsection{Predictive Performance}\label{sec-pred}

Test accuracy for CT is virtually identical to the baseline for
\emph{Adult}, \emph{Circ}, \emph{LS}, \emph{Moon}, and \emph{OL}, and
even slightly improved for \emph{Cred}. Exceptions to this general
pattern are \emph{MNIST}, \emph{CH}, and \emph{GMSC}, for which we
observe a reduction in test accuracy of 2, 5, and 15 percentage points
respectively. When looking at robust test accuracies (Acc.\(^*\)) for
these datasets in particular, we find that CT strongly outperforms the
baseline. In fact, we find that CT improves adversarial robustness on
all datasets.

\begin{table}

\caption{\label{tbl-main}Key performance metrics across all datasets
(column 1). \textbf{Plausibility}: Columns 2-6 show the percentage
reduction in implausibility (\(\text{IP}\)) for varying degrees of the
energy penalty \(\lambda_{\text{egy}}\) used for \emph{ECCo} at test
time; column 7 shows the reduction in \(\text{IP}^*\) (\(\text{MMD}\)),
aggregated across all test specifications. \textbf{Accuracy} (columns
8-11): test accuracies and robust accuracies (\(\text{Acc}^*\)) for CT
and the baseline (BL). \textbf{Actionability} (column 12): average
reduction in costs when imposing mutability constraints reported for the
four datasets for which we could identify meaningful features to
protect.}

\centering{

\input{tables/main.tex}

}

\end{table}%

\subsubsection{Actionability}\label{sec-act}

In Section~\ref{sec-method}, we show that our proposed way for encoding
mutability constraints leads to lower classifier sensitivity wrt.
immutable features for linear models, tilting the decision boundary in
favour of mutable features instead. For binding constraints at test
time, this leads to shorter counterfactual paths and hence smaller
average costs (\(\ell_1\)-norm) to individuals. To extend this to the
non-linear case, we test the effect of imposing mutability constraints
empirically for our synthetic data using the same evaluation scheme as
above. The final row in Table~\ref{tbl-main} reports the average
reduction in costs for CT compared to the ``vanilla'' baseline, when
imposing that either the first or the second feature is immutable. In
all cases, costs are reduced substantially, indicating that classifiers
trained with CT are indeed more sensitive to mutable features.

\subsubsection{Impact of hyperparameter
settings.}\label{sec-hyperparameters}

We test the impact of three types of hyperparameters; our complete
results are in the supplementary appendix.\\
\indent We note that CT is highly sensitive to the choice of a CE
generator and its hyperparameters but (a) there are manageable patterns
and (b) we can typically identify settings that improve either
plausibility or cost, and commonly both of them at the same time. For
example, \emph{REVISE} tends to perform the worst, most likely because
it uses a surrogate VAE to generate counterfactuals which impedes
faithfulness \cite{altmeyer2024faithful}. Increasing \(T\), the maximum
number of steps, generally yields better outcomes because more CEs can
mature in each training epoch. The impact of \(\tau\), the required
decision threshold is more difficult to predict. On ``harder'' datasets
it may be difficult to satisfy high \(\tau\) for any given sample (i.e.,
also factuals) and so increasing this threshold does not seem to
correlate with better outcomes. In fact, the choice of \(\tau=0.5\)
generally leads to optimal results because it is associated with high
proportions of mature counterfactuals.\\
\indent The strength of the energy regularization,
\(\lambda_{\text{reg}}\) is highly impactful and leads to poor
performance in terms of decreased plausibility and increased costs if
insufficiently high. The sensitivity with respect to
\(\lambda_{\text{div}}\) and \(\lambda_{\text{adv}}\) is much less
evident. While high values of \(\lambda_{\text{reg}}\) may increase the
variability in outcomes when combined with high values of
\(\lambda_{\text{div}}\) or \(\lambda_{\text{adv}}\), this effect is not
very pronounced.\\
\indent The effectiveness and stability of CT is positively associated
with the number of counterfactuals generated during each training epoch.
We also confirm that a higher number of training epochs is beneficial.
Interestingly, we observed desired improvements when CT was combined
with conventional training and applied only for the final 50\% of epochs
of the complete training process. Put differently, CT can improve the
explainability of models in a fine-tuning manner.

\section{Conclusions}\label{sec-discussion}

As our results indicate, counterfactual training produces models that
are more explainable. Nonetheless, it brings about three important
limitations.\\
\indent \emph{CT increases the training time of models.} CT can be more
time-consuming than conventional training regimes. While higher numbers
of CEs per iteration positively impact the quality of solutions, they
also increase the amount of computations. Relatively small grids with
270 settings can take almost four hours for more demanding datasets on a
high-performance computing cluster with 34 2GB CPUs.\footnote{See
  supplementary appendix for computational details.} Three factors
attenuate this effect: (1) CT amortizes the cost of CEs for the training
samples; (2) it can retain its value when used as a ``fine-tuning''
technique for conventionally-trained models; and (3) it yields itself to
parallel execution, which we have leveraged for our own experiments.\\
\indent \emph{Immutable features may have proxies.} We propose an
approach to protect immutable features and thus increase the
actionability of the generated CEs. However, it requires that model
owners define the mutability constraints for (all) features considered
by the model. Even if all immutable features are protected, there may
exist proxies that are mutable (and hence should not be protected) but
preserve enough information about the principals to hinder the
protections. Delineating actionability is a major open challenge in the
AR literature (see, e.g., \cite{venkatasubramanian2020philosophical})
impacting the capacity of CT to fulfill its intended goal.\\
\indent \emph{Interventions on features may impact fairness.} We provide
a tool that allows practitioners to modify the sensitivity of a model
with respect to certain features, which may have implication for the
fair and equitable treatment of decision subjects. Model owners could
misuse this solution by enforcing explanations based on features that
are more difficult to modify by some (group of) individuals. For
example, consider the \emph{Adult} dataset used in our experiments,
where \emph{workclass} or \emph{education} may be more difficult to
change for underpriviledged groups. When applied irresponsibly, CT could
result in an unfairly assigned burden of recourse
\cite{sharma2020certifai}, threatening the equality of opportunity in
the system \cite{bell2024fairness}. Nonetheless, these phenomena are not
specific to CT.\\
\hfill \break \indent We also highlight several important directions for
future research. Firstly, it is an interesting challenge to extend CT
beyond classification settings. Our formulation relies on the
distinction between non-target class(es) \(y^{-}\) and target class(es)
\(y^{+}\) to generate counterfactuals through Equation~\ref{eq-obj}.
While \(y^{-}\) and \(y^{+}\) can be arbitrarily defined, CT requires
the output space \(\mathcal{Y}\) to be discrete. Thus, it does not apply
to ML tasks where the change in outcome cannot be readily quantified.
Focus on classification models is a common restriction in research on
CEs and AR. Other settings have attracted some interest (e.g.,
regression in \cite{spooner2021counterfactual}), but there is little
consensus how to robustly extend the notion of CEs.\\
\indent Secondly, our approach is susceptible to training instabilities.
This problem has been recognized for JEMs \cite{grathwohl2020your} and
even though we depart from the SGLD-based sampling, we still encounter
considerable variability in the outcomes. CT is exposed to two potential
sources of instabilities: (1) the energy-based contrastive divergence
term in Equation~\ref{eq-div}, and (2) the underlying counterfactual
explainers. We find several promising ways to mitigate this problem:
regularizing energy (\(\lambda_{\text{reg}}\)), generating sufficiently
many counterfactuals during each epoch, and including only mature
counterfactuals for contrastive divergence.\\
\indent Finally, we believe that it is possible to substantially improve
hyperparameter selection procedures. Our method benefits from the tuning
of certain key hyperparameters (see Section~\ref{sec-hyperparameters}).
In this work, we have relied exclusively on grid search for this task.
Future work on CT could benefit from investigating more sophisticated
approaches. Notably, CT is iterative which makes methods such as
Bayesian or gradient-based optimization applicable (see, e.g.,
\cite{bischl2023hyperparameter}). \hfill \break

To conclude, state-of-the-art machine learning models are prone to
learning complex representations that cannot be interpreted by humans
and existing post-hoc explainability approaches cannot guarantee that
the explanations agree with the model's learned representation of data.
As a step towards addressing this challenge, we introduced
counterfactual training, a novel training regime that incentivizes
highly-explainable models. Our approach leads to explanations that are
both plausible---compliant with the underlying data-generating
process---and actionable---compliant with user-specified mutability
constraints---and thus meaningful to their recipients. Through extensive
experiments we demonstrate that CT satisfies its objective while
promoting robustness and preserving the predictive performance of the
models. It can also be used to fine-tune conventionally-trained models
and achieve similar gains. Finally, this work showcases that it is
practical to improve models \emph{and} their explanations at the same
time.


\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}
