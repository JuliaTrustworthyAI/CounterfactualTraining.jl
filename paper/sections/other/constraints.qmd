---
execute: 
  eval: false
---

```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)
```

```{julia}
using CTExperiments
using CTExperiments.CounterfactualExplanations
using CTExperiments.CounterfactualTraining
using CTExperiments.Flux
using CTExperiments.Plots
using CTExperiments.TaijaPlotting
```

## Details on Encoding Mutability Constraints {#sec-app-constraints}

In @sec-constraints we explained that to avoid penalizing implausibility that arises due to mutability constraints for any feature $d$, we impose a point mass prior on $p(\mathbf{x})$ for that features. This is best explained through a simple example involving a binary linear classifier.

:::{exm-constraints}

Let $\mathbf{M}_\theta(\mathbf{x})=\Theta\mathbf{x}$, with $y \in \{1,2\}$. Then for $y^+ \sim p(y)=2$ we have:

$$
\nabla_\theta \left(\text{div}(\mathbf{x},\mathbf{x^\prime},\mathbf{y};\theta)\right) = \nabla_\theta \left(-\mathbf{M}_\theta(\mathbf{x})[y^+]\right) - \nabla_\theta \left(-\mathbf{M}_\theta(\mathbf{x}^\prime)[y^+]\right) = \begin{pmatrix} 0 & 0 \\ \mathbf{x}^\prime[1] - \mathbf{x}[1] & \mathbf{x}^\prime[2] - \mathbf{x}[2] \end{pmatrix}
$$ {#eq-grad}

Contrastive divergence pushes down (up) on the coefficient $\Theta[y^+,d]$ by the amount that feature $d$ that is lower (higher) for counterfactuals than samples. This has the intended effect of aligning counterfactual explanations with the data, as discussed above. 

Suppose, for example, that feature $d=1$ represents *age* and our model predicts the likelihood of credit default: $y \in \{1:=\text{'no default'},2:=\text{'default'}\}$. Then if $\mathbf{x}^\prime[1] < \mathbf{x}[1]$, contrastive divergence will push down on the corresponding coefficient, $\Theta[\text{'default'},\text{'age'}]$. Intuitively, this has the effect of decreasing the effect of *age* that the model attributes to the log odds of default, i.e. the model learns to higher default risk younger individuals. But if *age* is immutable, then this corresponds to penalizing younger individuals for implausibility that arises due to a feature that they can change about themselves. This is precisely what we want to avoid, and therefore we impose no penalty at all. 

:::

```{julia}
using CTExperiments: 
    get_ce_data, 
    train_val_split, 
    build_model, 
    LinearModel, 
    get_input_encoder

# Data:
constraints = ["none", "both"]        # x1 cannot be mutated
# constraints = ["both", "none"]        # x2 cannot be mutated
data = LinearlySeparable(
    n_train=500,
    batchsize=50,
    mutability=constraints
)
ce_data = get_ce_data(data)
val_size = data.n_validation / (data.n_validation + data.n_train)
train_set, val_set, _ = train_val_split(data, ce_data, val_size)

# Model:
nin = size(first(train_set)[1], 1)
nout = size(first(train_set)[2], 1)
model = build_model(LinearModel(), nin, nout)
```

## Gradient

```{julia}
X1 = ce_data.X[:,ce_data.output_encoder.labels.==1]
X2 = ce_data.X[:,ce_data.output_encoder.labels.==2]
factual = X1[:,1]
target = 2
target_enc = Flux.onehotbatch([target], [1,2])
neighbor = X2[:,1]
```

## Full Training Loop

Next, we defined the training objective and the counterfactual generator. Finally, we initialize the optimization state.

```{julia}
using CounterfactualExplanations.Convergence
obj = EnergyDifferentialObjective(lambda=[1.0,1.0,0.0])
generator = GenericGenerator()
opt_state = Flux.setup(Descent(), model)
conv = MaxIterConvergence()
```

```{julia}
using CTExperiments: get_log_reg_params, get_decision_boundary

function plot_db(model, x)
    coeff = get_log_reg_params(model)
    db = get_decision_boundary(coeff)
    plt = Plots.scatter(ce_data)
    yhat = [argmax(y) for y in eachcol(model(x))]
    Plots.scatter!(x[1,:], x[2,:], label="Counterfactuals", ms=15, shape=:star, color=yhat)
    Plots.abline!(plt, db.slope, db.intercept; lw=5, label="Dec. Boundary")
    display(plt)
end
```

```{julia}
using Random
Random.seed!(42)

model, logs = counterfactual_training(
    obj,
    model,
    generator,
    train_set,
    opt_state;
    val_set = val_set,
    nepochs = 50,
    mutability = Symbol.(constraints),
    callback = plot_db,
    nce=50,
    convergence=conv, 
)
```

Finally, we visualize the results:

```{julia}
coeff = get_log_reg_params(model)
display(coeff)
db = get_decision_boundary(coeff)
Plots.scatter(ce_data)
Plots.abline!(db.slope, db.intercept; lw=5, label="Dec. Boundary")
```