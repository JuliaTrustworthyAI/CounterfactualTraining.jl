# Introduction

Todayâ€™s prominence of artificial intelligence (AI) has largely been driven by the success of representation learning with large degrees of freedom: instead of relying on features and rules hand-crafted by humans, modern machine learning (ML) models are tasked with learning highly complex representations directly from the data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. These models tend to be so complex that humans cannot easily interpret their decision-making.

Counterfactual explanations (CE) have become an integral part of the broader explainable AI (XAI) toolkit [@molnar2022interpretable] that can be employed to make sense of this complexity. Originally proposed by @wachter2017counterfactual, CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce an alternative, more desirable output (@fig-poc). This is useful and necessary to not only understand how opaque models make predictions, but also to provide recourse to individuals subjected to them: a retail bank, for example, could use CE to provide meaningful feedback to unsuccessful loan applicants that were rejected based on an opaque AI decision-making system (ADM).

For feedback to be meaningful, counterfactual explanations need to fulfill certain desiderata [@verma2020counterfactual; @karimi2020survey]: they should be faithful to the model [@altmeyer2024faithful], plausible [@joshi2019realistic] and actionable [@ustun2019actionable]. Plausibility is typically understood as counterfactuals being *in-domain*: unsuccessful loan applicants that implement recourse should end up with a credit profile that is genuinely similar to that of individuals who have successfully repaid their loans in the past. Actionable explanations comply with practical constraints: a young, unsuccessful loan applicant cannot increase their age in an instance.

Existing state-of-the-art (SOTA) approaches in the field have largely focused on designing model-agnostic CE methods that identify subsets of counterfactuals, which comply with specific desiderata. This is problematic, because a narrow focus on any specific desideratum can adversely affect others: it is possible, for example, to generate plausible counterfactuals for models that are also highly vulnerable to implausible, possibly adversarial counterfactuals [@altmeyer2024faithful]. In this work, we therefore follow the paradigm that models---as opposed to explanation methods---should be held accountable for plausible and actionable explanations. While previous work has shown that at least plausibility can be indirectly achieved through existing techniques aimed at models' generative capacity, generalization and robustness [@altmeyer2024faithful; @augustin2020adversarial; @schut2021generating], we directly incorporate both plausibility and actionability in the training objective of models to improve their overall explanatory capacity.

Specifically, **we propose Counterfactual Training (CT)**: a novel training regime that leverages counterfactual explanations on-the-fly to ensure that differentiable models learn plausible and actionable explanations for the underlying data, while at the same time also being more robust to adversarial examples (AE). @fig-poc illustrates the outcome of CT compared to conventionally trained models: in panel (a), faithful and valid counterfactuals end up near the decision boundary forming a clearly distinguishable cluster in the target class (orange); in panel (b), CT is applied to the same underlying linear classifier architecture resulting in much more plausible counterfactuals; in panel (c), the classifier is again trained conventionally and we have introduced a mutability constraint on the *age* feature at test time---counterfactuals are valid but the classifier is roughly equally sensitive to both features; by contrast, the decision boundary in panel (c) has titled, making the model trained with CT relatively less sensitive to the immutable *age* feature. To achieve these outcomes, CT draws inspiration from the literature on contrastive and robust learning: we contrast faithful CE with ground-truth data while protecting immutable features, and capitalize on methodological links between CE and AE by penalizing the model's adversarial loss on interim (*nascent*) counterfactuals. To the best of our knowledge, CT represents the first venture in this direction with promising results, both empirical and theoretical.

The remainder of this manuscript is structured as follows. @sec-lit presents related work, focusing on the links to contrastive and robust learning. Then follow our two principal contributions. In @sec-method, we introduce our methodological framework and show theoretically that it can be employed to enforce global actionability constraints. In our experiments (@sec-experiments), we find that thanks to counterfactual training, (1) implausibility of CE decreases by 16% on average; (2) the cost of reaching valid counterfactuals with protected features decreases by 15%; and (3) robust accuracy increases for all datasets by up to 42 percentage points. Finally, in @sec-discussion, we discuss open challenges and conclude.

![Counterfactual explanations (stars) for linear classifiers trained under different regimes on synthetic data: (a) conventional training, all mutable; (b) CT, all mutable; (c) conventional, *age* immutable; (d) CT, *age* immutable. The linear decision boundary is shown in green along with training data colored according to ground-truth labels: $y^-=1$ (blue) and $y^+=2$ (orange). Stars indicate counterfactuals in the target class. Synthetic features are annotated as *debt* and *age* for illustrative purposes.](/paper/figures/poc.svg){#fig-poc fig-env="figure*"}
