# Introduction

Today's prominence of artificial intelligence (AI) has largely been driven by advances in **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern machine learning (ML) models are tasked with learning the representations directly from data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such models with ever growing degrees of freedom to achieve that task, which has often led them to outperform traditionally more parsimonious models. Unfortunately, in doing so, the models learn increasingly complex and highly sensitive representations that we can no longer easily interpret.

The trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are sensitive to adversarial examples: counterfactuals of model inputs that yield vastly different model predictions despite being "imperceptible" in that they are semantically indifferent from their factual counterparts. Although some partially effective mitigation strategies have been proposed, for example **adversarial training** [@goodfellow2014explaining], truly robust deep learning (DL) remains unattainable even for models that are considered shallow by today's standards [@kolter2023keynote]. 

Part of the problem is that the high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case].^[We follow the standard ML convention, where "degrees of freedom" refer to the number of parameters estimated from data.] Indeed, recent work on the so called "lottery tickets" suggests that modern neural networks can be pruned by up to 90% while preserving their predictive performance [@frankle2018lottery] and generalizability [@morcos2019success]. Similarly, @zhang2021understanding showed that state-of-the-art neural networks are so expressive that they can fit randomly labeled data. Thus, looking at the predictive performance, the solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two related challenges. Firstly, there is no dependable way to verify if such complex representations correspond to meaningful and plausible explanations. Secondly, even if we could resolve the first challenge, it remains undecided how to ensure that models can *only* learn valuable explanations. 

The first challenge has attracted an abundance of research on **explainable AI** (XAI) which aims to develop tools to derive explanations from complex model representations. This can mitigate a scenario in which we deploy opaque models and blindly rely on their predictions. On countless occasions, this scenario has occurred in practice and caused real harm to people who were affected adversely and often unfairly by automated decision-making (ADM) systems involving opaque models [@oneil2016weapons; @mcgregor2021preventing]. Effective XAI tools can aid us in monitoring models and providing recourse to individuals to turn adverse outcomes (e.g., "loan application rejected") into positive ones (e.g., "application accepted"). @wachter2017counterfactual propose **counterfactual explanations** (CE) as an effective approach to achieve this goal: CEs explain how factual inputs need to change in order for some fitted model to produce some desired output, typically involving minimal perturbations.

To our surprise, the second challenge has not yet attracted any major consolidated research effort. Specifically, there has been no concerted effort towards improving improving models' explanatory capacity, which we will henceforth simply call "explainability", defined as the degree to which learned representations correspond to explanations that are interpretable and deemed **plausible** by humans (see @def-explainability). Instead, the choice has typically been to improve the ability of XAI tools to identify the subset explanations that are both plausible and valid for any given model, independent of whether the learned representations are also compatible with implausible explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that explainability can arise as byproduct of regularization techniques aimed at other objectives such as robustness, generalization, and generative capacity [@schut2021generating; @augustin2020adversarial; @altmeyer2024faithful]. 

Building on these findings, we introduce **counterfactual training**: a novel training regime explicitly geared towards aligning model representations with plausible explanations. Our contributions are as follows:

- We discuss existing related work on improving models and consolidate it through the lens of counterfactual explanations (@sec-lit).
- We present our proposed methodological framework that leverages faithful counterfactual explanations during the training phase of models to achieve the explainability objective (@sec-method).
- Through extensive experiments we demonstrate the counterfactual training improve model explainability while maintaining high predictive performance. We run ablation studies and grid searches to understand how the underlying model components and hyperparameters affect outcomes. (@sec-experiments). 

Despite some limitations of our approach discussed in @sec-discussion, we conclude in @sec-conclusion that counterfactual training provides a practical framework for researchers and practitioners interested in making opaque models more trustworthy. We also believe that this work serves as an opportunity for XAI researchers to re-evaluate the trend of improving XAI tools without improving the underlying models. 

