\makeatletter
\@ifpackageloaded{hyperref}{}{%
  \newcommand{\texorpdfstring}[2]{#1}%
}
\newcommand{\tightlist}
\makeatother
\providecommand\phantomsection{}

%File: anonymous-submission-latex-2026.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
% \title{AAAI Press Anonymous Submission\\Instructions for Authors Using \LaTeX{}}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{placeins}
\usepackage{siunitx}
\sisetup{uncertainty-mode = separate}
\DeclareMathSizes{10}{9}{7}{6.5}
\usepackage{enumitem}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}
\author{Patrick Altmeyer \and Aleksander Buszydlik \and Arie van
Deursen \and Cynthia C. S. Liem}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be plausible with
respect to the underlying data and actionable with respect to the
feature mutability constraints. Much existing research has therefore
focused on developing post-hoc methods to generate counterfactuals that
meet these desiderata. In this work, we instead hold models directly
accountable for the desired end goal: counterfactual training employs
counterfactuals during the training phase to minimize the divergence
between learned representations and plausible, actionable explanations.
We demonstrate empirically and theoretically that our proposed method
facilitates training models that deliver inherently desirable
counterfactual explanations and exhibit greatly improved adversarial
robustness.
\end{abstract}
\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by the success of representation learning with high degrees of
freedom: instead of relying on features and rules hand-crafted by
humans, modern machine learning (ML) models are tasked with learning
highly complex representations directly from the data, guided by narrow
objectives such as predictive accuracy \citep{goodfellow2016deep}. These
models tend to be so complex that humans cannot easily interpret their
decision logic.

Counterfactual explanations (CE) have become a key part of the broader
explainable AI (XAI) toolkit \citep{molnar2022interpretable} that can be
applied to make sense of this complexity. Originally proposed by
\citet{wachter2017counterfactual}, CEs prescribe minimal changes for
factual inputs that, if implemented, would prompt some fitted model to
produce an alternative, more desirable output. This is useful and
necessary to not only understand how opaque models make their
predictions, but also to provide algorithmic recourse (AR) to
individuals subjected to them: a retail bank, for example, could use CE
to provide meaningful feedback to unsuccessful loan applicants that were
rejected based on an opaque automated decision-making (ADM) system
(Figure~\ref{fig-poc}).

For such feedback to be meaningful, counterfactual explanations need to
fulfill certain desiderata
\citep{verma2020counterfactual, karimi2020survey}---they should be
faithful to the model \citep{altmeyer2024faithful}, plausible
\citep{joshi2019realistic} and actionable \citep{ustun2019actionable}.
Plausibility is typically understood as counterfactuals being
\emph{in-domain}: unsuccessful loan applicants that implement the
provided recourse should end up with credit profiles that are genuinely
similar to that of individuals who have successfully repaid their loans
in the past. Actionable explanations comply with practical constraints:
a young, unsuccessful loan applicant cannot increase their age in an
instance.

Existing state-of-the-art (SOTA) approaches in the field have largely
focused on designing model-agnostic CE methods that identify subsets of
counterfactuals, which comply with specific desiderata. This is
problematic, because the narrow focus on any specific desideratum can
adversely affect others: it is possible, for example, to generate
plausible counterfactuals for models that are also highly vulnerable to
implausible, possibly adversarial counterfactuals
\citep{altmeyer2024faithful}. In this work, we therefore embrace the
paradigm that models (as opposed to explanation methods) should be held
accountable for explanations that are plausible and actionable. While
previous work has shown that at least plausibility can be indirectly
achieved through existing techniques aimed at models' generative
capacity, generalization and robustness
\citep{altmeyer2024faithful, augustin2020adversarial, schut2021generating},
we directly incorporate both plausibility and actionability in the
training objective of models to improve their overall explanatory
capacity.

Specifically, we propose \textbf{counterfactual training (CT)}: a novel
training regime that leverages counterfactual explanations on-the-fly to
ensure that differentiable models learn plausible and actionable
explanations for the underlying data, while at the same time also being
more robust to adversarial examples (AE). Figure~\ref{fig-poc}
illustrates the outcomes of CT compared to a conventionally trained
model. First, in panel (a), faithful and valid counterfactuals end up
near the decision boundary forming a clearly distinguishable cluster in
the target class (orange). In panel (b), CT is applied to the same
underlying linear classifier architecture resulting in much more
plausible counterfactuals. In panel (c), the classifier is again trained
conventionally and we have introduced a mutability constraint on the
\emph{age} feature at test time---counterfactuals are valid but the
classifier is roughly equally sensitive to both features. By contrast,
the decision boundary in panel (d) has titled, making the model trained
with CT relatively less sensitive to the immutable \emph{age} feature.
To achieve these outcomes, CT draws inspiration from the literature on
contrastive and robust learning: we contrast faithful CEs with
ground-truth data while protecting immutable features, and capitalize on
methodological links between CE and AE by penalizing the model's
adversarial loss on interim (\emph{nascent}) counterfactuals. To the
best of our knowledge, CT represents the first venture in this direction
with promising empirical and theoretical results.

The remainder of this manuscript is structured as follows.
Section~\ref{sec-lit} presents related work, focusing on the links to
contrastive and robust learning. Then follow our two principal
contributions. In Section~\ref{sec-method}, we introduce our
methodological framework and show theoretically that it can be employed
to respect global actionability constraints. In our experiments
(Section~\ref{sec-experiments}), we find that thanks to counterfactual
training, (1) the implausibility of CEs decreases by up to 90\%; (2) the
cost of reaching valid counterfactuals with protected features decreases
by 19\% on average; and (3) models' adversarial robustness improves
across the board. Finally, we discuss open challenges in
Section~\ref{sec-discussion} and conclude in
Section~\ref{sec-conclusion}.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Counterfactual explanations (stars) for linear
classifiers trained under different regimes on synthetic data: (a)
conventional training, all mutable; (b) CT, all mutable; (c)
conventional, \emph{age} immutable; (d) CT, \emph{age} immutable. The
linear decision boundary is shown in green along with training data
colored according to ground-truth labels: \(y^-=\text{"loan withheld"}\)
(blue) and \(y^+=\text{"loan provided"}\) (orange). Class and feature
annotations (\emph{debt} and \emph{age}) are for illustrative purposes.}

\end{figure*}%

\section{Related Literature}\label{sec-lit}

To make the desiderata for our framework more concrete, we follow
previous work in tying the explanatory capacity of models to the quality
of CEs that can be generated for them
\citep{altmeyer2024faithful, augustin2020adversarial}. For simplicity,
we refer to ``explanatory capacity'' as ``explainability'' in the rest
of this manuscript (see Def. \ref{def-explainability}).

\subsection{Explainability and Contrastive
Learning}\label{explainability-and-contrastive-learning}

In a closely related work, \citet{altmeyer2024faithful} show that model
averaging and, in particular, contrastive model objectives can produce
more explainable and hence trustworthy models. The authors propose a way
to generate counterfactuals that are maximally faithful in that they are
consistent with what models have learned about the underlying data.
Formally, they rely on tools from energy-based modelling
\citep{teh2003energy} to minimize the contrastive divergence between the
distribution of counterfactuals and the conditional posterior over
inputs learned by a model. Their algorithm, \emph{ECCCo}, yields
plausible counterfactual explanations if and only if the underlying
model has learned representations that align with them. The authors find
that both deep ensembles \citep{lakshminarayanan2016simple} and joint
energy-based models (JEMs) \citep{grathwohl2020your}, a form of
constrastive learning, tend to do well in this regard.

It helps to look at these findings through the lens of representation
learning with high degrees of freedom. Deep ensembles are approximate
Bayesian model averages, which are particularly effective when models
are underspecified by the available data \citep{wilson2020case}.
Averaging across solutions mitigates the risk of overrelying on a single
locally optimal representation that corresponds to semantically
meaningless explanations. Likewise, previous work of
\citet{schut2021generating} found that generating plausible
(``interpretable'') CEs is almost trivial for deep ensembles that have
undergone adversarial training. The case for JEMs is even clearer: they
optimize a hybrid objective that induces both high predictive
performance and strong generative capacity \citep{grathwohl2020your},
which resembles the idea of aligning models with plausible explanations
and has inspired CT.

\subsection{Explainability and Robust
Learning}\label{explainability-and-robust-learning}

\citet{augustin2020adversarial} show that CEs tend to be more meaningful
(``explainable'') if the underlying model is more robust to adversarial
examples. Once again, we can make intuitive sense of this finding if we
look at adversarial training (AT) through the lens of representation
learning with high degrees of freedom: highly complex and flexible
models may learn representations that make them sensitive to implausible
or even adversarial examples \citep{szegedy2013intriguing}. Thus, by
inducing models to ``unlearn'' susceptibility to such examples,
adversarial training can effectively remove implausible explanations
from the solution space.

This interpretation of the link between explainability through
counterfactuals on the one side, and robustness to adversarial examples
on the other is backed by empirical evidence.
\citet{sauer2021counterfactual} demonstrate that using counterfactual
images during classifier training improves model robustness. Similarly,
\citet{abbasnejad2020counterfactual} argue that counterfactuals
represent potentially useful training data in machine learning,
especially in supervised settings where inputs may be reasonably mapped
to multiple outputs. They, too, show that augmenting the training data
of (image) classifiers can improve generalization performance. Finally,
\citet{teney2020learning} argue that counterfactual pairs tend to exist
in training data. Hence, their approach aims to identify similar input
samples with different annotations and ensure that the gradient of the
classifier aligns with the vector between such pairs of counterfactual
inputs using a cosine distance loss function.

CEs have also been used to improve models in the natural language
processing domain. For example, \citet{wu2021polyjuice} propose
\emph{Polyjuice}, a general-purpose CE generator for language models and
demonstrate that the augmentation of training data with \emph{Polyjuice}
improves robustness in a number of tasks, while
\citet{luu2023counterfactual} introduce the \emph{Counterfactual
Adversarial Training} (CAT) framework that aims to improve
generalization and robustness of language models by generating
counterfactuals for training samples that are subject to high predictive
uncertainty.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples. Pointing
to clear similarities in how CEs and AEs are generated,
\citet{freiesleben2022intriguing} makes the case for jointly studying
the opaqueness and robustness problems in representation learning.
Formally, AEs can be seen as the subset of CEs for which
misclassification is achieved \citep{freiesleben2022intriguing}.
Similarly, \citet{pawelczyk2022exploring} show that CEs and AEs are
equivalent under certain conditions.

Two other works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of the post-hoc counterfactual explanations. Firstly,
\citet{ross2021learning} propose a way to train models that guarantee
recourse to a positive target class with high probability. Their
approach builds on adversarial training by explicitly inducing
susceptibility to targeted AEs for the positive class. Additionally, the
method allows for imposing a set of actionability constraints ex-ante.
For example, users can specify that certain features are immutable.
Secondly, \citet{guo2023counternet} are the first to propose an
end-to-end training pipeline that includes CEs as part of the training
procedure. Their \emph{CounterNet} network architecture includes a
predictor and a CE generator, where the parameters of the CE generator
are learnable. Counterfactuals are generated during each training
iteration and fed back to the predictor. In contrast, we impose no
restrictions on the ANN architecture at all.

\section{Counterfactual Training}\label{sec-method}

This section introduces the counterfactual training framework, applying
ideas from contrastive and robust learning to counterfactual
explanations. CT produces models whose learned representations align
with plausible explanations that comply with user-defined actionability
constraints.

Counterfactual explanations are typically generated by solving
variations of the following optimization problem,
\begin{equation}\phantomsection\label{eq-general}{
\begin{aligned}
\min_{\mathbf{X}^\prime \in \mathcal{X}^D} \left\{  {\text{yloss}(\mathbf{M}_\theta(\mathbf{x}^{\prime}),\mathbf{y}^+)}+ \lambda {\text{reg}(\mathbf{x}^{\prime}) }  \right\} 
\end{aligned}
}\end{equation} where
\(\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}\) denotes a
classifier, \(\mathbf{x}^{\prime}\) denotes the counterfactual with
\(D\) features and \(\mathbf{y}^+\in\mathcal{Y}\) denotes some target
class. The \(\text{yloss}(\cdot)\) function quantifies the discrepancy
between current model predictions for \(\mathbf{x}^{\prime}\) and the
target class (a conventional choice is cross-entropy). Finally, we use
\(\text{reg}(\cdot)\) to denote any form of regularization used to
induce certain properties on the counterfactual. In their seminal paper,
\citet{wachter2017counterfactual} propose regularizing the distance
between counterfactuals and their original factual values to ensure that
individuals seeking recourse through CE face minimal costs in terms of
feature changes. Different variations of Equation~\ref{eq-general} have
been proposed in the literature to address many desiderata including the
ones discussed above (faithfulness, plausibility and actionability).
Like \citet{wachter2017counterfactual}, most of these approaches rely on
gradient descent to optimize Equation~\ref{eq-general}. For more details
on the approaches tested in this work, we refer the reader to the
supplementary appendix. In the following, we describe in detail how
counterfactuals are generated and used in counterfactual training.

\subsection{Proposed Training
Objective}\label{proposed-training-objective}

The goal of CT is to improve model explainability by aligning models
with faithful explanations that are plausible and actionable. Formally,
we define explainability as follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$, where $\arg\max_y{\mathbf{y}^{+}}=y^+$ is the index of the target class. 

We say that $\mathbf{M}_\theta$ has an \textbf{explanatory capacity} to the extent that faithfully generated, valid counterfactuals are also plausible and actionable. We define these properties as:

\begin{itemize}
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ is an arbitrarily small region around $\mathbf{x}^{\prime}$.
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ as specified above.
    \item (Actionability) Perturbations $\Delta$ may be subject to some actionability constraints.
\end{itemize}
Here, $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior distribution over inputs. For simplicity, we refer to a model with high explanatory capacity as \textbf{explainable} in this manuscript. 
\end{definition}

The characterization of faithfulness and plausibility in Def.
\ref{def-explainability} follows \citet{altmeyer2024faithful}, with
adapted notation. Intuitively, plausible counterfactuals are consistent
with the data and faithful counterfactuals are consistent with what the
model has learned about the input data. Ac tionability constraints in
Def. \ref{def-explainability} vary and depend on the context in which
\(\mathbf{M}_\theta\) is deployed. In this work, we choose to only
consider domain and mutability constraints for individual features
\(x_d\) for \(d=1,...,D\). We also limit ourselves to classification
tasks for reasons discussed in Section~\ref{sec-discussion}.

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
generated through gradient descent over \(T\) iterations as originally
proposed by \citet{wachter2017counterfactual}. CT adopts gradient-based
CE search in training to generate on-the-fly model explanations
\(\mathbf{x}^\prime\) for the training samples. We use the term
\emph{nascent} to denote interim counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) that have not yet converged. As we
explain below, these nascent counterfactuals can be stored and
repurposed as adversarial examples. Conversely, we consider
counterfactuals \(\mathbf{x}_T^\prime\) as \emph{mature} explanations if
they have either exhausted all \(T\) iterations or converged by reaching
a pre-specified threshold, \(\tau\), for the predicted probability of
the target class:
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.

Formally, we propose the following counterfactual training objective to
train explainable (as in Def. \ref{def-explainability}) models,
\begin{equation}\phantomsection\label{eq-obj}{
\begin{aligned}
&\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) \\+ &\lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) + \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{aligned}
}\end{equation} where \(\text{yloss}(\cdot)\) is any classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms are explained in detail below. For now, they can
be summarized as inducing explainability directly and indirectly by
penalizing (1) the contrastive divergence, \(\text{div}(\cdot)\),
between mature counterfactuals \(\mathbf{x}_T^\prime\) and observed
samples \(\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}\) in the
target class \(y^+\), and (2) the adversarial loss,
\(\text{advloss}(.)\), wrt. nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\). Finally, \(\text{ridge}(\cdot)\)
denotes a Ridge penalty (\(\ell_2\)-norm) that regularizes the magnitude
of the energy terms involved in \(\text{div}(\cdot)\)
\citep{du2019implicit}. The trade-offs between these components are
adjusted through \(\lambda_{\text{div}}\), \(\lambda_{\text{adv}}\) and
\(\lambda_{\text{reg}}\). The full training regime is sketched out in
Algorithm \ref{alg-experiment}.

\begin{algorithm}[h]
  \caption{Counterfactual Training}
    \label{alg-experiment}
    \begin{algorithmic}[1]
    \REQUIRE Training dataset $\mathcal{D}$, initialize model $\mathbf{M}_{\theta}$
    \WHILE{not converged}
        \STATE Sample $\mathbf{x}$ and $\mathbf{y}$ from dataset $\mathcal{D}$.
        \STATE Sample $\mathbf{x}^{\prime}_0$, $\mathbf{y}^+$ and $\mathbf{x}^+$.
        \FOR{$t = 1$ to $T$}
            \STATE Backpropagate $\nabla_{\mathbf{x}^\prime}$ through Equation \ref{eq-general}. Store $\mathbf{x}_t^\prime$.
        \ENDFOR
        \STATE Backpropagate $\nabla_{\theta}$ through Equation \ref{eq-obj}.
    \ENDWHILE
    \RETURN $\mathbf{M}_\theta$
    \end{algorithmic}
\end{algorithm}

\subsection{Directly Inducing Explainability with Contrastive
Divergence}\label{directly-inducing-explainability-with-contrastive-divergence}

\citet{grathwohl2020your} observe that any classifier can be
re-interpreted as a joint energy-based model that learns to discriminate
output classes conditional on the observed (training) samples from
\(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). The authors show that JEMs can be trained to
perform well at both tasks by directly maximizing the joint
log-likelihood:
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\),
where the first term can be optimized using cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\), they
minimize the contrastive divergence between the observed samples from
\(p(\mathbf{x})\) and samples generated from \(p_\theta(\mathbf{x})\).

To generate samples, \citet{grathwohl2020your} use Stochastic Gradient
Langevin Dynamics (SGLD) with an uninformative prior for initialization
but we depart from their methodology: we propose to leverage
counterfactual explainers to generate counterfactuals of observed
training samples. Specifically, we have:
\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation} where \(\mathcal{E}_\theta(\cdot)\) denotes the energy
function defined as
\(\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]\),
with \(y^+\) denoting the index of the randomly drawn target class,
\(y^+ \sim p(y)\). Conditional on the target class \(y^+\),
\(\mathbf{x}_T^\prime\) denotes a mature counterfactual for a randomly
sampled factual from a non-target class generated with a gradient-based
CE generator for up to \(T\) iterations. Intuitively, the gradient of
Equation~\ref{eq-div} decreases the energy of observed training samples
(positive samples) while increasing the energy of counterfactuals
(negative samples) \citep{du2019implicit}. As the counterfactuals get
more plausible (Def. \ref{def-explainability}) during training, these
opposing effects gradually balance each other out
\citep{lippe2024uvadlc}.

Since maturity of counterfactuals in terms of a probability threshold is
often reached before \(T\), this form of sampling is not only more
closely aligned with Def. \ref{def-explainability}., but can also speed
up training times compared to SGLD. The departure from SGLD also allows
us to tap into the vast repertoire of explainers that have been proposed
in the literature to meet different desiderata. For example, many
methods support domain and mutability constraints. In principle, any
existing approach for generating CEs is viable, so long as it does not
violate the faithfulness condition. Like JEMs
\citep{murphy2022probabilistic}, counterfactual training can be
considered a form of contrastive representation learning.

\subsection{Indirectly Inducing Explainability with Adversarial
Robustness}\label{indirectly-inducing-explainability-with-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
\citep{balashankar2023improving, luu2023counterfactual} or adversarial
examples \citep{freiesleben2022intriguing, pawelczyk2022exploring}. This
leaves some flexibility with regards to the choice for the
\(\text{advloss}(\cdot)\) term in Equation~\ref{eq-obj}. An intuitive
functional form, but likely not the only sensible choice, is inspired by
adversarial training: \begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation} Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as AEs as long as the magnitude of the
perturbation to any single feature is at most \(\varepsilon\). This is
closely aligned with \citet{szegedy2013intriguing} who define an
adversarial attack as an ``imperceptible non-random perturbation''.
Thus, we work with a different distinction between CE and AE than
\citet{freiesleben2022intriguing} who considers misclassification as the
distinguishing feature of adversarial examples. One of the key
observations of this work is that we can leverage CEs during training
and get AEs essentially for free to reap the aforementioned benefits of
adversarial training.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints. In fact, both types of constraints can be implemented for
any explainer that relies on gradient descent in the feature space for
optimization \citep{altmeyer2023explaining}. In this context, domain
constraints can be imposed by simply projecting counterfactuals back to
the specified domain, if the previous gradient step resulted in updated
feature values that were out-of-domain. Similarly, mutability
constraints can be enforced by setting partial derivatives to zero to
ensure that features are only perturbed in the allowed direction, if at
all.

Since actionability constraints are binding at test time, we also impose
them when generating \(\mathbf{x}^\prime\) during each training
iteration to inform model representations. Through their effect on
\(\mathbf{x}^\prime\), both types of constraints influence model
outcomes via Equation~\ref{eq-div}. Here it is crucial that we avoid
penalizing implausibility that arises due to mutability constraints. For
any mutability-constrained feature \(d\) this can be achieved by
enforcing \(\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0\) whenever
perturbing \(\mathbf{x}^\prime[d]\) in the direction of
\(\mathbf{x}^+[d]\) would violate mutability constraints. Specifically,
we set \(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  \(\mathbf{x}^+[d]>\mathbf{x}^\prime[d]\), but \(d\) can only be
  decreased in practice.
\item
  \(\mathbf{x}^+[d]<\mathbf{x}^\prime[d]\), but \(d\) can only be
  increased in practice.
\end{enumerate}

\noindent From a Bayesian perspective, setting
\(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) can be understood as
assuming a point mass prior for \(p(\mathbf{x}^+)\) wrt. feature \(d\).
Intuitively, we think of this as ignoring implausibility costs of
immutable features, which effectively forces the model to instead seek
plausibility through the remaining features. This can be expected to
result in relatively lower sensitivity to immutable features; and higher
relative sensitivity to mutable features should make
mutability-constrained recourse less costly
(Section~\ref{sec-experiments}). Under certain conditions, this result
holds theoretically; for the proof, see the supplementary appendix:

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. Assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}

\section{Experiments}\label{sec-experiments}

We seek to answer the following four research questions:

\begin{enumerate}[label={(\makebox[2em][c]{RQ\arabic*})}, leftmargin=3.5em]
    \item To what extent does the CT objective in Equation 1 induce models to learn plausible explanations?
    \item To what extent does CT result in more favorable algorithmic recourse outcomes in the presence of actionability constraints?
    \item To what extent does CT influence the adversarial robustness of trained models?
    \item What are the effects of hyperparameter selection on counterfactual training?
\end{enumerate}

\subsection{Experimental Setup}\label{experimental-setup}

Our focus is the improvement in explainability (Def.
\ref{def-explainability}). Thus, we primarily look at the plausibility
and cost of faithfully generated counterfactuals at test time. Other
metrics, such as validity and redundancy, are reported in the
supplementary appendix. To measure the cost, we follow the standard
proxy of distances (\(\ell_1\)-norm) between factuals and
counterfactuals. For plausibility, we assess how similar CEs are to
observed samples in the target domain,
\(\mathbf{X}^+\subset\mathcal{X}^+\). We rely on the metric used by
\citet{altmeyer2024faithful},
\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{IP}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation} and introduce a novel divergence-based adaptation,
\begin{equation}\phantomsection\label{eq-impl-div}{
\text{IP}^*(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation} where \(\mathbf{X}^\prime\) denotes a collection of
counterfactuals and \(\text{MMD}(\cdot)\) is the unbiased estimate of
the squared population maximum mean discrepancy, proposed by
\citet{gretton2012kernel}. The metric in Equation~\ref{eq-impl-div} is
equal to zero if and only if the two distributions are exactly the same,
\(\mathbf{X}^\prime=\mathbf{X}^+\).

To assess outcomes with respect to actionability for non-linear models,
we look at the average costs of valid counterfactuals in terms of their
distances from factual starting points. While this an imperfect proxy of
sensitivity, we hypothesize that CT can reduce these costs by teaching
models to seek plausibility with respect to mutable features, much like
we observe in Figure~\ref{fig-poc} in panel (d) compared to (c). We
supplement this analysis with qualitative findings for integrated
gradients \citep{sundararajan2017ig}. Finally, for predictive
performance, we use standard metrics, such as robust accuracy estimated
on adversarially perturbed data using FGSM
\citep{goodfellow2014explaining}.

We run experiments with three gradient-based generators: \emph{Generic}
of \citet{wachter2017counterfactual} as a simple baseline approach,
\emph{REVISE} \citep{joshi2019realistic} that aims to generate plausible
counterfactuals using a surrogate Variational Autoencoder (VAE), and
\emph{ECCCo} \citep{altmeyer2024faithful}, which targets faithfulness.

We make use of nine classification datasets common in the CE/AR
literature. Four of them are synthetic with two classes and different
characteristics: linearly separable clusters (\emph{LS}), overlapping
clusters (\emph{OL}), concentric circles (\emph{Circ}), and interlocking
moons (\emph{Moon}). Next, we have four real-world binary tabular
datasets: \emph{Adult} (Census data) of \citet{becker1996adult2},
California housing (\emph{CH}) of \citet{pace1997sparse}, Default of
Credit Card Clients (\emph{Cred}) of \citet{yeh2016default}, and Give Me
Some Credit (\emph{GMSC}) from \citet{kaggle2011give}. Finally, for the
convenience of illustration, we use the 10-class \emph{MNIST}
\citep{lecun1998mnist}.

To assess CT, we investigate the improvements in performance metrics
when using it on top of a weak baseline (BL): a multilayer perceptron
(\emph{MLP}). This is the best way to get a clear picture of the
effectiveness of CT, and it is consistent with evaluation practices in
the related literature
\citep{goodfellow2014explaining, ross2021learning, teney2020learning}.

\subsection{Experimental Results}\label{experimental-results}

Our main results for plausibility and actionability for \emph{MLP}
models are summarised in Table~\ref{tbl-main} that presents
counterfactual outcomes grouped by dataset along with standard errors
averaged across bootstrap samples. Asterisks (\(^*\)) are used when the
bootstrapped 99\%-confidence interval of differences in mean outcomes
does \emph{not} include zero, so the observed effects are statistically
significant at the 0.01 level.

The first two columns (\(\text{IP}\) and \(\text{IP}^*\)) show the
percentage reduction in implausibility for our two metrics when using CT
on top of the weak baseline. As an example, consider the first row for
\emph{LS} data: the observed positive values indicate that faithful
counterfactuals are around 30-55\% more plausible for models trained
with CT, in line with our observations in panel (b) of
Figure~\ref{fig-poc} compared to panel (a).

The third column shows the results for a scenario when mutability
constraints are imposed on the selected features. Again, we are
comparing CT to the baseline, so reductions in the positive direction
imply that valid counterfactuals are ``cheaper'' (more actionable) when
using CT with feature protection. Relating this back to
Figure~\ref{fig-poc}, the third column represents the reduction in
distances travelled by counterfactuals in panel (d) compared to panel
(c). In the following paragraphs, we summarize the results for all
datasets.

\begin{table}

\caption{\label{tbl-main}Key evaluation metrics for valid counterfactual
along with bootstrapped standard errors for all datasets.
\textbf{Plausibility} (columns 1-2): percentage reduction in
implausibility for \(\text{IP}\) and \(\text{IP}^*\), respectively;
\textbf{Cost} / \textbf{Actionability} (column 3): percentage reduction
in costs when selected features are protected. Outcomes are aggregated
across bootstrap samples (100 rounds) and varying degrees of the energy
penalty \(\lambda_{\text{egy}}\) used for \emph{ECCCo} at test time.
Asterisks (\(^*\)) indicate that the bootstrapped 99\%-confidence
interval of differences in mean outcomes does \emph{not} include zero.}

\centering{

\small
\centering
\begin{tabular}{
  l
  S[table-format=2.2(1.2)]
  S[table-format=3.2(3.2)]
  S[table-format=3.2(1.2)]
}
  \toprule
  \textbf{Data} & \textbf{$ \text{IP} $ $(-\%)$} & \textbf{$ \text{IP}^* $ $(-\%)$} & \textbf{Cost $(-\%)$} \\\midrule
  LS & 29.05\pm0.67 $^{*}$ & 55.33\pm2.03 $^{*}$ & 14.07\pm0.6 $^{*}$ \\
  Circ & 56.29\pm0.44 $^{*}$ & 89.38\pm9.3 $^{*}$ & 45.55\pm0.76 $^{*}$ \\
  Moon & 20.62\pm0.69 $^{*}$ & 19.26\pm8.12 $^{*}$ & 2.86\pm1.03 $^{*}$ \\
  OL & -1.13\pm0.88 $^{}$ & -24.52\pm14.52 $^{}$ & 38.39\pm2.21 $^{*}$ \\\midrule
  Adult & 0.77\pm1.34 $^{}$ & 32.29\pm6.87 $^{*}$ & -2.82\pm4.88 $^{}$ \\
  CH & 12.05\pm1.41 $^{*}$ & 70.27\pm3.72 $^{*}$ & 40.71\pm1.55 $^{*}$ \\
  Cred & 12.31\pm1.84 $^{*}$ & 54.89\pm11.21 $^{*}$ & -17.43\pm5.17 $^{*}$ \\
  GMSC & 23.44\pm1.99 $^{*}$ & 73.31\pm4.83 $^{*}$ & 62.64\pm2.04 $^{*}$ \\
  MNIST & 7.05\pm1.8 $^{*}$ & -25.09\pm109.05 $^{}$ & -12.34\pm6.52 $^{}$ \\\midrule
  Avg. & 17.83 & 38.35 & 19.07 \\\bottomrule
\end{tabular}

}

\end{table}%

\subsubsection{Plausibility (RQ1).}\label{sec-plaus}

\emph{CT generally produces substantial and statistically significant
improvements in plausibility.}

Average reductions in \(\text{IP}\) range from around 7\% for
\emph{MNIST} to almost 60\% for \emph{Circ}. For the real-world tabular
datasets they are around 12\% for \emph{CH} and \emph{Cred} and almost
25\% for \emph{GMSC}; for \emph{Adult} and \emph{OL} we find no
significant impact of CT on \(\text{IP}\). Reductions in \(\text{IP}^*\)
are even more substantial and generally statistically significant,
although the average degree of uncertainty is higher than for
\(\text{IP}\): reductions range from around 20\% (\emph{Moons}) to
almost 90\% (\emph{Circ}). The only negative findings are for OL and
MNIST, but they are not statistically significant. A qualitative
inspection of the counterfactuals in Figure~\ref{fig-mnist} (columns
2-5) suggests recognizable digits 1-4 for the model trained with CT
(bottom row), unlike the baseline (top row).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_body.png}}

}

\caption{\label{fig-mnist}Visual explanations for \emph{MNIST} for BL
(top) and CT (bottom). \textbf{Plausibility}: col.~1 is a random factual
0 (blue); cols. 2-5 are corresponding \emph{ECCCo} counterfactuals in
target classes 1 to 4. \textbf{Actionability}: cols. 6-10 show
integrated gradients averaged over test images in classes 5 to 9.}

\end{figure}%

\subsubsection{Actionability (RQ2).}\label{sec-act}

\emph{CT tends to improve actionability in the presence of immutable
features, but this is not guaranteed if the assumptions in Proposition
\ref{prp-mtblty} are violated.}

For synthetic datasets, we always protect the first feature; for all
real-world tabular datasets we could identify and protect an \emph{age}
variable; for \emph{MNIST}, we protect the five upper and lower pixel
rows of the full image. Statistically significant reductions in costs
overwhelmingly point in the expected positive direction reaching up to
around 60\% for \emph{GMSC}. Only in the case of \emph{Cred}, average
costs increase, likely because any potential benefits from protecting
the \emph{age} are outweighed by the increase in costs required for
greater plausibility. The findings for \emph{Adult} and \emph{MNIST} are
not significant. A qualitative inspection of the class-conditional
integrated gradients in Figure~\ref{fig-mnist} (columns 6-10) suggests
that CT still has the expected effect: the model (bottom) is insensitive
(blue) to the protected rows of pixels; details of this experiment are
reported in the supplementary appendix.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/acc.png}}

}

\caption{\label{fig-acc}Test accuracies on adversarially perturbed data
with varying perturbation sizes for all non-synthetic datasets.}

\end{figure*}%

\subsubsection{Predictive Performance (RQ3).}\label{sec-pred}

\emph{Models trained with CT are substantially more robust to
gradient-based adversarial attacks than conventionally-trained
baselines.}

Test accuracies on adversarially perturbed data are shown in
Figure~\ref{fig-acc}. The perturbations size, \(\varepsilon\in[0,0.1]\),
increases along the horizontal axis and includes zero, corresponding to
standard test accuracy for non-perturbed data. For all synthetic
datasets, predictive performance of CT is virtually identical to the
baseline and unaffected by perturbations. For all real-world datasets,
we find that CT substantially improves robustness: while in some cases
baseline accuracies drop to essentially zero for large enough
perturbation sizes, accuracies of CT models remain remarkably robust.

\subsubsection{Hyperparameter settings
(RQ4).}\label{sec-hyperparameters}

\emph{CT is highly sensitive to the choice of a CE generator and its
hyperparameters but (1) we observe manageable patterns, and (2) we can
usually identify settings that improve either plausibility or
actionability, and typically both of them at the same time.}

We evaluate the impacts of three types of hyperparameters on CT. In this
section we focus on the highlights and make the full results available
in the supplementary appendix.

Firstly, we find that optimal results are generally obtained when using
\emph{ECCCo} to generate counterfactuals. Conversely, using a generator
that may inhibit faithfulness (\emph{REVISE}), tends to yield poor
results. Concerning hyperparameters that guide the gradient-based
counterfactual search, we find that increasing \(T\), the maximum number
of steps, generally yields better outcomes because more CEs can mature.
Relatedly, we also find that the effectiveness and stability of CT is
positively associated with the total number of counterfactuals generated
during each training epoch. The impact of \(\tau\), the decision
threshold, is more difficult to predict. On ``harder'' datasets it may
be difficult to satisfy high \(\tau\) for any given sample (i.e., also
factuals) and so increasing this threshold does not seem to correlate
with better outcomes. In fact, \(\tau=0.5\) generally leads to optimal
results as it is associated with high proportions of mature
counterfactuals.

Secondly, the strength of the energy regularization,
\(\lambda_{\text{reg}}\) is highly impactful and should be set
sufficiently high to avoid common problems associated with exploding
gradients. The sensitivity with respect to \(\lambda_{\text{div}}\) and
\(\lambda_{\text{adv}}\) is much less evident. While high values of
\(\lambda_{\text{reg}}\) may increase the variability in outcomes when
combined with high values of \(\lambda_{\text{div}}\) or
\(\lambda_{\text{adv}}\), this effect is not particularly pronounced.

Finally, we also observe desired improvements when CT was combined with
conventional training and applied only for the final 50\% of epochs of
the complete training process. Put differently, CT can improve the
explainability of models in a post-hoc, fine-tuning manner.

\section{Discussion}\label{sec-discussion}

As our results indicate, counterfactual training produces models that
are more explainable. Nonetheless, these advantages come at the cost of
two important limitations.

\emph{Interventions on features have implications for fairness.} We
provide a method to modify the sensitivity of a model to certain
features, which can be misused by enforcing explanations based on
features that are more difficult to modify by a (group of) decision
subjects. Such abuse could result in an unfairly assigned burden of
recourse \citep{sharma2020certifai}, threatening the equality of
opportunity \citep{bell2024fairness}. Also, even if all immutable
features are protected, there may exist proxies that are theoretically
mutable, but preserve sufficient information about the principals to
hinder these protections. Indeed, deciding on the actionability of
features remains a major open challenge in the AR literature
\citep{venkatasubramanian2020philosophical}.

\emph{Plausibility is costly.} As noted by \citet{altmeyer2024faithful},
more plausible counterfactuals are inevitably more costly. CT improves
plausibility and robustness, but it can impact average costs and
validity when cheap, implausible and adversarial explanations are
removed from the solution space.

\emph{CT increases the training times.} Just like contrastive and robust
learning, CT is more resource-intensive than conventional regimes. Three
factors mitigate this effect: (1) CT yields itself to parallel
execution; (2) it amortizes the cost of CEs for the training samples;
and (3) it can be used to fine-tune conventionally-trained models.

We also highlight three key directions for future research. Firstly, it
is an interesting challenge to extend CT beyond classification settings.
Our formulation relies on the distinction between non-target class(es)
and target class(es), requiring the output space to be discrete. Thus,
it does not apply to ML tasks where the change in outcome cannot be
readily discretized. Focus on classification is a common choice in
research on CEs and AR; other settings have attracted some interest,
e.g., regression \citep{spooner2021counterfactual}, but there is little
consensus how to robustly extend the notion of CEs.

Secondly, our analysis covers CE generators with different
characteristics, but it is interesting to extend it to more algorithms,
including ones that do not rely on computationally costly gradient-based
optimization. This should reduce training costs while possibly
preserving the benefits of CT.

Finally, we believe that it is possible to considerably improve
hyperparameter selection procedures, and thus performance. We have
relied exclusively on grid searches, but future work could benefit from
more sophisticated approaches.

\section{Conclusion}\label{sec-conclusion}

State-of-the-art machine learning models are prone to learning complex
representations that cannot be interpreted by humans. Existing
explainability solutions cannot guarantee that explanations agree with
these learned representation. As a step towards addressing this
challenge, we introduce counterfactual training, a novel training regime
that integrates recent advances in contrastive learning, adversarial
robustness, and counterfactual explanations to incentivize
highly-explainable models. Through extensive experiments, we demonstrate
that CT satisfies this goal while preserving the predictive performance
and promoting robustness of models. Explanations generated from CT-based
models are both more plausible (compliant with the underlying
data-generating process) and more actionable (compliant with
user-specified mutability constraints), and thus meaningful to their
recipients. In turn, our work highlights the value of simultaneously
improving models and their explanations.

\bibliography{../quarto_aaai/bibliography.bib}
\end{document}
