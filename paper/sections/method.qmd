# Counterfactual Training {#sec-method}

This section introduces the counterfactual training framework. As mentioned above, CT applies ideas from contrastive and robust learning to counterfactual explanations in order to produce models whose learned representations align with plausible explanations that further comply with user-defined actionability constraints. 

Counterfactual explanations are typically generated by solving variations of the following optimization problem,
$$
\begin{aligned}
\min_{\mathbf{X}^\prime \in \mathcal{X}^D} \left\{  {\text{yloss}(\mathbf{M}_\theta(\mathbf{x}^{\prime}),\mathbf{y}^+)}+ \lambda {\text{reg}(\mathbf{x}^{\prime}) }  \right\} 
\end{aligned}
$$ {#eq-general}
where $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denotes a classifier, $\mathbf{x}^{\prime}$ denotes the counterfactual with $D$ features and $\mathbf{y}^+$ denotes some target class. The $\text{yloss}(\cdot)$ function quantifies the discrepancy between current model predictions for $\mathbf{x}^{\prime}$ and the target class (a conventional choice is cross-entropy). Finally, we use $\text{reg}(\cdot)$ to denote any form of regularization used to induce certain properties on the counterfactual. In their seminal paper, @wachter2017counterfactual propose regularizing the distance between counterfactuals and their original factual values to ensure that individuals seeking recourse through CE face minimal costs in terms of feature changes. Different variations of @eq-general have been proposed in the literature to address many desiderata including the ones discussed above (faithfulness, plausibility and actionability). Like @wachter2017counterfactual, most of these approaches rely on gradient descent to optimize @eq-general. For more details on the approaches tested in this work, we refer the reader to the supplementary appendix. In the following, we describe in detail how counterfactuals are generated and used in counterfactual training.

## Proposed Training Objective

The goal of CT is to improve model explainability by aligning models with faithful explanations that are plausible and actionable. Formally, we define explainability as follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$, where $\arg\max_y{\mathbf{y}^{+}}=y^+$ is the index of the target class. 

We say that $\mathbf{M}_\theta$ has an \textbf{explanatory capacity} to the extent that faithfully generated counterfactuals are also plausible and actionable. We define these properties as follows:

\begin{itemize}
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ is an arbitrarily small region around $\mathbf{x}^{\prime}$.
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ as specified above.
    \item (Actionability) Perturbations $\Delta$ may be subject to some actionability constraints.
\end{itemize}
and $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior distribution over inputs. For simplicity, we refer to a model with high explanatory capacity as \textbf{explainable} in this manuscript. 
\end{definition}

The characterization of faithfulness and plausibility in Def. \ref{def-explainability} is the same as in @altmeyer2024faithful, with adapted notation. Intuitively, plausible counterfactuals are consistent with the data and faithful counterfactuals are consistent with what the model has learned about the input data. Actionability constraints in Def. \ref{def-explainability} vary and depend on the context in which $\mathbf{M}_\theta$ is deployed. In this work, we choose to only consider domain and mutability constraints for individual features $x_d$ for $d=1,...,D$. We also limit ourselves to classification tasks for reasons discussed in @sec-discussion.

Let $\mathbf{x}_t^\prime$ for $t=0,...,T$ denote a counterfactual generated through gradient descent over $T$ iterations as originally proposed by @wachter2017counterfactual. CT adopts gradient-based CE search in training to generate on-the-fly model explanations $\mathbf{x}^\prime$ for the training samples. We use the term *nascent* to denote interim counterfactuals $\mathbf{x}_{t\leq T}^\prime$ that have not yet converged. As we explain below, these nascent counterfactuals can be stored and repurposed as adversarial examples. Conversely, we consider counterfactuals $\mathbf{x}_T^\prime$ as *mature* explanations if they have either exhausted all $T$ iterations or converged by reaching a pre-specified threshold, $\tau$, for the predicted probability of the target class: $\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau$, where $\mathcal{S}$ is the softmax function.

Formally, we propose the following counterfactual training objective to train explainable (as in Def. \ref{def-explainability}) models:
$$
\begin{aligned}
&\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) \\+ &\lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) + \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{aligned}
$$ {#eq-obj}
where $\text{yloss}(\cdot)$ is any classification loss that induces discriminative performance (e.g., cross-entropy). The second and third terms are explained in detail below. For now, they can be summarized as inducing explainability directly and indirectly by penalizing (1) the contrastive divergence, $\text{div}(\cdot)$, between mature counterfactuals $\mathbf{x}_T^\prime$ and observed samples $\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}$ in the target class $y^+$, and (2) the adversarial loss, $\text{advloss}(.)$, wrt. nascent counterfactuals $\mathbf{x}_{t\leq T}^\prime$. Finally, $\text{ridge}(\cdot)$ denotes a Ridge penalty ($\ell_2$-norm) that regularizes the magnitude of the energy terms involved in $\text{div}(\cdot)$ [@du2019implicit]. The trade-offs between these components are adjusted through $\lambda_{\text{div}}$, $\lambda_{\text{adv}}$ and $\lambda_{\text{reg}}$. The full training regime is sketched out in Algorithm \ref{alg-experiment}.

\begin{algorithm}[h]
  \caption{Counterfactual Training}
    \label{alg-experiment}
    \begin{algorithmic}[1]
    \REQUIRE Training dataset $\mathcal{D}$, initial model $\mathbf{M}_{\theta_0}$
    \WHILE{not converged}
        \STATE Sample $\mathbf{x}$ and $\mathbf{y}$ from dataset $\mathcal{D}$.
        \STATE Sample $\mathbf{x}^{\prime}_0$, $\mathbf{y}^+$ and $\mathbf{x}^+$.
        \FOR{$t = 1$ to $T$}
            \STATE Backpropagate $\nabla_{\mathbf{x}^\prime}$ through Equation \ref{eq-general}. Store $\mathbf{x}_t^\prime$.
        \ENDFOR
        \STATE Backpropagate $\nabla_{\theta}$ through Equation \ref{eq-obj}.
    \ENDWHILE
    \RETURN $\mathbf{M}_\theta$
    \end{algorithmic}
\end{algorithm}

## Directly Inducing Explainability with Contrastive Divergence

@grathwohl2020your observe that any classifier can be re-interpreted as a joint energy-based model that learns to discriminate output classes conditional on the observed (training) samples from $p(\mathbf{x})$ and the generated samples from $p_\theta(\mathbf{x})$. The authors show that JEMs can be trained to perform well at both tasks by directly maximizing the joint log-likelihood: $\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})$, where the first term can be optimized using cross-entropy as in @eq-obj. To optimize $\log p_\theta(\mathbf{x})$, they minimize the contrastive divergence between the observed samples from $p(\mathbf{x})$ and the generated samples from $p_\theta(\mathbf{x})$. 

A key empirical finding of @altmeyer2024faithful was that JEMs perform well on the plausibility objective in Def. \ref{def-explainability}. This follows directly if we consider samples drawn from $p_\theta(\mathbf{x})$ as counterfactuals --- the JEM objective effectively minimizes the divergence between the conditional posterior and $p(\mathbf{x}|\mathbf{y}^{+})$. To generate samples, @grathwohl2020your use Stochastic Gradient Langevin Dynamics (SGLD) with an uninformative prior for initialization but we depart from their methodology. Instead we propose to leverage counterfactual explainers to generate counterfactuals of observed training samples. Specifically, we have:
$$
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
$$ {#eq-div}
where $\mathcal{E}_\theta(\cdot)$ denotes the energy function defined as $\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]$, with $y^+$ denoting the index of the randomly drawn target class, $y^+ \sim p(y)$. Conditional on the target class $y^+$, $\mathbf{x}_T^\prime$ denotes a mature counterfactual for a randomly sampled factual from a non-target class generated with a gradient-based CE generator for up to $T$ iterations. Mature counterfactuals are ones that have either reached convergence wrt. the decision threshold $\tau$ or exhausted $T$.

Intuitively, the gradient of @eq-div decreases the energy of observed training samples (positive samples) while increasing the energy of counterfactuals (negative samples) [@du2019implicit]. As the counterfactuals get more plausible (Def. \ref{def-explainability}) during training, these opposing effects gradually balance each other out [@lippe2024uvadlc]. 

The departure from SGLD allows us to tap into the vast repertoire of explainers that have been proposed in the literature to meet different desiderata. For example, many methods support domain and mutability constraints. In principle, any existing approach for generating CEs is viable, so long as it does not violate the faithfulness condition. Like JEMs [@murphy2022probabilistic], Counterfactual Training can be considered a form of contrastive representation learning. 

## Indirectly Inducing Explainability with Adversarial Robustness

Based on our analysis in @sec-lit, counterfactuals $\mathbf{x}^\prime$ can be repurposed as additional training samples [@balashankar2023improving;@luu2023counterfactual] or adversarial examples [@freiesleben2022intriguing;@pawelczyk2022exploring]. This leaves some flexibility with regards to the choice for the $\text{advloss}(\cdot)$ term in @eq-obj. An intuitive functional form, but likely not the only sensible choice, is inspired by adversarial training:
$$
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
$$ {#eq-adv}
Under this choice, we consider nascent counterfactuals $\mathbf{x}_{t\leq T}^\prime$ as AEs as long as the magnitude of the perturbation to any single feature is at most $\varepsilon$. This is closely aligned with @szegedy2013intriguing who define an adversarial attack as an "imperceptible non-random perturbation". Thus, we work with a different distinction between CE and AE than @freiesleben2022intriguing who considers misclassification as the distinguishing feature of adversarial examples. One of the key observations of this work is that we can leverage CEs during training and get AEs essentially for free to reap the aforementioned benefits of adversarial training.

## Encoding Actionability Constraints {#sec-constraints}

Many existing counterfactual explainers support domain and mutability constraints out-of-the-box. In fact, both types of constraints can be implemented for any explainer that relies on gradient descent in the feature space for optimization [@altmeyer2023explaining]. In this context, domain constraints can be imposed by simply projecting counterfactuals back to the specified domain, if the previous gradient step resulted in updated feature values that were out-of-domain. Similarly, mutability constraints can be enforced by setting partial derivatives to zero to ensure that features are only perturbed in the allowed direction, if at all. 

Since actionability constraints are binding at test time, we should also impose them when generating $\mathbf{x}^\prime$ during each training iteration to inform model representations. Through their effect on $\mathbf{x}^\prime$, both types of constraints influence model outcomes via @eq-div. Here it is crucial that we avoid penalizing implausibility that arises due to mutability constraints. For any mutability-constrained feature $d$ this can be achieved by enforcing $\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0$ whenever perturbing $\mathbf{x}^\prime[d]$ in the direction of $\mathbf{x}^+[d]$ would violate mutability constraints. Specifically, we set $\mathbf{x}^+[d] := \mathbf{x}^\prime[d]$ if:

1. Feature $d$ is strictly immutable in practice.
2. $\mathbf{x}^+[d]>\mathbf{x}^\prime[d]$, but $d$ can only be decreased in practice.
3. $\mathbf{x}^+[d]<\mathbf{x}^\prime[d]$, but $d$ can only be increased in practice.

`\noindent`{=latex} From a Bayesian perspective, setting $\mathbf{x}^+[d] := \mathbf{x}^\prime[d]$ can be understood as assuming a point mass prior for $p(\mathbf{x}^+)$ wrt. feature $d$. Intuitively, we think of this as ignoring implausibility costs of immutable features, which effectively forces the model to instead seek plausibility through the remaining features. This can be expected to result in lower overall sensitivity to immutable features, which we investigate empirically in @sec-experiments. Under certain conditions, this result holds theoretically; for the proof, see the supplementary appendix:

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. Assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}
