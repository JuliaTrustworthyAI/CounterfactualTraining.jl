% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Latin Modern Roman}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{placeins}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{corollary}{Research Question}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Counterfactual Training: Teaching Models Plausible and Actionable Explanations},
  pdfauthor={Patrick Altmeyer; Aleksander Buszydlik; Arie van Deursen; Cynthia C. S. Liem},
  pdfkeywords={Counterfactual Training, Counterfactual
Explanations, Algorithmic Recourse, Explainable AI, Representation
Learning},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\usepackage{lineno}
\linenumbers
\newcommand{\runninghead}{A Preprint }
\renewcommand{\runninghead}{Counterfactual Training (A Preprint) }
\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}
\def\asep{\\\\\\ } % default: all authors on same column
\def\asep{\And }
\author{\textbf{Patrick
Altmeyer}~\orcidlink{0000-0003-4726-8613}\\Faculty of Electrical
Engineering, Mathematics and Computer Science\\Delft University of
Technology\\\\\href{mailto:p.altmeyer@tudelft.nl}{p.altmeyer@tudelft.nl}\asep\textbf{Aleksander
Buszydlik}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Arie van
Deursen}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\\asep\textbf{Cynthia C. S.
Liem}\\Faculty of Electrical Engineering, Mathematics and Computer
Science\\Delft University of Technology\\\\}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime called Counterfactual Training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be plausible with
respect to the underlying data and actionable with respect to
stakeholder requirements. Much existing research has therefore focused
on developing post-hoc methods to generate counterfactuals that meet
these desiderata. In this work, we instead hold models directly
accountable for this desired end goal: Counterfactual Training employs
counterfactuals ad-hoc during the training phase to minimize the
divergence between learned representations and plausible, actionable
explanations. We demonstrate empirically and theoretically that our
proposed method facilitates training models that deliver inherently
desirable explanations while maintaining high predictive performance.
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
Counterfactual Training \sep Counterfactual
Explanations \sep Algorithmic Recourse \sep Explainable AI \sep 
Representation Learning



\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by advances in \textbf{representation learning}: instead of
relying on features and rules that are carefully hand-crafted by humans,
modern machine learning (ML) models are tasked with learning the
representations directly from data, guided by narrow objectives such as
predictive accuracy (\citeproc{ref-goodfellow2016deep}{I. Goodfellow,
Bengio, and Courville 2016}). Modern advances in computing have made it
possible to provide such models with ever growing degrees of freedom to
achieve that task, which has often led them to outperform traditionally
more parsimonious models. Unfortunately, in doing so, the models learn
increasingly complex and highly sensitive representations that we can no
longer easily interpret.

The trend towards complexity for the sake of performance has come under
serious scrutiny in recent years. At the very cusp of the deep learning
revolution, Szegedy et al. (\citeproc{ref-szegedy2013intriguing}{2013})
showed that artificial neural networks (ANN) are sensitive to
adversarial examples: counterfactuals of model inputs that yield vastly
different model predictions despite being ``imperceptible'' in that they
are semantically indifferent from their factual counterparts. Although
some partially effective mitigation strategies have been proposed, for
example \textbf{adversarial training}
(\citeproc{ref-goodfellow2014explaining}{I. J. Goodfellow, Shlens, and
Szegedy 2014}), truly robust deep learning (DL) remains unattainable
even for models that are considered shallow by today's standards
(\citeproc{ref-kolter2023keynote}{Kolter 2023}).

Part of the problem is that the high degrees of freedom provide room for
many solutions that are locally optimal with respect to narrow
objectives (\citeproc{ref-wilson2020case}{Wilson 2020}).\footnote{We
  follow the standard ML convention, where ``degrees of freedom'' refer
  to the number of parameters estimated from data.} Indeed, recent work
on the so called ``lottery tickets'' suggests that modern neural
networks can be pruned by up to 90\% while preserving their predictive
performance (\citeproc{ref-frankle2018lottery}{Frankle and Carbin 2019})
and generalizability (\citeproc{ref-morcos2019success}{Morcos et al.
2019}). Similarly, Zhang et al.
(\citeproc{ref-zhang2021understanding}{2021}) showed that
state-of-the-art neural networks are so expressive that they can fit
randomly labeled data. Thus, looking at the predictive performance, the
solutions may seem to provide compelling explanations for the data, when
in fact they are based on purely associative, semantically meaningless
patterns. This poses two related challenges. Firstly, there is no
dependable way to verify if such complex representations correspond to
meaningful and plausible explanations. Secondly, even if we could
resolve the first challenge, it remains undecided how to ensure that
models can \emph{only} learn valuable explanations.

The first challenge has attracted an abundance of research on
\textbf{explainable AI} (XAI) which aims to develop tools to derive
explanations from complex model representations. This can mitigate a
scenario in which we deploy opaque models and blindly rely on their
predictions. On countless occasions, this scenario has occurred in
practice and caused real harm to people who were affected adversely and
often unfairly by automated decision-making (ADM) systems involving
opaque models (\citeproc{ref-oneil2016weapons}{O'Neil 2016};
\citeproc{ref-mcgregor2021preventing}{McGregor 2021}). Effective XAI
tools can aid us in monitoring models and providing recourse to
individuals to turn adverse outcomes (e.g., ``loan application
rejected'') into positive ones (e.g., ``application accepted'').
Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) propose
\textbf{counterfactual explanations} (CE) as an effective approach to
achieve this goal: CEs explain how factual inputs need to change in
order for some fitted model to produce some desired output, typically
involving minimal perturbations.

To our surprise, the second challenge has not yet attracted any major
consolidated research effort. Specifically, there has been no concerted
effort towards improving improving models' explanatory capacity, which
we will henceforth simply call ``explainability'', defined as the degree
to which learned representations correspond to explanations that are
interpretable and deemed \textbf{plausible} by humans (see
Definition~\ref{def-explainability}). Instead, the choice has typically
been to improve the ability of XAI tools to identify the subset
explanations that are both plausible and valid for any given model,
independent of whether the learned representations are also compatible
with implausible explanations
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).
Fortunately, recent findings indicate that explainability can arise as
byproduct of regularization techniques aimed at other objectives such as
robustness, generalization, and generative capacity
(\citeproc{ref-schut2021generating}{Schut et al. 2021};
\citeproc{ref-augustin2020adversarial}{Augustin, Meinke, and Hein 2020};
\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}).

Building on these findings, we introduce \textbf{counterfactual
training}: a novel training regime explicitly geared towards aligning
model representations with plausible explanations. Our contributions are
as follows:

\begin{itemize}
\tightlist
\item
  We discuss existing related work on improving models and consolidate
  it through the lens of counterfactual explanations
  (Section~\ref{sec-lit}).
\item
  We present our proposed methodological framework that leverages
  faithful counterfactual explanations during the training phase of
  models to achieve the explainability objective
  (Section~\ref{sec-method}).
\item
  Through extensive experiments we demonstrate the counterfactual
  training improve model explainability while maintaining high
  predictive performance. We run ablation studies and grid searches to
  understand how the underlying model components and hyperparameters
  affect outcomes. (Section~\ref{sec-experiments}).
\end{itemize}

Despite some limitations of our approach discussed in
Section~\ref{sec-discussion}, we conclude in
Section~\ref{sec-conclusion} that counterfactual training provides a
practical framework for researchers and practitioners interested in
making opaque models more trustworthy. We also believe that this work
serves as an opportunity for XAI researchers to re-evaluate the trend of
improving XAI tools without improving the underlying models.

\section{Related Literature}\label{sec-lit}

To the best of our knowledge, our proposed framework of counterfactual
training represents the first attempt to use counterfactual explanations
during training to improve model explainability. In high-level terms, we
define model explainability as the extent to which valid explanations
derived for an opaque model are also deemed plausible with respect to
the underlying data and stakeholder requirements. To make this more
concrete, we follow Augustin, Meinke, and Hein
(\citeproc{ref-augustin2020adversarial}{2020}) in tying the concept of
explainability to the quality of counterfactual explanations that we can
generate for a given model. The authors show that counterfactual
explanations---understood here as minimal input perturbations that yield
some desired model prediction---are generally more meaningful if the
underlying model is more robust to adversarial examples. We can make
intuitive sense of this finding when looking at adversarial training
(AT) through the lens of representation learning with high degrees of
freedom: by inducing models to ``unlearn'' representations that are
susceptible to worst-case counterfactuals (i.e., adversarial examples),
AT effectively removes some implausible explanations from the solution
space.

\subsection{Adversarial Examples are Counterfactual
Explanations}\label{adversarial-examples-are-counterfactual-explanations}

This interpretation of the link between explainability through
counterfactuals on one side, and robustness to adversarial examples on
the other, is backed by empirical evidence. Sauer and Geiger
(\citeproc{ref-sauer2021counterfactual}{2021}) demonstrate that using
counterfactual images during classifier training improves model
robustness. Similarly, Abbasnejad et al.
(\citeproc{ref-abbasnejad2020counterfactual}{2020}) argue that
counterfactuals represent potentially useful training data in machine
learning, especially in supervised settings where inputs may be
reasonably mapped to multiple outputs. They, too, demonstrate the
augmenting the training data of image classifiers can improve
generalization. Teney, Abbasnedjad, and Hengel
(\citeproc{ref-teney2020learning}{2020}) propose an approach using
counterfactuals in training that does not rely on data augmentation:
they argue that counterfactual pairs typically already exist in training
datasets. Specifically, their approach relies on identifying similar
input samples with different annotations and ensuring that the gradient
of the classifier aligns with the vector between such pairs of
counterfactual inputs using the cosine distance as the loss function.

In the natural language processing (NLP) domain, counterfactuals have
similarly been used to improve models through data augmentation: Wu et
al. (\citeproc{ref-wu2021polyjuice}{2021}), propose \emph{Polyjuice}, a
general-purpose counterfactual generator for language models. They
demonstrate empirically that augmenting training data through
\emph{Polyjuice} counterfactuals improves robustness in a number of NLP
tasks. Balashankar et al.
(\citeproc{ref-balashankar2023improving}{2023}) also use
\emph{Polyjuice} to augment NLP datasets through diverse counterfactuals
and show that classifier robustness improves up to 20\%. Finally, Luu
and Inoue (\citeproc{ref-luu2023counterfactual}{2023}) introduce
Counterfactual Adversarial Training (CAT), which also aims at improving
generalization and robustness of language models. Specifically, they
propose to proceed as follows: firstly, they identify training samples
that are subject to high predictive uncertainty; secondly, they generate
counterfactual explanations for those samples; and, finally, they
fine-tune the given language model on the augmented dataset that
includes the generated counterfactuals.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples (AE).
Pointing to clear similarities in how CE and AE are generated,
Freiesleben (\citeproc{ref-freiesleben2022intriguing}{2022}) makes the
case for jointly studying the opaqueness and robustness problem in
representation learning. Formally, AE can be seen as the subset of CE
for which misclassification is achieved
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022}). Similarly,
Pawelczyk et al. (\citeproc{ref-pawelczyk2022exploring}{2022}) show that
CE and AE are equivalent under certain conditions and derive theoretical
upper bounds on the distances between them.

Two recent works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of post-hoc counterfactual explanations. Firstly,
Ross, Lakkaraju, and Bastani (\citeproc{ref-ross2021learning}{2024})
propose a way to train models that are guaranteed to provide recourse
for individuals to move from an adverse outcome to some positive target
class with high probability. Their approach builds on adversarial
training, where in this context susceptibility to targeted adversarial
examples for the positive class is explicitly induced. The proposed
method allows for imposing a set of actionability constraints ex-ante:
for example, users can specify that certain features (e.g., \emph{age},
\emph{gender}, \ldots) are immutable. Secondly, Guo, Nguyen, and Yadav
(\citeproc{ref-guo2023counternet}{2023}) are the first to propose an
end-to-end training pipeline that includes counterfactual explanations
as part of the training procedure. In particular, they propose a
specific network architecture that includes a predictor and CE generator
network, where the parameters of the CE generator network are learnable.
Counterfactuals are generated during each training iteration and fed
back to the predictor network. In contrast to Guo, Nguyen, and Yadav
(\citeproc{ref-guo2023counternet}{2023}), we impose no restrictions on
the neural network architecture at all.

\subsection{Beyond Robustness}\label{beyond-robustness}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a work
closely related to this one, Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) show that explainability can
be improved through model averaging and refined model objectives. The
authors propose a way to generate counterfactuals that are maximally
\textbf{faithful} to the model in that they are consistent with what the
model has learned about the underlying data. Formally, they rely on
tools from energy-based modelling to minimize the divergence between the
distribution of counterfactuals and the conditional posterior over
inputs learned by the model. Their proposed counterfactual explainer,
\emph{ECCCo}, yields plausible explanations if and only if the
underlying model has learned representations that align with them. They
find that both deep ensembles
(\citeproc{ref-lakshminarayanan2016simple}{Lakshminarayanan, Pritzel,
and Blundell 2017}) and joint energy-based models (JEMs)
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}) tend to do
well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are most called for when
models are underspecified by the available data
(\citeproc{ref-wilson2020case}{Wilson 2020}). Averaging across solutions
mitigates the aforementioned risk of relying on a single locally optimal
representations that corresponds to semantically meaningless
explanations for the data. Previous work by Schut et al.
(\citeproc{ref-schut2021generating}{2021}) similarly found that
generating plausible (``interpretable'') counterfactual explanations is
almost trivial for deep ensembles that have also undergone adversarial
training. The case for JEMs is even clearer: they involve a hybrid
objective that induces both high predictive performance and generative
capacity (\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}). This
is closely related to the idea of aligning models with plausible
explanations and has inspired our proposed counterfactual training
objective, as we explain in Section~\ref{sec-method}.

\section{Counterfactual Training}\label{sec-method}

Counterfactual training combines ideas from adversarial training,
energy-based modelling and counterfactuals explanations with the
explicit objective of aligning representations with plausible
explanations that comply with user requirements. In the context of CEs,
plausibility has broadly been defined as the degree to which
counterfactuals comply with the underlying data generating process
(\citeproc{ref-poyiadzi2020face}{Poyiadzi et al. 2020};
\citeproc{ref-guidotti2022counterfactual}{Guidotti 2022};
\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}). Plausibility
is a necessary but insufficient condition for using CEs to provide
algorithmic recourse (AR) to individuals affected by opaque models in
practice. This is because for recourse recommendations to be
\textbf{actionable}, they need to not only result in plausible
counterfactuals but also be attainable. A plausible CE for a rejected
20-year-old loan applicant, for example, might reveal that their
application would have been accepted, if only they were 20 years older.
Ignoring all other features, this would comply with the definition of
plausibility if 40-year-old individuals were in fact more credit-worthy
on average than young adults. But of course this CE does not qualify for
providing actionable recourse to the applicant since \emph{age} is not a
(directly) mutable feature. For our intents and purposes, counterfactual
training aims to improve model explainability by aligning models with
counterfactuals that meet both desiderata, plausibility and
actionability. Formally, we define explainability as follows:

\begin{definition}[Model
Explainability]\protect\hypertarget{def-explainability}{}\label{def-explainability}

Let \(\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}\) denote a
supervised classification model that maps from the \(D\)-dimensional
input space \(\mathcal{X}\) to representations
\(\phi(\mathbf{x};\theta)\) and finally to the \(K\)-dimensional output
space \(\mathcal{Y}\). Assume that for any given input-output pair
\(\{\mathbf{x},\mathbf{y}\}_i\) there exists a counterfactual
\(\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})\)
where \(\arg\max_y{\mathbf{y}^{+}}=y^+\) and \(y^+\) denotes the index
of the target class.

We say that \(\mathbf{M}_\theta\) is \textbf{explainable} to the extent
that faithfully generated counterfactuals are plausible (i.e.~consistent
with the data) and actionable. Formally, we define these properties as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Plausibility)
  \(\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is some small region around \(\mathbf{x}^{\prime}\).
\item
  (Actionability) Permutations \(\Delta\) are subject to some
  actionability constraints.
\end{enumerate}

We consider counterfactuals as faithful to the extent that they are
consistent with what the model has learned about the input data. Let
\(p_\theta(\mathbf{x}|\mathbf{y}^{+})\) denote the conditional posterior
over inputs, then formally:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  (Faithfulness)
  \(\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1\)
  where \(A\) is defined as above.
\end{enumerate}

\end{definition}

The definitions of faithfulness and plausibility in
Definition~\ref{def-explainability} are the same as in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}), with adapted notation.
Actionability constraints in Definition~\ref{def-explainability} vary
and depend on the context in which \(\mathbf{M}_\theta\) is deployed. In
this work, we focus on domain and mutability constraints for individual
features \(x_d\) for \(d=1,...,D\). We limit ourselves to classification
tasks for reasons discussed in Section~\ref{sec-discussion}.

\subsection{Our Proposed Objective}\label{our-proposed-objective}

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
explanation generated through gradient descent over \(T\) iterations as
initially proposed by Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). For our purposes, we
let \(T\) vary and consider the counterfactual search as converged as
soon as the predicted probability for the target class has reached a
pre-determined threshold, \(\tau\):
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.\footnote{For detailed
  background information on gradient-based counterfactual search and
  convergence see \textbf{?@sec-app-ce}.}

To train models with high explainability as defined in
Definition~\ref{def-explainability}, we propose to leverage
counterfactuals in the following objective:

\begin{equation}\phantomsection\label{eq-obj}{
\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x},\mathbf{x}_T^\prime,y;\theta) + \lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) + \lambda_{\text{reg}}\text{ridge}(\mathbf{x},\mathbf{x}_T^\prime,y;\theta)
}\end{equation}

where \(\text{yloss}(\cdot)\) is any conventional classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms in Equation~\ref{eq-obj} are explained in more
detail below. For now, they can be sufficiently described as inducing
explainability directly and indirectly by penalizing: (1) the
contrastive divergence, \(\text{div}(\cdot)\), between mature
counterfactuals \(\mathbf{x}_T^\prime\) and observed samples \(x\) and,
(2) the adversarial loss, \(\text{advloss}(.)\), with respect to nascent
counterfactuals \(\mathbf{x}_{t\leq T}^\prime\). Finally,
\(\text{ridge}(\cdot)\) denotes a Ridge penalty (\(\ell_2\)-norm) that
regularises the magnitude of the energy terms involved in
\(\text{div}(\cdot)\) (\citeproc{ref-du2019implicit}{Du and Mordatch
2020}). The tradeoff between the different components can be governed by
adjusting the strengths of the penalties \(\lambda_{\text{div}}\),
\(\lambda_{\text{adv}}\) and \(\lambda_{\text{reg}}\).

\subsubsection{Directly Inducing Explainability through Contrastive
Divergence}\label{directly-inducing-explainability-through-contrastive-divergence}

Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) observe that
any classifier can be re-interpreted as a joint energy-based model (JEM)
that learns to discriminate output classes conditional on the observed
(training) samples from \(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). They show that JEMs can be trained to perform
well at both tasks by directly maximizing the joint log-likelihood
factorized as
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\).
The first factor can be optimized using conventional cross-entropy as in
Equation~\ref{eq-obj}. Then, to optimize \(\log p_\theta(\mathbf{x})\)
Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) minimize the
contrastive divergence between these observed samples from
\(p(\mathbf{x})\) and generated samples from \(p_\theta(\mathbf{x})\).

A key empirical finding in Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) was that JEMs tend to do
well with respect to the plausibility objective in
Definition~\ref{def-explainability}. If we consider samples drawn from
\(p_\theta(\mathbf{x})\) as counterfactuals, this is an expected
finding, because the JEM objective effectively minimizes the divergence
between the conditional posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\).
To generate samples, Grathwohl et al.
(\citeproc{ref-grathwohl2020your}{2020}) rely on Stochastic Gradient
Langevin Dynamics (SGLD) using an uninformative prior for
initialization. This is where we depart from their methodology: instead
of SGLD, we propose to use counterfactual explainers to generate
counterfactuals of observed training samples. Specifically, we have:

\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x},\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x},y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation}

where \(\mathcal{E}_\theta(\cdot)\) denotes the energy function. In
particular, we set
\(\mathcal{E}_\theta(\mathbf{x},\mathbf{y})=-\mathbf{M}_\theta(\mathbf{x^+})[y^+]\)
where \(y^+\) denotes the index of the randomly drawn target class,
\(y^+ \sim p(y)\), and \(\mathbf{x^+}\) denotes an observed data point
sampled from target domain: \(\mathbf{X}^+=\{\mathbf{x}:y=y^+\}\).
Conditional on the target class \(y^+\), \(\mathbf{x}_T^\prime\) denotes
a mature counterfactual for a randomly sampled factual from a non-target
class generated through a gradient-based counterfactual generator for at
most \(T\) iterations. We define mature counterfactuals as those that
have either exhausted \(T\) or reached convergence in terms of the
pre-determined decision threshold earlier.

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while at same time
increasing the energy of counterfactuals (negative samples)
(\citeproc{ref-du2019implicit}{Du and Mordatch 2020}). As the generated
counterfactuals get more plausible (Definition~\ref{def-explainability})
over the course of training, these two opposing effects gradually
balance each out (\citeproc{ref-lippe2024uvadlc}{Lippe 2024}).

The departure from SGLD allows us to tap into the vast repertoire of
explainers that have been proposed in the literature to meet different
desiderata. Typically, these methods facilitate the imposition of domain
and mutability constraints, for example. In principle, any existing
approach for generating counterfactual explanations is viable, so long
as it does not violate the faithfulness condition. Like JEMs
(\citeproc{ref-murphy2022probabilistic}{Murphy 2022}), counterfactual
training can be considered as a form of contrastive representation
learning.

\subsubsection{Indirectly Inducing Explainability through Adversarial
Robustness}\label{indirectly-inducing-explainability-through-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
(\citeproc{ref-luu2023counterfactual}{Luu and Inoue 2023};
\citeproc{ref-balashankar2023improving}{Balashankar et al. 2023}) or
adversarial examples
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022};
\citeproc{ref-pawelczyk2022exploring}{Pawelczyk et al. 2022}). This
leaves some flexibility with respect to the exact choice for
\(\text{advloss}(\cdot)\) in Equation~\ref{eq-obj}. An intuitive
functional form to use, though likely not the only reasonable choice, is
inspired by adversarial training:

\begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation}

Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as adversarial examples as long as the
magnitude of the perturbation to any individual feature is at most
\(\varepsilon\). This is closely aligned with Szegedy et al.
(\citeproc{ref-szegedy2013intriguing}{2013}), who define an adversarial
attack as an ``imperceptible non-random perturbation''. Thus, we choose
to work with a different distinction between CE and AE than Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}), who considers
misclassification as the key distinguishing feature of AE. One of the
key observations in this work is that we can leverage counterfactual
explanations during training and get adversarial examples, essentially
for free.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any counterfactual explainer that relies on gradient
descent in the feature space for optimization
(\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen, et al. 2023}).
In this context, domain constraints can be imposed by simply projecting
counterfactuals back to the specified domain, if the previous gradient
step resulted in updated feature values that were out-of-domain.
Mutability constraints can similarly be enforced by setting partial
derivatives to zero to ensure that features are only mutated in the
allowed direction, if at all.

Since actionability constraints are binding at test time, we should also
impose them when generating \(\mathbf{x}^\prime\) during each training
iteration to align model representations with user requirements. Through
their effect on \(\mathbf{x}^\prime\), both types of constraints
influence model outcomes through Equation~\ref{eq-div}. Here it is
crucial that we avoid penalizing implausibility that arises due to
mutability constraints. For any mutability-constrained feature \(d\)
this can be achieved by enforcing
\(\mathbf{x}[d] - \mathbf{x}^\prime[d]:=0\) whenever perturbing
\(\mathbf{x}^\prime[d]\) in the direction of \(\mathbf{x}[d]\) would
violate mutability constraints. Specifically, we set
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  We have \(\mathbf{x}[d]>\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be decreased in practice.
\item
  We have \(\mathbf{x}[d]<\mathbf{x}^\prime[d]\) but feature \(d\) can
  only be increased in practice.
\end{enumerate}

From a Bayesian perspective, setting
\(\mathbf{x}[d] := \mathbf{x}^\prime[d]\) can be understood as assuming
a point mass prior for \(p(\mathbf{x})\) with respect to feature \(d\).
Intuitively, we think of this simply in terms ignoring implausibility
costs with respect to immutable features, which effectively forces the
model to instead seek plausibility with respect to the remaining
features. This in turn results in lower overall sensitivity to immutable
features, which we demonstrate empirically for different classifiers in
Section~\ref{sec-experiments}. Under certain conditions, this results
holds theoretically:\footnote{For the proof, see the supplementary
  appendix.}

\begin{proposition}[Protecting Immutable
Features]\protect\hypertarget{prp-mtblty}{}\label{prp-mtblty}

Let
\(f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})\)
denote a linear classifier with softmax activation \(\mathcal{S}\)
(i.e., \emph{multinomial logistic regression}) where
\(y\in\{1,...,K\}=\mathcal{K}\) and \(\mathbf{x} \in \mathbb{R}^D\). If
we assume multivariate Gaussian class densities with common diagonal
covariance matrix \(\Sigma_k=\Sigma\) for all \(k \in \mathcal{K}\),
then protecting an immutable feature from the contrastive divergence
penalty (Equation~\ref{eq-div}) will result in lower classifier
sensitivity to that feature relative to the remaining features, provided
that at least one of those is discriminative and mutable.

\end{proposition}

It is worth highlighting that Proposition~\ref{prp-mtblty} assumes
independence of features. This raises a valid concern about the effect
of protecting immutable features in the presence of proxy features that
remain unprotected. We discuss this limitation in
Section~\ref{sec-discussion}.

\subsection{Illustration}\label{illustration}

To better convey the intuition underlying our proposed method, we
illustrate different model outcomes in Example~\ref{exm-grad}.

\begin{example}[Prediction of Consumer Credit
Default]\protect\hypertarget{exm-grad}{}\label{exm-grad}

Suppose we are interested in predicting the likelihood that loan
applicants default on their credit. We have access to historical data on
previous loan takers comprised of a binary outcome variable
(\(y\in\{1=\text{default},2=\text{no default}\}\)) two input features:
(1) the subjects' \emph{age}, which we define as immutable, and (2) the
subjects' existing level of \emph{debt}, which we define as mutable.

We have simulated this scenario using synthetic data with independent
features and Gaussian class-conditional densities in
Figure~\ref{fig-poc}. The four panels in Figure~\ref{fig-poc} show the
outcomes for different training procedures using the same model
architecture each time (a linear classifier). In each case, we show the
linear decision boundary (green) and the training data colored according
to their ground-truth label: orange points belong to the target class,
\(y^+=2\), blue points belong to the non-target class, \(y^-=1\). Stars
indicate counterfactuals in the target class generated at test time
using generic gradient descent until convergence.

In panel (a), we have trained our model conventionally, and we do not
impose mutability constraints at test time. The generated
counterfactuals are all valid, but not plausible: they are clearly
distinguishable from the ground-truth data. In panel (b), we have
trained our model with counterfactual training, once again not imposing
mutability constraints at test time. We observe that the counterfactuals
are clearly plausible, therefore meeting the first objective of
Definition~\ref{def-explainability}.

In panel (c), we have used conventional training again, this time
imposing the mutability constraint on \emph{age} at test time.
Counterfactuals are valid but involve some substantial reductions in
\emph{debt} for some individuals (very young applicants). By comparison,
counterfactual paths are shorter on average in panel (d), where we have
used counterfactual training and protected immutable features as
described in Section~\ref{sec-constraints}. In particular, we observe
that due to the classifier's lower sensitivity to \emph{age}, recourse
recommendations with respect to \emph{debt} are much more homogenous, in
that they do not disproportionately punish younger individuals. The
counterfactuals are also plausible with respect to the mutable feature.
Thus, we consider the model in panel (d) as the most explainable
according to Definition~\ref{def-explainability}.

\end{example}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Visual illustration of how counterfactual
training improves explainability. See Example~\ref{exm-grad} for
details.}

\end{figure}%

\section{Experiments}\label{sec-experiments}

In this section, we present experiments that we have conducted in order
to answer the following research questions:

\begin{corollary}[Plausibility]\protect\hypertarget{cor-plaus}{}\label{cor-plaus}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) induce models to learn plausible explanations?

\end{corollary}

\begin{corollary}[Actionability]\protect\hypertarget{cor-action}{}\label{cor-action}

Does our proposed counterfactual training objective
(Equation~\ref{eq-obj}) yield more favorable algorithmic recourse
outcomes in the presence of actionability constraints?

\end{corollary}

Beyond this, we are also interested in understanding how robust our
answers to RQ~\ref{cor-plaus} and RQ~\ref{cor-action} are:

\begin{corollary}[Hyperparameters]\protect\hypertarget{cor-hyper}{}\label{cor-hyper}

What are the effects of different hyperparameter choices with respect to
Equation~\ref{eq-obj}?

\end{corollary}

\subsection{Experimental Setup}\label{experimental-setup}

\subsubsection{Evaluation}\label{evaluation}

Our key outcome of interest is how well models perform with respect to
explainability (Definition~\ref{def-explainability}): to this end, we
focus primarily on the plausibility and cost of faithfully generated
counterfactuals at test time. To measure the cost of counterfactuals, we
follow the standard convention of using distances (\(\ell_1\)-norm)
between factuals and counterfactuals as a proxy. For plausibility, we
assess how similar counterfactuals are to observed samples in the target
domain. We rely on the distance-based metric used by Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}),

\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{implaus}_{\text{dist}}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation}

and introduce a novel divergence metric,

\begin{equation}\phantomsection\label{eq-impl-div}{
\text{implaus}_{\text{div}}(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation}

where \(\mathbf{X}^\prime\) denotes a set of multiple counterfactuals
and \(\text{MMD}(\cdot)\) is an unbiased estimate of the squared
population maximum mean discrepancy
(\citeproc{ref-gretton2012kernel}{Gretton et al. 2012}). The metric in
Equation~\ref{eq-impl-div} is equal to zero iff
\(\mathbf{X}^\prime=\mathbf{X}^+\).

In addition to cost and plausibility, we also compute other standard
metrics to evaluate counterfactuals at test time including validity and
redundancy. Finally, we also assess the predictive performance of models
using standard metrics.

We run the experiments with three CE generators: \emph{Generic} of
Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) as a simple baseline
approach, \emph{REVISE} (\citeproc{ref-joshi2019realistic}{Joshi et al.
2019}) that aims to generate plausible counterfactuals using a surrogate
Variational Autoencoder (VAE), and \emph{ECCo}---the generator of
Altmeyer et al. (\citeproc{ref-altmeyer2023faithful}{2023}) but without
the conformal prediction component---as a method that directly targets
both faithfulness and plausibility of the CEs.

\subsection{Experimental Results}\label{experimental-results}

\subsubsection{Plausibility}\label{plausibility-1}

\subsubsection{Actionability}\label{actionability-1}

\subsubsection{Impact of hyperparameter
settings}\label{sec-hyperparameters}

We extensively test the impact of three types of hyperparameters on the
proposed training regime. Our complete results are available in the
technical appendix; this section focuses on the main findings.

\textbf{\emph{Hyperparameters of the CE generators.}} First, we observe
that CT is highly sensitive to hyperparameter settings but (a) there are
manageable patterns and (b) we can typically identify settings that
improve either plausibility or cost, and commonly both of them at the
same time. Second, we note that the choice of a CE generator has a major
impact on the results. For example, \emph{REVISE} tends to perform the
worst, most likely because it uses a surrogate VAE to generate
counterfactuals which impedes faithfulness
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}). Third,
increasing \(T\), the maximum number of steps, generally yields better
outcomes because more CEs can mature in each training epoch. Fourth, the
impact of \(\tau\), the required decision threshold is more difficult to
predict. On ``harder'' datasets it may be difficult to satisfy high
\(\tau\) for any given sample (i.e., also factuals) and so increasing
this threshold does not seem to correlate with better outcomes. In fact,
we have generally found that a choice of \(\tau=0.5\) leads to optimal
results because it is associated with high proportions of mature
counterfactuals.

\textbf{\emph{Hyperparameters for penalties.}} We find that the strength
of the energy regularization, \(\lambda_{\text{reg}}\) is highly
impactful; energy must be sufficiently regularized to avoid poor
performance in terms of decreased plausibility and increased costs. The
sensitivity with respect to \(\lambda_{\text{div}}\) and
\(\lambda_{\text{adv}}\) is much less evident. While high values of
\(\lambda_{\text{reg}}\) may increase the variability in outcomes when
combined with high values for any of the other penalties in
Equation~\ref{eq-obj}, this effect is not very pronounced.

\textbf{\emph{Other hyperparameters.}} We observe that the effectiveness
and stability of CT is positively associated with the number of
counterfactuals generated during each training epoch. We also confirm
that a higher number of training epochs is beneficial. Interestingly, we
find that it is not necessary to employ CT during the entire training
phase to achieve the desired improvements in explainability. We have
tested training models conventionally during the first half of training
before switching to CT after this initial ``burn-in'' period and
observed positive results. Put differently, CT may be a way to improve
the explainability of trained models in a fine-tuning manner.

\section{Discussion}\label{sec-discussion}

We begin the discussion by addressing the direct extensions of the
counterfactual training approach in Section~\ref{sec-approach}. Then, we
look at its broader limitations and challenges in
Section~\ref{sec-limitations}.

\subsection{Future research}\label{sec-approach}

\textbf{\emph{CT is defined only for classification settings.}} Our
formulation relies on the distinction between non-target class(es)
\(y^{-}\) and target class(es) \(y^{+}\) to generate counterfactuals
through Equation~\ref{eq-obj}. While \(y^{-}\) and \(y^{+}\) can be
arbitrarily defined by the user, CT requires the output space
\(\mathcal{Y}\) to be discrete. Thus, it applies to binary and
multi-class classification but it is not well-defined for other ML tasks
where the change in outcome with respect to a decision threshold
\(\tau\) cannot be readily quantified. In fact, this is a common
restriction in research on CEs and AR that predominantly focuses on
classification models. Although other settings have attracted some
interest (e.g., regression in
\citeproc{ref-spooner2021counterfactual}{Spooner et al. 2021};
\citeproc{ref-zhao2023counterfactual}{Zhao, Broelemann, and Kasneci
2023}), there is still no consensus on what constitutes a counterfactual
in such settings.

\textbf{\emph{CT is subject to training instabilities.}} Joint
energy-based models are susceptible to instabilities during training
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}) and even
though we depart from the SGLD-based sampling, we still encounter major
variability in the outcomes. CT is exposed to two potential sources of
instabilities: (1) the energy-based contrastive divergence term in
Equation~\ref{eq-div}, and (2) the underlying counterfactual explainers.
For example, Altmeyer et al. (\citeproc{ref-altmeyer2023faithful}{2023})
recognize this to be a challenge for \emph{ECCCo} and so it may have
downstream impacts on our proposed method. Still, we find that training
instabilities can be successfully mitigated by regularizing energy
(\(\lambda_{\text{reg}}\)), generating a sufficiently large number of
counterfactuals during each training epoch and including only mature
counterfactuals for contrastive divergence.

\textbf{\emph{CT is sensitive to hyperparameter selection.}} As
discussed in Section~\ref{sec-hyperparameters}, our method benefits from
tuning certain key hyperparameters. In this work, we have relied
exclusively on grid search for this task. Future work on CT could
benefit from investigating more sophisticated approaches towards
hyperparameter tuning. Notably, counterfactual training is iterative
which makes a variety of methods applicable, including Bayesian (e.g.,
\citeproc{ref-snoek2012practical}{Snoek, Larochelle, and Adams 2012}) or
gradient-based (e.g., \citeproc{ref-franceschi2017forward}{Franceschi et
al. 2017}) optimization.

\subsection{Limitations and challenges}\label{sec-limitations}

\textbf{\emph{CT increases the training time of models.}} Counterfactual
training promotes explainability through CEs and robustness through AEs
at the cost of longer training times compared to conventional training
regimes. While higher numbers of iterations and counterfactuals per
iteration positively impact the quality of found solutions, they also
increase the required amount of computations. We find that relatively
small grids with 270 settings can take almost four hours for more
demanding datasets on a high-performance computing cluster with 34 2GB
CPUs (see details in \textbf{?@sec-hardware}). However, there are three
factors that attenuate the impact of this limitation. First, CT provides
counterfactual explanations for the training samples essentially for
free, which may be beneficial in many ADM systems. Second, we find that
CT can retain its value when used as a ``fine-tuning'' training regime
for conventionally-trained models. Third, in principle, CT yields itself
to parallel execution, which we have leveraged for our own experiments.

\textbf{\emph{Immutable features may have proxies.}} In
Proposition~\ref{prp-mtblty} we define an approach to protect immutable
features and thus increase the actionability of the generated
counterfactuals. However, this approach requires that model owners
define the mutability constraints for (all) features considered by the
model. Even with sufficient domain knowledge to protect all immutable
features---ones that cannot be changed at all and ones that cannot be
reasonably expected to change---there may exist proxies that are
theoretically mutable (and hence should not be protected) but preserve
enough information about the principals to counteract the protections.
As one example, consider the Adult dataset used in our experiments where
the mutable education status is a proxy for the immutable age, in that
the attainment of degrees is correlated with age. Delineating
actionability is a major undecided challenge in the AR literature (see,
e.g.,
\citeproc{ref-venkatasubramanian2020philosophical}{Venkatasubramanian
and Alfano 2020}) impacting the capacity of CT to increase the
explainability of the model.

\textbf{\emph{Interventions on features may have downstream impacts on
fairness.}}\\
- \href{https://arxiv.org/abs/2401.16088}{Equality of opportunity}?\\
- \href{https://proceedings.mlr.press/v202/gao23d/gao23d.pdf}{Social
segregation}?\\
- Supporting AR within one context/system may still unfairly target
recourse-affected individuals within other contexts/systems? In Example
3.1 younger individuals gain access to a loan but their age could still
lead to, e.g., enforcing stronger supervisory mechanisms?

\section{Conclusion}\label{sec-conclusion}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abbasnejad2020counterfactual}
Abbasnejad, Ehsan, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van
den Hengel. 2020. {``Counterfactual Vision and Language Learning.''} In
\emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 10041--51.
\url{https://doi.org/10.1109/CVPR42600.2020.01006}.

\bibitem[\citeproctext]{ref-altmeyer2023explaining}
Altmeyer, Patrick, Arie van Deursen, et al. 2023. {``Explaining
Black-Box Models Through Counterfactuals.''} In \emph{Proceedings of the
JuliaCon Conferences}, 1:130. 1.

\bibitem[\citeproctext]{ref-altmeyer2023faithful}
Altmeyer, Patrick, Mojtaba Farmanbar, Arie van Deursen, and Cynthia C.
S. Liem. 2023. {``Faithful Model Explanations Through Energy-Constrained
Conformal Counterfactuals.''} \url{https://arxiv.org/abs/2312.10648}.

\bibitem[\citeproctext]{ref-altmeyer2024faithful}
Altmeyer, Patrick, Mojtaba Farmanbar, Arie van Deursen, and Cynthia CS
Liem. 2024. {``Faithful Model Explanations Through Energy-Constrained
Conformal Counterfactuals.''} In \emph{Proceedings of the AAAI
Conference on Artificial Intelligence}, 38:10829--37. 10.

\bibitem[\citeproctext]{ref-augustin2020adversarial}
Augustin, Maximilian, Alexander Meinke, and Matthias Hein. 2020.
{``Adversarial Robustness on in-and Out-Distribution Improves
Explainability.''} In \emph{European Conference on Computer Vision},
228--45. Springer.

\bibitem[\citeproctext]{ref-balashankar2023improving}
Balashankar, Ananth, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed
Chi, Jilin Chen, and Alex Beutel. 2023. {``Improving Classifier
Robustness Through Active Generative Counterfactual Data
Augmentation.''} In \emph{Findings of the Association for Computational
Linguistics: EMNLP 2023}, 127--39.

\bibitem[\citeproctext]{ref-du2019implicit}
Du, Yilun, and Igor Mordatch. 2020. {``Implicit Generation and
Generalization in Energy-Based Models.''}
\url{https://arxiv.org/abs/1903.08689}.

\bibitem[\citeproctext]{ref-franceschi2017forward}
Franceschi, Luca, Michele Donini, Paolo Frasconi, and Massimiliano
Pontil. 2017. {``{Forward and Reverse Gradient-Based Hyperparameter
Optimization}.''} In \emph{Proceedings of the 34th International
Conference on Machine Learning}, edited by Doina Precup and Yee Whye
Teh, 70:1165--73. Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v70/franceschi17a.html}.

\bibitem[\citeproctext]{ref-frankle2018lottery}
Frankle, Jonathan, and Michael Carbin. 2019. {``The Lottery Ticket
Hypothesis: Finding Sparse, Trainable Neural Networks.''} In
\emph{International Conference on Learning Representations}.
\url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[\citeproctext]{ref-freiesleben2022intriguing}
Freiesleben, Timo. 2022. {``The Intriguing Relation Between
Counterfactual Explanations and Adversarial Examples.''} \emph{Minds and
Machines} 32 (1): 77--109.

\bibitem[\citeproctext]{ref-goodfellow2014explaining}
Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014.
{``Explaining and Harnessing Adversarial Examples.''}
\url{https://arxiv.org/abs/1412.6572}.

\bibitem[\citeproctext]{ref-goodfellow2016deep}
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \emph{Deep
{Learning}}. {MIT Press}.

\bibitem[\citeproctext]{ref-grathwohl2020your}
Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
Mohammad Norouzi, and Kevin Swersky. 2020. {``Your Classifier Is
Secretly an Energy Based Model and You Should Treat It Like One.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-gretton2012kernel}
Gretton, Arthur, Karsten M Borgwardt, Malte J Rasch, Bernhard Schlkopf,
and Alexander Smola. 2012. {``A Kernel Two-Sample Test.''} \emph{The
Journal of Machine Learning Research} 13 (1): 723--73.

\bibitem[\citeproctext]{ref-guidotti2022counterfactual}
Guidotti, Riccardo. 2022. {``Counterfactual Explanations and How to Find
Them: Literature Review and Benchmarking.''} \emph{Data Mining and
Knowledge Discovery}, 1--55.

\bibitem[\citeproctext]{ref-guo2023counternet}
Guo, Hangzhi, Thanh H. Nguyen, and Amulya Yadav. 2023. {``CounterNet:
End-to-End Training of Prediction Aware Counterfactual Explanations.''}
In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining}, 577--89. KDD '23. New York, NY, USA:
Association for Computing Machinery.
\url{https://doi.org/10.1145/3580305.3599290}.

\bibitem[\citeproctext]{ref-joshi2019realistic}
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and
Joydeep Ghosh. 2019. {``Towards Realistic Individual Recourse and
Actionable Explanations in Black-Box Decision Making Systems.''}
\url{https://arxiv.org/abs/1907.09615}.

\bibitem[\citeproctext]{ref-kolter2023keynote}
Kolter, Zico. 2023.{``{Keynote Addresses: SaTML 2023 }.''} In \emph{2023
IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
xvi--. Los Alamitos, CA, USA: IEEE Computer Society.
\url{https://doi.org/10.1109/SaTML54575.2023.00009}.

\bibitem[\citeproctext]{ref-lakshminarayanan2016simple}
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2017.
{``Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.''} \emph{Advances in Neural Information Processing Systems}
30.

\bibitem[\citeproctext]{ref-lippe2024uvadlc}
Lippe, Phillip. 2024. {``{UvA Deep Learning Tutorials}.''}
\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}.

\bibitem[\citeproctext]{ref-luu2023counterfactual}
Luu, Hoai Linh, and Naoya Inoue. 2023. {``Counterfactual Adversarial
Training for Improving Robustness of Pre-Trained Language Models.''} In
\emph{Proceedings of the 37th Pacific Asia Conference on Language,
Information and Computation}, 881--88.

\bibitem[\citeproctext]{ref-mcgregor2021preventing}
McGregor, Sean. 2021. {``{Preventing repeated real world AI failures by
cataloging incidents: The AI incident database}.''} In \emph{Proceedings
of the AAAI Conference on Artificial Intelligence}, 35:15458--63. 17.

\bibitem[\citeproctext]{ref-morcos2019success}
Morcos, Ari S., Haonan Yu, Michela Paganini, and Yuandong Tian. 2019.
{``One Ticket to Win Them All: Generalizing Lottery Ticket
Initializations Across Datasets and Optimizers.''} In \emph{Proceedings
of the 33rd International Conference on Neural Information Processing
Systems}. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-murphy2022probabilistic}
Murphy, Kevin P. 2022. \emph{Probabilistic {Machine Learning}: {An}
Introduction}. {MIT Press}.

\bibitem[\citeproctext]{ref-oneil2016weapons}
O'Neil, Cathy. 2016. \emph{Weapons of Math Destruction: {How} Big Data
Increases Inequality and Threatens Democracy}. {Crown}.

\bibitem[\citeproctext]{ref-pawelczyk2022exploring}
Pawelczyk, Martin, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and
Himabindu Lakkaraju. 2022. {``Exploring Counterfactual Explanations
Through the Lens of Adversarial Examples: A Theoretical and Empirical
Analysis.''} In \emph{Proceedings of the 25th International Conference
on Artificial Intelligence and Statistics}, edited by Gustau
Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, 151:4574--94.
Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v151/pawelczyk22a.html}.

\bibitem[\citeproctext]{ref-poyiadzi2020face}
Poyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and
Peter Flach. 2020. {``{FACE}: {Feasible} and Actionable Counterfactual
Explanations.''} In \emph{Proceedings of the {AAAI}/{ACM Conference} on
{AI}, {Ethics}, and {Society}}, 344--50.

\bibitem[\citeproctext]{ref-ross2021learning}
Ross, Alexis, Himabindu Lakkaraju, and Osbert Bastani. 2024. {``Learning
Models for Actionable Recourse.''} In \emph{Proceedings of the 35th
International Conference on Neural Information Processing Systems}. NIPS
'21. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-sauer2021counterfactual}
Sauer, Axel, and Andreas Geiger. 2021. {``Counterfactual Generative
Networks.''} \url{https://arxiv.org/abs/2101.06046}.

\bibitem[\citeproctext]{ref-schut2021generating}
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan
Sacaleanu, Yarin Gal, et al. 2021. {``Generating {Interpretable
Counterfactual Explanations By Implicit Minimisation} of {Epistemic} and
{Aleatoric Uncertainties}.''} In \emph{International {Conference} on
{Artificial Intelligence} and {Statistics}}, 1756--64. {PMLR}.

\bibitem[\citeproctext]{ref-snoek2012practical}
Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012. {``{Practical
Bayesian Optimization of Machine Learning Algorithms}.''} In
\emph{Advances in Neural Information Processing Systems}, edited by F.
Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran
Associates, Inc.
\url{https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf}.

\bibitem[\citeproctext]{ref-spooner2021counterfactual}
Spooner, Thomas, Danial Dervovic, Jason Long, Jon Shepard, Jiahao Chen,
and Daniele Magazzeni. 2021. {``{Counterfactual Explanations for
Arbitrary Regression Models}.''} \emph{CoRR} abs/2106.15212.
\url{https://arxiv.org/abs/2106.15212}.

\bibitem[\citeproctext]{ref-szegedy2013intriguing}
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. {``Intriguing
Properties of Neural Networks.''} \url{https://arxiv.org/abs/1312.6199}.

\bibitem[\citeproctext]{ref-teney2020learning}
Teney, Damien, Ehsan Abbasnedjad, and Anton van den Hengel. 2020.
{``Learning What Makes a Difference from Counterfactual Examples and
Gradient Supervision.''} In \emph{Computer Vision--ECCV 2020: 16th
European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part
x 16}, 580--99. Springer.

\bibitem[\citeproctext]{ref-venkatasubramanian2020philosophical}
Venkatasubramanian, Suresh, and Mark Alfano. 2020. {``The Philosophical
Basis of Algorithmic Recourse.''} In \emph{Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency}, 284--93. FAT*
'20. New York, NY, USA: Association for Computing Machinery.
\url{https://doi.org/10.1145/3351095.3372876}.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box:
{Automated} Decisions and the {GDPR}.''} \emph{Harv. JL \& Tech.} 31:
841. \url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wilson2020case}
Wilson, Andrew Gordon. 2020. {``The Case for Bayesian Deep Learning.''}
\url{https://arxiv.org/abs/2001.10995}.

\bibitem[\citeproctext]{ref-wu2021polyjuice}
Wu, Tongshuang, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld.
2021. {``Polyjuice: Generating Counterfactuals for Explaining,
Evaluating, and Improving Models.''} In \emph{Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers)}, edited by Chengqing Zong, Fei Xia, Wenjie Li,
and Roberto Navigli, 6707--23. Online: Association for Computational
Linguistics. \url{https://doi.org/10.18653/v1/2021.acl-long.523}.

\bibitem[\citeproctext]{ref-zhang2021understanding}
Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol
Vinyals. 2021. {``Understanding Deep Learning (Still) Requires
Rethinking Generalization.''} \emph{Commun. ACM} 64 (3): 107--15.
\url{https://doi.org/10.1145/3446776}.

\bibitem[\citeproctext]{ref-zhao2023counterfactual}
Zhao, Xuan, Klaus Broelemann, and Gjergji Kasneci. 2023.
{``{Counterfactual Explanation for Regression via Disentanglement in
Latent Space}.''} In \emph{2023 IEEE International Conference on Data
Mining Workshops (ICDMW)}, 976--84. Los Alamitos, CA, USA: IEEE Computer
Society. \url{https://doi.org/10.1109/ICDMW60847.2023.00130}.

\end{CSLReferences}

\newpage{}




\end{document}
