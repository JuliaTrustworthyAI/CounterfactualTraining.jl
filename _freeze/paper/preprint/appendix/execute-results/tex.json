{
  "hash": "06bc68724258cc9042ebed66258c553d",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: Supplementary Appendix\nsubtitle: \"Counterfactual Training: Teaching Models Plausible and Actionable Explanations\"\nabstract: |\n  This is the supplementary appendix to our paper titled *Counterfactual Training: Teaching Models Plausible and Actionable Explanations*. It provides helpful details on mathematical notations and formulas, our proposed training regime, extended empirical findings, hyperparameter tuning and grid searches as well as software and computations.\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  arxiv-pdf:\n    toc: true\n    keep-tex: true  \n    linenumbers: false\n    doublespacing: false\n    runninghead: \"Counterfactual Training\"\n    authorcols: true\n    include-in-header:\n      text: |\n        \\usepackage[title]{appendix}\n        \\usepackage{placeins}\n        \\usepackage{amsthm}\n        \\theoremstyle{plain}\n        \\newtheorem{proposition}{Proposition}[section]\n        \\theoremstyle{definition}\n        \\newtheorem{definition}{Definition}[section]\n        \\theoremstyle{definition}\n        \\newtheorem{example}{Example}[section]\n        \\theoremstyle{plain}\n---\n\n{{< pagebreak >}}\n\n```{=latex}\n\\begin{appendices}\n```\n\n# Notation {.appendix}\n\nBelow we provide an overview of some notation used frequently throughout the paper:\n\n- $y^+$: The target class and also the index of the target class.\n- $y^-$: The non-target class and also the index of non-the target class.\n- $\\mathbf{x}$: a single training sample.\n- $\\mathbf{x}^\\prime$: a counterfactual. \n- $\\mathbf{x}^+$: a training sample in the target class (ground-truth).\n- $\\mathbf{y}^+$: The one-hot encoded output vector for the target class. \n- $\\theta$: Model parameters (unspecified).\n- $\\Theta$: Matrix of parameters. \n- $\\mathbf{M}(\\cdot)$: linear predictions (logits) of the classifier.\n\n## Other Technical Details\n\nMaximum mean discrepancy is defined as follows,\n\n$$\n\\begin{aligned}\n\\text{MMD}({X}^\\prime,\\tilde{X}^\\prime) &= \\frac{1}{m(m-1)}\\sum_{i=1}^m\\sum_{j\\neq i}^m k(x_i,x_j) \\\\ &+ \\frac{1}{n(n-1)}\\sum_{i=1}^n\\sum_{j\\neq i}^n k(\\tilde{x}_i,\\tilde{x}_j) \\\\ &- \\frac{2}{mn}\\sum_{i=1}^m\\sum_{j=1}^n k(x_i,\\tilde{x}_j)\n\\end{aligned}\n$$ {#eq-mmd}\n\nwhere $k(\\cdot,\\cdot)$ is a kernel function [@gretton2012kernel]. We make use of a Gaussian kernel with a constant length-scale parameter of $0.5$. In our implementation, @eq-mmd is by default applied to the entire subset of the training data for which $y=y^+$.\n\n\n\n# Technical Details of Our Approach {.appendix} \n\n## Generating Counterfactuals through Gradient Descent {#sec-app-ce}\n\nIn this section, we provide some background on gradient-based counterfactual generators (@sec-app-ce-background) and discuss how we define convergence in this context (@sec-app-conv).\n\n### Background {#sec-app-ce-background}\n\nGradient-based counterfactual search was originally proposed by @wachter2017counterfactual. It generally solves the following unconstrained objective,\n\n$$\n\\begin{aligned}\n\\min_{\\mathbf{z}^\\prime \\in \\mathcal{Z}^L} \\left\\{  {\\text{yloss}(\\mathbf{M}_{\\theta}(g(\\mathbf{z}^\\prime)),\\mathbf{y}^+)}+ \\lambda {\\text{cost}(g(\\mathbf{z}^\\prime)) }  \\right\\} \n\\end{aligned} \n$$\n\nwhere $g: \\mathcal{Z} \\mapsto \\mathcal{X}$ is an invertible function that maps from the $L$-dimensional counterfactual state space to the feature space and $\\text{cost}(\\cdot)$ denotes one or more penalties that are used to induce certain properties of the counterfactual outcome. As above, $\\mathbf{y}^+$ denotes the target output and $\\mathbf{M}_{\\theta}(\\mathbf{x})$ returns the logit predictions of the underlying classifier for $\\mathbf{x}=g(\\mathbf{z})$.\n\nFor all generators used in this work we use standard logit crossentropy loss for $\\text{yloss}(\\cdot)$. All generators also penalize the distance ($\\ell_1$-norm) of counterfactuals from their original factual state. For *Generic* and *ECCCo*, we have $\\mathcal{Z}:=\\mathcal{X}$ and $g(\\mathbf{z})=g(\\mathbf{z})^{-1}=\\mathbf{z}$, that is counterfactual are searched directly in the feature space. Conversely, *REVISE* traverses the latent space of a variational autoencoder (VAE) fitted to the training data, where $g(\\cdot)$ corresponds to the decoder [@joshi2019realistic]. In addition to the distance penalty, *ECCCo* uses an additional penalty component that regularizes the energy associated with the counterfactual, $\\mathbf{x}^\\prime$ [@altmeyer2024faithful]. \n\n<!-- TODO: Add note on why we omit the conformal prediction component. -->\n\n### Convergence {#sec-app-conv}\n\nAn important consideration when generating counterfactual explanations using gradient-based methods is how to define convergence. Two common choices are to 1) perform gradient descent over a fixed number of iterations $T$, or 2) conclude the search as soon as the predicted probability for the target class has reached a pre-determined threshold, $\\tau$: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$. We prefer the latter for our purposes, because it explicitly defines convergence in terms of the black-box model, $\\mathbf{M}(\\mathbf{x})$.\n\nDefining convergence in this way allows for a more intuitive interpretation of the resulting counterfactual outcomes than with fixed $T$. Specifically, it allows us to think of counterfactuals as explaining 'high-confidence' predictions by the model for the target class $y^+$. Depending on the context and application, different choices of $\\tau$ can be considered as representing 'high-confidence' predictions.\n\n\n\n\n\n\n## Protecting Mutability Constraints with Linear Classifiers {#sec-app-constraints}\n\nIn the main paper, we explain that to avoid penalizing implausibility that arises due to mutability constraints, we impose a point mass prior on $p(\\mathbf{x})$ for the corresponding feature. We argue that this approach induces models to be relatively less sensitive to immutable features, propose a theoretical result supporting this and provide empirical evidence that strengthens our argument (both in the main paper and additional findings in this appendix). Below we derive the analytical results in Prp.~\\ref{prp-mtblty}.\n\n::: {.proof}\n\nLet $d_{\\text{mtbl}}$ and $d_{\\text{immtbl}}$ denote some mutable and immutable feature, respectively. Suppose that $\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}}$ and $\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}}$, where $\\mu_{k,d}$ denotes the conditional sample mean of feature $d$ in class $k$. In words, we assume that the immutable feature tends to take lower values for samples in the non-target class $y^-$ than in the target class $y^+$. We assume the opposite to hold for the mutable feature.\n\nAssuming multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, we have for the log likelihood ratio between any two classes $k,m \\in \\mathcal{K}$ [@hastie2009elements]:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})}=\\mathbf{x}^\\intercal \\Sigma^{-1}(\\mu_{k}-\\mu_{m})  + \\text{const}\n$$ {#eq-loglike}\n\nBy independence of $x_1,...,x_D$, the full log-likelihood ratio decomposes into:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\frac{\\mu_{k,d}-\\mu_{m,d}}{\\sigma_{d}^2} x_{d} + \\text{const}\n$$ {#eq-loglike-decomp}\n\nBy the properties of our classifier (*multinomial logistic regression*), we have:\n\n$$\n\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\left( \\theta_{k,d} - \\theta_{m,d} \\right)x_d + \\text{const}\n$$ {#eq-multi}\n\nwhere $\\theta_{k,d}=\\Theta[k,d]$ denotes the coefficient on feature $d$ for class $k$. \n\nBased on @eq-loglike-decomp and @eq-multi we can identify that $(\\mu_{k,d}-\\mu_{m,d}) \\propto (\\theta_{k,d} - \\theta_{m,d})$ under the assumptions we made above. Hence, we have that $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$ and $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$\n\nLet $\\mathbf{x}^\\prime$ denote some randomly chosen individual from class $y^-$ and let $y^+ \\sim p(y)$ denote the randomly chosen target class. Then the partial derivative of the contrastive divergence penalty with respect to coefficient $\\theta_{y^+,d}$ is equal to \n\n$$\n\\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left(\\text{div}(\\mathbf{x}^+,\\mathbf{x^\\prime},\\mathbf{y};\\theta)\\right) = \\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left( \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^+)[y^+]\\right) - \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^\\prime)[y^+]\\right) \\right) = x_{d}^\\prime - x^+_{d}\n$$ {#eq-grad}\n\nand equal to zero everywhere else.\n\nSince $(\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}})$ we are more likely to have $(x_{d_{\\text{immtbl}}}^\\prime - x^+_{d_{\\text{immtbl}}}) < 0$ than vice versa at initialization. Similarly, we are more likely to have $(x_{d_{\\text{mtbl}}}^\\prime - x^+_{d_{\\text{mtbl}}}) > 0$ since $(\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}})$.\n\nThis implies that if we do not protect feature $d_{\\text{immtbl}}$, the contrastive divergence penalty will decrease $\\theta_{y^-,d_{\\text{immtbl}}}$ thereby exacerbating the existing effect $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$. In words, not protecting the immutable feature would have the undesirable effect of making the classifier more sensitive to this feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for lower values of $d_{\\text{immtbl}}$. \n\nBy the same rationale, the contrastive divergence penalty can generally be expected to increase $\\theta_{y^-,d_{\\text{mtbl}}}$ exacerbating $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$. In words, this has the effect of making the classifier more sensitive to the mutable feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for higher values of $d_{\\text{mtbl}}$.\n\nThus, our proposed approach of protecting feature $d_{\\text{immtbl}}$ has the net affect of decreasing the classifier's sensitivity to the immutable feature relative to the mutable feature (i.e. no change in sensitivity for $d_{\\text{immtbl}}$ relative to increased sensitivity for $d_{\\text{mtbl}}$).\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Domain Constraints\n\nWe apply domain constraints on counterfactuals during training and evaluation. There are at least two good reasons for doing so. Firstly, within the context of explainability and algorithmic recourse, real-world attributes are often domain constrained: the *age* feature, for example, is lower bounded by zero and upper bounded by the maximum human lifespan. Secondly, domain constraints help mitigate training instabilities commonly associated with energy-based modelling [@grathwohl2020your;@altmeyer2024faithful].\n\nFor our image datasets, features are pixel values and hence the domain is constrained by the lower and upper bound of values that pixels can take depending on how they are scaled (in our case $[-1,1]$). For all other features $d$ in our synthetic and tabular datasets, we automatically infer domain constraints $[x_d^{\\text{LB}},x_d^{\\text{UB}}]$  as follows,\n\n$$\n\\begin{aligned}\nx_d^{\\text{LB}} &= \\arg\\min_{x_d} \\{\\mu_d - n_{\\sigma_d}\\sigma_d, \\arg \\min_{x_d} x_d\\} \\\\\nx_d^{\\text{UB}} &= \\arg\\max_{x_d} \\{\\mu_d + n_{\\sigma_d}\\sigma_d, \\arg \\max_{x_d} x_d\\} \n\\end{aligned}\n$$ {#eq-domain}\n\nwhere $\\mu_d$ and $\\sigma_d$ denote the sample mean and standard deviation of feature $d$. We set $n_{\\sigma_d}=3$ across the board but higher values and hence wider bounds may be appropriate depending on the application.\n\n\n\n\n\n\n## Training Hyperparameters {#sec-app-training}\n\n@nte-train-default presents the default hyperparameters used during training. \n\n::: {#nte-train-default .callout-note}\n\n## Training Phase\n\n- Meta Parameters:\n    - Generator: `ecco`\n    - Model: `mlp`\n- Model:\n    - Activation: `relu`\n    - No. Hidden: `32`\n    - No. Layers: `1`\n- Training Parameters:\n    - Burnin: `0.0`\n    - Class Loss: `logitcrossentropy`\n    - Convergence: `threshold`\n    - Generator Parameters:\n        - Decision Threshold: `0.75`\n        - $\\lambda_{\\text{cst}}$: `0.001`\n        - $\\lambda_{\\text{egy}}$: `5.0`\n        - Learning Rate: `0.25`\n        - Maximum Iterations: `30`\n        - Optimizer: `sgd`\n        - Type: `ECCo`\n    - $\\lambda_{\\text{adv}}$: `0.25`\n    - $\\lambda_{\\text{clf}}$: `1.0`\n    - $\\lambda_{\\text{div}}$: `0.5`\n    - $\\lambda_{\\text{reg}}$: `0.1`\n    - Learning Rate: `0.001`\n    - No. Counterfactuals: `1000`\n    - No. Epochs: `100`\n    - Objective: `full`\n    - Optimizer: `adam`\n\n\n\n\n:::\n\n## Evaluation Details {#sec-app-eval}\n\nFor all of our evaluations, we proceed as follows: for each experiment setting we generate multiple counterfactuals (\"No. Counterfactuals\"), randomly choosing the factual and target class each time (@nte-eval-default). We do this across multiple rounds (\"No. Runs\") with different random seeds to account for stochasticity (@nte-eval-default). This is in line with standard practice in the related literature on CE. @nte-eval-default presents the default hyperparameters used during evaluation. For our final results presented in the main paper, we rely on held out test sets to sample factuals (and outputs for our performance metrics). For tuning purposes we rely on training or validation sets. \n\n### Robust Accuracy\n\nTo evaluate robust accuracy (Acc.$^*$), we use the Fast Gradient Sign Method (FGSM) to perturb test samples [@goodfellow2014explaining]. For the main results, we have set the perturbation size to $\\epsilon=0.03$. We have also tested other perturbation sizes, as well as randomly perturbed data. Although not reported here, we have consistently found strong outperformance of CT compared to the weak baseline. \n\n::: {#nte-eval-default .callout-note}\n\n## Evaluation Phase\n\n- Counterfactual Parameters:\n    - Convergence: `threshold`\n    - Decision Threshold: `0.95`\n    - Generator Parameters:\n        - Decision Threshold: `0.75`\n        - $\\lambda_{\\text{cst}}$: `0.001`\n        - $\\lambda_{\\text{egy}}$: `5.0`\n        - Learning Rate: `0.25`\n        - Maximum Iterations: `30`\n        - Optimizer: `sgd`\n        - Type: `ECCo`\n    - Maximum Iterations: `50`\n    - No. Individuals: `100`\n    - No. Runs: `5`\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\\FloatBarrier\n\n# Details on Main Experiments {#sec-app-main .appendix}\n\n## Final Hyperparameters\n\nAs discussed the main paper, CT is sensitive to certain hyperparameter choices. We study the effect of many hyperparameters extensively in @sec-app-grid. For the main results, we tune a small set of key hyperparameters (@sec-app-tune). The final choices for the main results are presented for each data set in @tbl-final-params along with training, test and batch sizes.\n\n::: {#tbl-final-params}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\\begin{tabular}{cccccccc}\n  \\toprule\n  \\textbf{Data} & \\textbf{No. Train} & \\textbf{No. Test} & \\textbf{Batchsize} & \\textbf{Domain} & \\textbf{Decision Threshold} & \\textbf{No. Counterfactuals} & \\textbf{$\\lambda_{\\text{reg}}$} \\\\\\midrule\n  LS & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.01 \\\\\n  Circ & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.5 \\\\\n  Moon & 3600 & 600 & 30 & none & 0.9 & 1000 & 0.25 \\\\\n  OL & 3600 & 600 & 30 & none & 0.5 & 1000 & 0.25 \\\\\\midrule\n  Adult & 26049 & 5010 & 1000 & none & 0.75 & 5000 & 0.25 \\\\\n  CH & 16504 & 3101 & 1000 & none & 0.5 & 5000 & 0.25 \\\\\n  Cred & 10617 & 1923 & 1000 & none & 0.5 & 5000 & 0.25 \\\\\n  GMSC & 13371 & 2474 & 1000 & none & 0.5 & 5000 & 0.5 \\\\\n  MNIST & 11000 & 2000 & 1000 & (-1.0, 1.0) & 0.5 & 5000 & 0.01 \\\\\n\\end{tabular}\n\n\n\n:::\n\nFinal hyperparameters used for the main results presented in the main paper. Any hyperparameter not shown here is set to its default value (@nte-train-default).\n\n:::\n\n## Final Results {#sec-app-final-results}\n\nPlus/minus two standard deviations of bootstrap estimates. \n\n<!-- Standard deviation of bootstrap is the standard error. -->\n\n\n\n\n\n\n\n\n### Robust Performance Plots\n\n\n\n\n### Confidence Intervals\n\n\n\n\n::: {#tbl-ci}\n\n\\begin{tabular}{lccccc}\n  \\toprule\n  \\textbf{Variable} & \\textbf{Data} & \\textbf{CT} & \\textbf{BL} & \\textbf{LB} & \\textbf{UB} \\\\\\midrule\n  Cost & Adult & 2.26 & 2.2 & -0.22 & 0.28 \\\\\n  Cost & CH & 1.46 & 2.46 & -1.1 & -0.89 \\\\\n  Cost & Circ & 0.67 & 1.23 & -0.58 & -0.53 \\\\\n  Cost & Cred & 2.68 & 2.29 & 0.16 & 0.63 \\\\\n  Cost & GMSC & 1.14 & 3.05 & -2.45 & -1.77 \\\\\n  Cost & LS & 3.82 & 4.44 & -0.7 & -0.56 \\\\\n  Cost & MNIST & 77.04 & 68.67 & -3.47 & 18.34 \\\\\n  Cost & Moon & 1.55 & 1.6 & -0.08 & -0.01 \\\\\n  Cost & OL & 1.62 & 2.63 & -1.15 & -0.81 \\\\\n  $ \\text{IP}^* $ & Adult & 0.07 & 0.11 & -0.06 & -0.01 \\\\\n  $ \\text{IP}^* $ & CH & 0.02 & 0.06 & -0.06 & -0.04 \\\\\n  $ \\text{IP}^* $ & Circ & 0.0 & 0.0 & -0.01 & -0.0 \\\\\n  $ \\text{IP}^* $ & Cred & 0.03 & 0.06 & -0.05 & -0.01 \\\\\n  $ \\text{IP}^* $ & GMSC & 0.02 & 0.07 & -0.06 & -0.04 \\\\\n  $ \\text{IP}^* $ & LS & 0.1 & 0.23 & -0.14 & -0.12 \\\\\n  $ \\text{IP}^* $ & MNIST & 0.04 & 0.04 & -0.1 & 0.09 \\\\\n  $ \\text{IP}^* $ & Moon & 0.02 & 0.02 & -0.01 & -0.0 \\\\\n  $ \\text{IP}^* $ & OL & 0.12 & 0.09 & -0.01 & 0.05 \\\\\n  $ \\text{IP} $ & Adult & 15.03 & 15.15 & -0.68 & 0.26 \\\\\n  $ \\text{IP} $ & CH & 6.61 & 7.52 & -1.17 & -0.63 \\\\\n  $ \\text{IP} $ & Circ & 1.03 & 2.36 & -1.37 & -1.29 \\\\\n  $ \\text{IP} $ & Cred & 19.31 & 22.03 & -3.69 & -1.74 \\\\\n  $ \\text{IP} $ & GMSC & 6.19 & 8.09 & -2.4 & -1.49 \\\\\n  $ \\text{IP} $ & LS & 2.41 & 3.4 & -1.04 & -0.94 \\\\\n  $ \\text{IP} $ & MNIST & 258.83 & 278.54 & -30.49 & -7.64 \\\\\n  $ \\text{IP} $ & Moon & 1.36 & 1.71 & -0.38 & -0.32 \\\\\n  $ \\text{IP} $ & OL & 4.49 & 4.44 & -0.03 & 0.13 \\\\\\bottomrule\n\\end{tabular}\n\n\n\nMean outcomes for **CT** and **BL** along with bootstrapped confidence intervals (99%) for difference in mean outcomes grouped by dataset and evaluation metric. Column **LB** and **UB** show the lower and upper bound of the intervals, respectively, and computed using the percentile method. The underlying counterfactual evaluations are the same as the ones used to produce the main table in the paper. \n\n:::\n\n### Qualitative Findings for Image Data\n\n\n\n\n\n\n@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla).\n\n::: {layout=\"[10,-2,10]\" layout-valign=\"top\"}\n![Counterfactual images for *MLP* with counterfactual training. Factual images are shown on the diagonal, with the corresponding counterfactual for each target class (columns) in that same row. The underlying generator, *ECCCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}\n\n![The same setup, factuals, model architecture and generator as in @fig-mnist, but the model was trained conventionally.](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}\n:::\n\n::: {layout-ncol=2}\n\n### Integrated Gradients\n\n:::{#tbl-ig}\n\n\\begin{tabular}{\nl\nS[table-format=2.2(3.2)]\nS[table-format=2.2(2.2)]\n}\n  \\toprule\n  \\textbf{Data} & \\textbf{CT} & \\textbf{BL} \\\\\\midrule\n  LS & 0.03 \\pm 0.0 & 10.24 \\pm 2.4 \\\\\n  Circ & 3.2 \\pm 0.67 & 149.76 \\pm 842.75 \\\\\n  Moon & 60.84 \\pm 128.51 & 0.55 \\pm 0.06 \\\\\n  OL & 0.78 \\pm 0.12 & 4.81 \\pm 1.08 \\\\\\midrule\n  Adult & 0.43 \\pm 0.01 & 1.0 \\pm 0.0 \\\\\n  CH & 0.08 \\pm 0.01 & 0.23 \\pm 0.01 \\\\\n  Cred & 0.0 \\pm 0.0 & 0.43 \\pm 0.01 \\\\\n  GMSC & 1.0 \\pm 0.0 & 0.21 \\pm 0.03 \\\\\n  MNIST & 0.18 \\pm 0.01 & 0.41 \\pm 0.01 \\\\\n\\end{tabular}\n\n\n\nIntegrated gradients.\n\n:::\n\n![Class-conditional integrated gradients.](/paper/figures/mnist_ig.png){#fig-mnist-ig}\n\n:::\n\n### Costs and Validity\n\n::: {#tbl-panel layout-ncol=3}\n\n\n\n\n::: {#tbl-costs}\n\n\\begin{tabular}{\nl\nS[table-format=2.2(1.2)]\n}\n  \\toprule\n  \\textbf{Data} & \\textbf{Cost $(-\\%)$} \\\\\\midrule\n  LS & -26.82\\pm0.86 $^{*}$ \\\\\n  Circ & 40.97\\pm0.82 $^{*}$ \\\\\n  Moon & 33.83\\pm0.98 $^{*}$ \\\\\n  OL & 10.35\\pm1.28 $^{*}$ \\\\\\midrule\n  Adult & 1.16\\pm3.53 $^{}$ \\\\\n  CH & -34.89\\pm2.31 $^{*}$ \\\\\n  Cred & 28.24\\pm1.08 $^{*}$ \\\\\n  GMSC & 3.54\\pm5.78 $^{}$ \\\\\n  MNIST & -31.67\\pm7.72 $^{*}$ \\\\\\midrule\n  Avg. & 2.75 \\\\\\bottomrule\n\\end{tabular}\n\n\n\nCosts\n\n:::\n\n\n\n\n::: {#tbl-val}\n\n\\begin{tabular}{lcc}\n  \\toprule\n  \\textbf{Data} & \\textbf{CT} & \\textbf{BL} \\\\\\midrule\n  LS & 1.0 & 1.0 \\\\\n  Circ & 0.97 & 0.52 \\\\\n  Moon & 1.0 & 1.0 \\\\\n  OL & 0.87 & 0.98 \\\\\\midrule\n  Adult & 0.61 & 0.99 \\\\\n  CH & 0.96 & 1.0 \\\\\n  Cred & 0.7 & 1.0 \\\\\n  GMSC & 0.63 & 1.0 \\\\\n  MNIST & 1.0 & 1.0 \\\\\n\\end{tabular}\n\n\n\nValidity\n\n:::\n\n::: {#tbl-val-mtbl}\n\n\\begin{tabular}{lcc}\n  \\toprule\n  \\textbf{Data} & \\textbf{CT} & \\textbf{BL} \\\\\\midrule\n  LS & 1.0 & 1.0 \\\\\n  Circ & 0.67 & 0.49 \\\\\n  Moon & 0.99 & 0.98 \\\\\n  OL & 0.37 & 0.57 \\\\\\midrule\n  Adult & 0.56 & 0.99 \\\\\n  CH & 0.96 & 1.0 \\\\\n  Cred & 0.67 & 1.0 \\\\\n  GMSC & 0.38 & 1.0 \\\\\n  MNIST & 1.0 & 1.0 \\\\\n\\end{tabular}\n\n\n\nValidity\n\n:::\n\nCosts and validity.\n\n:::\n\n\n\n\\FloatBarrier\n\n# Grid Searches {#sec-app-grid}\n\n\n\n\n\n\n\nTo assess the hyperparameter sensitivity of our proposed training regime we ran multiple large grid searches for all of our synthetic datasets. We have grouped these grid searches into multiple categories: \n\n1. **Generator Parameters** (@sec-app-grid-gen): Investigates the effect of changing hyperparameters that affect the counterfactual outcomes during the training phase.\n2. **Penalty Strengths** (@sec-app-grid-pen): Investigates the effect of changing the penalty strengths in our proposed training objective.\n3. **Other Parameters** (@sec-app-grid-train): Investigates the effect of changing other training parameters, including the total number of generated counterfactuals in each epoch.\n\nWe begin by summarizing the high-level findings in @sec-app-grid-hl. For each of the categories, @sec-app-grid-gen to @sec-app-grid-train then present all details including the exact parameter grids, average predictive performance outcomes and key evaluation metrics for the generated counterfactuals. \n\n## Evaluation Details\n\nTo measure predictive performance, we compute the accuracy and F1-score for all models on test data (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). With respect to explanatory performance, we report here our findings for the (im)plausibility and cost of counterfactuals at test time. Since the computation of our proposed divergence-based adaption ($\\text{IP}^*$) is memory-intensive, we rely on the distance-based metric for the grid searches. For the counterfactual evaluation, we draw factual samples from the training data for the grid searches to avoid data leakage with respect to our final results reported in the body of the paper. Specifically, we want to avoid choosing our default hyperparameters based on results on the test data. Since we are optimizing for explainability, not predictive performance, we still present test accuracy and F1-scores. \n\n### Predictive Performance\n\nWe find that CT is associated with little to no decrease in average predictive performance for our synthetic datasets: test accuracy and F1-scores decrease by at most ~1 percentage point, but generally much less (@tbl-acc-gen, @tbl-acc-pen, @tbl-acc-train). Variation across hyperparameters is negligible as indicated by small standard deviations for these metrics across the board. \n\n### Counterfactual Outcomes {#sec-app-grid-hl}\n\nOverall, we find that counterfactual training achieves it key objectives consistently across all hyperparameter settings and also broadly across datasets: plausibility is improved by up to 60 percent (%) for the *Circles* data (e.g. @fig-grid-gen_params-plaus-circles), 25-30% for the *Moons* data (e.g. @fig-grid-gen_params-plaus-moons) and 10-20% for the *Linearly Separable* data (e.g. @fig-grid-gen_params-plaus-lin_sep). At the same time, the average costs of faithful counterfactuals are reduced in many cases by around 20-25% for  *Circles* (e.g. @fig-grid-gen_params-cost-circles) and up to 50% for *Moons* (e.g. @fig-grid-gen_params-cost-moons). For the *Linearly Separable* data, costs are generally increased although typically by less than 10% (e.g. @fig-grid-gen_params-cost-lin_sep), which reflects a common tradeoff between costs and plausibility [@altmeyer2024faithful]. \n\nWe do observe strong sensitivity to certain hyperparameters, with clear an manageable patterns. Concerning generator parameters, we firstly find that using *REVISE* to generate counterfactuals during training typically yields the worst outcomes out of all generators, often leading to a substantial decrease in plausibility. This finding can be attributed to the fact that *REVISE* effectively assigns the task of learning plausible explanations from the model itself to a surrogate VAE. In other words, counterfactuals generated by *REVISE* are less faithful to the model that *ECCCo* and *Generic*, and hence we would expect them to be a less effective and, in fact, potentially detrimental role in our training regime. Secondly, we observe that allowing for a higher number of maximum steps $T$ for the counterfactual search generally yields better outcomes. This is intuitive, because it allows more counterfactuals to reach maturity in any given iteration. Looking in particular at the results for *Linearly Separable*, it seems that higher values for $T$ in combination with higher decision thresholds ($\\tau$) yields the best results when using *ECCCo*. But depending on the degree of class separability of the underlying data, a high decision-threshold can also affect results adversely, as evident from the results for the *Overlapping* data (@fig-grid-gen_params-plaus-over): here we find that CT generally fails to achieve its objective because only a tiny proportion of counterfactuals ever reaches maturity.\n\nRegarding penalty strengths, we find that the strength of the energy regularization, $\\lambda_{\\text{reg}}$ is a key hyperparameter, while sensitivity with respect to $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$ is much less evident. In particular, we observe that not regularizing energy enough or at all typically leads to poor performance in terms of decreased plausibility and increased costs, in particular for *Circles* (@fig-grid-pen-plaus-circles), *Linearly Separable* (@fig-grid-pen-plaus-lin_sep) and *Overlapping* (@fig-grid-pen-plaus-over). High values of $\\lambda_{\\text{reg}}$ can increase the variability in outcomes, in particular when combined with high values for $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$, but this effect is less pronounced.\n\nFinally, concerning other hyperparameters we observe that the effectiveness and stability of CT is positively associated with the number of counterfactuals generated during each training epoch, in particular for *Circles* (@fig-grid-train-plaus-circles) and *Moons* (@fig-grid-train-plaus-moons). We further find that a higher number of training epochs is beneficial as expected, where we tested training models for 50 and 100 epochs. Interestingly, we find that it is not necessary to employ CT during the entire training phase to achieve the desired improvements in explainability: specifically, we have tested training models conventionally during the first half of training before switching to CT after this initial burn-in period. \n\n## Generator Parameters {#sec-app-grid-gen}\n\n\n\n\nThe hyperparameter grid with varying generator parameters during training is shown in @nte-gen-params-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-gen-params-final-run-eval.\n\n::: {#nte-gen-params-final-run-train .callout-note}\n\n## Training Phase\n\n- Generator Parameters:\n    - Decision Threshold: `0.75, 0.9, 0.95`\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 5.0, 10.0, 20.0`\n    - Maximum Iterations: `5, 25, 50`\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - Objective: `full, vanilla`\n\n\n\n\n:::\n\n::: {#nte-gen-params-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n:::\n\n### Predictive Performance\n\nPredictive performance measures for this grid search are shown in @tbl-acc-gen.\n\n::: {#tbl-acc-gen}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 1.0 & 0.0 \\\\\n  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  Circ & F1-score & Full & 1.0 & 0.0 \\\\\n  Circ & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  LS & Accuracy & Full & 1.0 & 0.0 \\\\\n  LS & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  LS & F1-score & Full & 1.0 & 0.0 \\\\\n  LS & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  Moon & Accuracy & Full & 1.0 & 0.0 \\\\\n  Moon & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  Moon & F1-score & Full & 1.0 & 0.0 \\\\\n  Moon & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  OL & Accuracy & Full & 0.91 & 0.0 \\\\\n  OL & Accuracy & Vanilla & 0.92 & 0.0 \\\\\n  OL & F1-score & Full & 0.91 & 0.0 \\\\\n  OL & F1-score & Vanilla & 0.92 & 0.0 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-gen-params-final-run-train) and evaluation-phase parameters (@nte-gen-params-final-run-eval).\n\n:::\n\n### Plausibility\n\nThe results with respect to the plausibility measure are shown in @fig-grid-gen_params-plaus-circles to @fig-grid-gen_params-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-circles width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-lin_sep width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-moons width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/plausibility_distance_from_target.png){#fig-grid-gen_params-plaus-over width=80%}\n\n\n\n### Cost\n\nThe results with respect to the cost measure are shown in @fig-grid-gen_params-cost-circles to @fig-grid-gen_params-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/gen_params/mlp/circles/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-circles width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/gen_params/mlp/lin_sep/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-lin_sep width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/gen_params/mlp/moons/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-moons width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/gen_params/mlp/over/evaluation/results/ce/decision_threshold_exper---lambda_energy_exper---maxiter_exper---maxiter---decision_threshold_exper/distance.png){#fig-grid-gen_params-cost-over width=80%}\n\n\n\n## Penalty Strengths {#sec-app-grid-pen}\n\n\n\n\nThe hyperparameter grid with varying penalty strengths during training is shown in @nte-pen-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-pen-final-run-eval.\n\n::: {#nte-pen-final-run-train .callout-note}\n\n## Training Phase\n\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{adv}}$: `0.1, 0.25, 1.0`\n    - $\\lambda_{\\text{div}}$: `0.01, 0.1, 1.0`\n    - $\\lambda_{\\text{reg}}$: `0.0, 0.01, 0.1, 0.25, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n:::\n\n::: {#nte-pen-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n:::\n\n### Predictive Performance\n\nPredictive performance measures for this grid search are shown in @tbl-acc-pen.\n\n::: {#tbl-acc-pen}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 0.99 & 0.01 \\\\\n  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  Circ & F1-score & Full & 0.99 & 0.01 \\\\\n  Circ & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  LS & Accuracy & Full & 1.0 & 0.01 \\\\\n  LS & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  LS & F1-score & Full & 1.0 & 0.01 \\\\\n  LS & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  Moon & Accuracy & Full & 0.99 & 0.04 \\\\\n  Moon & Accuracy & Vanilla & 1.0 & 0.01 \\\\\n  Moon & F1-score & Full & 0.99 & 0.04 \\\\\n  Moon & F1-score & Vanilla & 1.0 & 0.01 \\\\\n  OL & Accuracy & Full & 0.91 & 0.02 \\\\\n  OL & Accuracy & Vanilla & 0.92 & 0.0 \\\\\n  OL & F1-score & Full & 0.91 & 0.02 \\\\\n  OL & F1-score & Vanilla & 0.92 & 0.0 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-pen-final-run-train) and evaluation-phase parameters (@nte-pen-final-run-eval).\n\n:::\n\n### Plausibility\n\nThe results with respect to the plausibility measure are shown in @fig-grid-pen-plaus-circles to @fig-grid-pen-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-circles width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-lin_sep width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-moons width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/plausibility_distance_from_target.png){#fig-grid-pen-plaus-over width=80%}\n\n\n\n### Cost\n\nThe results with respect to the cost measure are shown in @fig-grid-pen-cost-circles to @fig-grid-pen-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/penalties/mlp/circles/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-circles width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/penalties/mlp/lin_sep/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-lin_sep width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/penalties/mlp/moons/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-moons width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/penalties/mlp/over/evaluation/results/ce/lambda_adversarial---lambda_energy_reg---lambda_energy_diff---lambda_adversarial---lambda_adversarial/distance.png){#fig-grid-pen-cost-over width=80%}\n\n\n\n## Other Parameters {#sec-app-grid-train}\n\n\n\n\nThe hyperparameter grid with other varying training parameters is shown in @nte-train-final-run-train. The corresponding evaluation grid used for these experiments is shown in @nte-train-final-run-eval.\n\n::: {#nte-train-final-run-train .callout-note}\n\n## Training Phase\n\n- Generator: `ecco, generic, revise`\n- Model: `mlp`\n- Training Parameters:\n    - Burnin: `0.0, 0.5`\n    - No. Counterfactuals: `100, 1000`\n    - No. Epochs: `50, 100`\n    - Objective: `full, vanilla`\n\n\n\n\n:::\n\n::: {#nte-train-final-run-eval .callout-note}\n\n## Evaluation Phase\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n:::\n\n### Predictive Performance\n\nPredictive performance measures for this grid search are shown in @tbl-acc-train.\n\n::: {#tbl-acc-train}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Dataset} & \\textbf{Variable} & \\textbf{Objective} & \\textbf{Mean} & \\textbf{Se} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  Circ & Accuracy & Full & 0.99 & 0.0 \\\\\n  Circ & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  Circ & F1-score & Full & 0.99 & 0.0 \\\\\n  Circ & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  LS & Accuracy & Full & 1.0 & 0.0 \\\\\n  LS & Accuracy & Vanilla & 1.0 & 0.0 \\\\\n  LS & F1-score & Full & 1.0 & 0.0 \\\\\n  LS & F1-score & Vanilla & 1.0 & 0.0 \\\\\n  Moon & Accuracy & Full & 1.0 & 0.01 \\\\\n  Moon & Accuracy & Vanilla & 0.99 & 0.02 \\\\\n  Moon & F1-score & Full & 1.0 & 0.01 \\\\\n  Moon & F1-score & Vanilla & 0.99 & 0.02 \\\\\n  OL & Accuracy & Full & 0.91 & 0.01 \\\\\n  OL & Accuracy & Vanilla & 0.92 & 0.0 \\\\\n  OL & F1-score & Full & 0.91 & 0.01 \\\\\n  OL & F1-score & Vanilla & 0.92 & 0.0 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n:::\n\nPredictive performance measures by dataset and objective averaged across training-phase parameters (@nte-train-final-run-train) and evaluation-phase parameters (@nte-train-final-run-eval).\n\n:::\n\n### Plausibility\n\nThe results with respect to the plausibility measure are shown in @fig-grid-train-plaus-circles to @fig-grid-train-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-circles width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-lin_sep width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-moons width=80%}\n\n![Average outcomes for the plausibility measure across hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/plausibility_distance_from_target.png){#fig-grid-train-plaus-over width=80%}\n\n\n\n### Cost\n\nThe results with respect to the cost measure are shown in @fig-grid-train-cost-circles to @fig-grid-train-cost-over.\n\n\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/training_params/mlp/circles/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-circles width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/training_params/mlp/lin_sep/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-lin_sep width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/training_params/mlp/moons/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-moons width=80%}\n\n![Average outcomes for the cost measure across hyperparameters. This shows the % change from the baseline model for the distance-based cost metric [@wachter2017counterfactual]. Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/training_params/mlp/over/evaluation/results/ce/burnin---nce---nepochs---burnin---burnin/distance.png){#fig-grid-train-cost-over width=80%}\n\n\n\n\n\n\\FloatBarrier\n\n# Tuning Key Parameters {#sec-app-tune}\n\n\n\n\n\n\n\nBased on the findings from our initial large grid searches (@sec-app-grid), we tune selected hyperparameters for all datasets: namely, the decision threshold $\\tau$ and the strength of the energy regularization $\\lambda_{\\text{reg}}$. The final hyperparameter choices for each dataset are presented in @tbl-final-params in @sec-app-main. Detailed results for each data set are shown in @fig-tune-plaus-adult to @fig-tune-mat-over. From @tbl-final-params, we notice that the same decision threshold of $\\tau=0.5$ is optimal for all but on dataset. We attribute this to the fact that a low decision threshold results in a higher share of mature counterfactuals and hence more opportunities for the model to learn from examples (@fig-tune-mat-adult to @fig-tune-mat-over). This has played a role in particular for our real-world tabular datasets and MNIST, which suffered from low levels of maturity for higher decision thresholds. In cases where maturity is not an issue, as for *Moons*, higher decision thresholds lead to better outcomes, which may have to do with the fact that the resulting counterfactuals are more faithful to the model. Concerning the regularization strength, we find somewhat high variation across datasets. Most notably, we find that relatively low levels of regularization are optimal for MNIST. We hypothesize that this finding may be attributed to the uniform scaling of all input features (digits). \n\nFinally, to increase the proportion of mature counterfactuals for some datasets, we have also investigated the effect on the learning rate $\\eta$ for the counterfactual search and even smaller regularization strengths for a fixed decision threshold of 0.5 (@fig-tune_lr-plaus-adult to @fig-tune_lr-plaus-over). For the given low decision threshold, we find that the learning rate has no discernable impact on the proportion of mature counterfactuals (@fig-tune_lr-mat-adult to @fig-tune_lr-mat-over). We do notice, however, that the results for MNIST are much improved when using a low value $\\lambda_{\\text{reg}}$, the strength for the engery regularization: plausibility is increased by up to ~10% (@fig-tune_lr-plaus-mnist) and the proportion of mature counterfactuals reaches 100%. \n\nOne consideration worth exploring is to combine high decision thresholds with high learning rates, which we have not investigated here. \n\n<!-- ::: {.callout-warning}\n\n## Package Version (Reproducibility)\n\nTuning was run using `v1.1.3` of `TaijaData`. The follow-up version `v1.1.4` introduced an option to split real-world tabular datasets into train and test set, ensuring that pre-processing steps like standardization is fit on the training set only. If you are rerunning the tuning experiments with a version of `TaijaData` that is higher than `v1.1.3`, than for the default parameters specified in the configuration files, you may end up with slightly different results, although we would not expect any changes in terms of qualitative findings. For exact reproducibility, please use `v1.1.3`.\n\n::: -->\n\n## Key Parameters {#sec-app-tune-key}\n\n\n\n\nThe hyperparameter grid for tuning key parameters is shown in @nte-tune-train. The corresponding evaluation grid used for these experiments is shown in @nte-tune-eval.\n\n::: {#nte-tune-train .callout-note}\n\n## Training Phase\n\n- Generator Parameters:\n    - Decision Threshold: `0.5, 0.75, 0.9`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{reg}}$: `0.1, 0.25, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n:::\n\n::: {#nte-tune-eval .callout-note}\n\n## Evaluation Phase\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n:::\n\n### Plausibility\n\nThe results with respect to the plausibility measure are shown in @fig-tune-plaus-adult to @fig-tune-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Adult.](/paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-adult width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: California Housing.](/paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-cali width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-circles width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Credit.](/paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-credit width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: GMSC.](/paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-gmsc width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-lin_sep width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: MNIST.](/paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-mnist width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-moons width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/tune/mlp/over/evaluation/results/ce/lambda_energy_reg---decision_threshold_exper---lambda_energy_eval---lambda_energy_reg---decision_threshold_exper/plausibility_distance_from_target.png){#fig-tune-plaus-over width=100%}\n\n\n\n### Proportion of Mature CE\n\nThe results with respect to the proportion of mature counterfactuals in each epoch are shown in @fig-tune-mat-adult to @fig-tune-mat-over.\n\n\n\n![Proportion of mature counterfactuals in each epoch. Data: Adult.](/paper/experiments/output/final_run/tune/mlp/adult/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-adult width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: California Housing.](/paper/experiments/output/final_run/tune/mlp/cali/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-cali width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Circles.](/paper/experiments/output/final_run/tune/mlp/circles/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-circles width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Credit.](/paper/experiments/output/final_run/tune/mlp/credit/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-credit width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: GMSC.](/paper/experiments/output/final_run/tune/mlp/gmsc/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-gmsc width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Linearly Separable.](/paper/experiments/output/final_run/tune/mlp/lin_sep/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-lin_sep width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: MNIST.](/paper/experiments/output/final_run/tune/mlp/mnist/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-mnist width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Moons.](/paper/experiments/output/final_run/tune/mlp/moons/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-moons width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Overlapping.](/paper/experiments/output/final_run/tune/mlp/over/evaluation/results/logs/objective---decision_threshold---lambda_energy_reg/percent_valid.png){#fig-tune-mat-over width=100%}\n\n\n\n## Learning Rate {#sec-app-tune-lr}\n\n\n\n\nThe hyperparameter grid for tuning the learning rate is shown in @nte-tune_lr-train. The corresponding evaluation grid used for these experiments is shown in @nte-tune_lr-eval.\n\n::: {#nte-tune_lr-train .callout-note}\n\n## Training Phase\n\n- Generator Parameters:\n    - Learning Rate: `0.1, 0.5, 1.0`\n- Model: `mlp`\n- Training Parameters:\n    - $\\lambda_{\\text{reg}}$: `0.01, 0.1, 0.5`\n    - Objective: `full, vanilla`\n\n\n\n\n:::\n\n::: {#nte-tune_lr-eval .callout-note}\n\n## Evaluation Phase\n\n- Generator Parameters:\n    - $\\lambda_{\\text{egy}}$: `0.1, 0.5, 1.0, 5.0, 10.0`\n\n\n\n\n:::\n\n### Plausibility\n\nThe results with respect to the plausibility measure are shown in @fig-tune_lr-plaus-adult to @fig-tune_lr-plaus-over.\n\n\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Adult.](/paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-adult width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: California Housing.](/paper/experiments/output/final_run/tune_lr/mlp/cali/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-cali width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Circles.](/paper/experiments/output/final_run/tune_lr/mlp/circles/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-circles width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Credit.](/paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-credit width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: GMSC.](/paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-gmsc width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Linearly Separable.](/paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-lin_sep width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: MNIST.](/paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-mnist width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Moons.](/paper/experiments/output/final_run/tune_lr/mlp/moons/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-moons width=100%}\n\n![Average outcomes for the plausibility measure across key hyperparameters. This shows the % change from the baseline model for the distance-based implausibility metric ($\text{IP}$). Boxplots indicate the variation across evaluation runs and test settings (varying parameters for *ECCCo*). Data: Overlapping.](/paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/ce/lambda_energy_reg---lr_exper---lambda_energy_eval---lambda_energy_reg---lr_exper/plausibility_distance_from_target.png){#fig-tune_lr-plaus-over width=100%}\n\n\n\n### Proportion of Mature CE\n\nThe results with respect to the proportion of mature counterfactuals in each epoch are shown in @fig-tune_lr-mat-adult to @fig-tune_lr-mat-over.\n\n\n\n![Proportion of mature counterfactuals in each epoch. Data: Adult.](/paper/experiments/output/final_run/tune_lr/mlp/adult/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-adult width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: California Housing.](/paper/experiments/output/final_run/tune_lr/mlp/cali/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-cali width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Circles.](/paper/experiments/output/final_run/tune_lr/mlp/circles/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-circles width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Credit.](/paper/experiments/output/final_run/tune_lr/mlp/credit/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-credit width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: GMSC.](/paper/experiments/output/final_run/tune_lr/mlp/gmsc/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-gmsc width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Linearly Separable.](/paper/experiments/output/final_run/tune_lr/mlp/lin_sep/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-lin_sep width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: MNIST.](/paper/experiments/output/final_run/tune_lr/mlp/mnist/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-mnist width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Moons.](/paper/experiments/output/final_run/tune_lr/mlp/moons/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-moons width=100%}\n\n![Proportion of mature counterfactuals in each epoch. Data: Overlapping.](/paper/experiments/output/final_run/tune_lr/mlp/over/evaluation/results/logs/objective---lr---lambda_energy_reg/percent_valid.png){#fig-tune_lr-mat-over width=100%}\n\n\n\n\n\n\\FloatBarrier\n\n# Computation Details {.appendix}\n\n<!-- [@DHPC2022] -->\n## Hardware {#sec-app-hardware}\n\nWe performed our experiments on a high-performance cluster. Details about the cluster will be disclosed upon publication to avoid revealing information that might interfere with the double-blind review process. Since our experiments involve highly parallel tasks and rather small models by today's standard, we have relied on distributed computing across multiple central processing units (CPU). Graphical processing units (GPU) were not required. \n\n### Grid Searches\n\nModel training for the largest grid searches with 270 unique parameter combinations was parallelized across 34 CPUs with 2GB memory each. The time to completion varied by dataset: 0h49m (*Moons*), 1h4m (*Linearly Separable*), 1h49m (*Circles*), 3h52m (*Overlapping*). Model evaluations for large grid searches were parallelized across 20 CPUs with 3GB memory each. Evaluations for all data sets took less than one hour (<1h) to complete. \n\n### Tuning\n\nFor tuning of selected hyperparameters, we distributed the task of generating counterfactuals during training across 40 CPUs with 2GB memory each for all tabular datasets. Except for the *Adult* dataset, all training runs were completed in less that half an hour (<0h30m). The *Adult* dataset took around 0h35m to complete. Evaluations across 20 CPUs with 3GB memory each generally took less than 0h30m to complete. For *MNIST*, we relied on 100 CPUs with 2GB memory each. For the *MLP*, training of all models could be completed in 1h30m, while the evaluation across 20 CPUs (6GB memory) took 4h12m. For the *CNN*, training of all models took ~8h, with conventionally trained models taking ~0h15m each and model with CT taking ~0h30m-0h45m each.\n\n## Software\n\nAll computations were performed in the Julia Programming Language [@bezanson2017julia]. We have developed a package for counterfactual training that leverages and extends the functionality provided by several existing packages, most notably [CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) [@altmeyer2023explaining] and the [Flux.jl](https://fluxml.ai/Flux.jl/v0.16/) library for deep learning [@innes2018fashionable;@innes2018flux]. For data-wrangling and presentation-ready tables we relied on [DataFrames.jl](https://dataframes.juliadata.org/v1.7/) [@milan2023dataframes] and [PrettyTables.jl](https://ronisbr.github.io/PrettyTables.jl/v2.4/) [@chagas2024pretty], respectively. For plots and visualizations we used both [Plots.jl](https://docs.juliaplots.org/v1.40/) [@PlotsJL] and [Makie.jl](https://docs.makie.org/v0.22/) [@danisch2021makie], in particular [AlgebraOfGraphics.jl](https://aog.makie.org/v0.9.3/). To distribute computational tasks across multiple processors, we have relied on [MPI.jl](https://juliaparallel.org/MPI.jl/v0.20/) [@byrne2021mpi].\n\n\n\n\n\n```{=latex}\n\\end{appendices}\n```\n\n# References {-}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "appendix_files"
    ],
    "filters": []
  }
}