{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "projectdir = splitpath(pwd()) |>\n",
        "    ss -> joinpath(ss[1:findall([s == \"CounterfactualTraining.jl\" for s in ss])[1]]...) \n",
        "cd(projectdir)"
      ],
      "id": "1be91580",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using CTExperiments\n",
        "using CTExperiments.CSV\n",
        "using CTExperiments.DataFrames\n",
        "using CTExperiments.StatsBase\n",
        "\n",
        "using DotEnv\n",
        "DotEnv.load!()"
      ],
      "id": "a055ab98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\FloatBarrier\n",
        "\n",
        "\\setcounter{section}{0}\n",
        "\\renewcommand{\\thesection}{\\Alph{section}}\n",
        "\n",
        "\\setcounter{table}{0}\n",
        "\\renewcommand{\\thetable}{A\\arabic{table}}\n",
        "\n",
        "\\setcounter{figure}{0}\n",
        "\\renewcommand{\\thefigure}{A\\arabic{figure}}\n",
        "\n",
        "<!-- # Supplementary Material {.appendix} -->\n",
        "\n",
        "# Training Details {.appendix} \n",
        "\n",
        "## Initial Grid Search\n"
      ],
      "id": "7fb39650"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results_dir = ENV[\"INITIAL_RUN_RESULTS\"]\n",
        "config_dir = ENV[\"INITIAL_RUN_CONFIG\"]"
      ],
      "id": "a299e195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the initial round of experiments we \n",
        "\n",
        "### Generator Parameters\n",
        "\n",
        "The hyperparameter grids for the first investigation of the effect of generator parameters are shown in @exr-gen-params-first-run-train and @exr-gen-params-first-run-eval.\n",
        "\n",
        "::: {#exr-gen-params-first-run-train}\n",
        "\n",
        "## Training Phase\n"
      ],
      "id": "1ee225ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(config_dir,\"gen_params.toml\")) \n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "8460ed1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "::: {#exr-gen-params-first-run-eval}\n",
        "\n",
        "## Evaluation Phase\n"
      ],
      "id": "718ba72a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "dict = CTExperiments.from_toml(joinpath(results_dir, \"gen_params/mlp/lin_sep/evaluation/evaluation_grid_config.toml\"))\n",
        "println(CTExperiments.dict_to_quarto_markdown(dict))"
      ],
      "id": "fb919559",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n"
      ],
      "id": "0771663a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gen_params_dir = joinpath(results_dir, \"gen_params/mlp\")"
      ],
      "id": "e4471d2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Linearly Separable \n",
        "\n",
        "- **Energy Penalty** (@tbl-lin_sep-lambda_energy_exper): *ECCo* generally does yield better results than *Vanilla* for higher choices of the energy penalty (10,15) during training. *Generic* performs poorly accross the board. *Omni* seems to have an anchoring effect, in that it never performs terribly but also never as good as the best *ECCo* results. *REVISE* performs poorly across the board.\n",
        "- **Cost** (@tbl-lin_sep-lambda_cost_exper): Results for all generators (except *Omni*) are quite bad, which can likely be attributed to extremely bad results for some choices of the **Energy Penalty** (results here are averaged). For *ECCo* and *Generic*, higher cost values generally lead to worse results.\n",
        "- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n",
        "- **Validity**: *ECCo* almost always valid except for very low values during training and high values at evaluation time. *Generic* often has poor validity.\n",
        "- **Accuracy**: Seems largely unaffected.\n"
      ],
      "id": "5bf2f88f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = CSV.read(\"$(gen_params_dir)/lin_sep/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv\", DataFrame)\n",
        "df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> \n",
        "  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>\n",
        "  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type])"
      ],
      "id": "a52adf18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#tbl-lin_sep-lambda_energy_exper}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}\n"
      ],
      "id": "1baa96b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "get_table_inputs(df, \"value\"; alpha=0.9, byvars=\"lambda_energy_exper\", backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs; wrap_table=false)"
      ],
      "id": "50a87d66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Results for Linearly Separable data by energy penalty.\n",
        "\n",
        ":::\n",
        "\n",
        "<!-- Cost -->\n"
      ],
      "id": "26b2b00a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = CSV.read(\"$(gen_params_dir)/lin_sep/evaluation/results/ce/objective---lambda_cost_exper---lambda_energy_eval/plausibility_distance_from_target.csv\", DataFrame)\n",
        "df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> \n",
        "  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>\n",
        "  df -> sort(df, [:lambda_cost_exper, :objective, :generator_type])"
      ],
      "id": "db29315b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#tbl-lin_sep-lambda_cost_exper}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}\n"
      ],
      "id": "be3fd033"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "get_table_inputs(df, \"value\"; alpha=0.9, byvars=\"lambda_cost_exper\", backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs; wrap_table=false)"
      ],
      "id": "3b72ab30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Results for Linearly Separable data by cost penalty.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "#### Moons\n",
        "\n",
        "- **Energy Penalty** (@tbl-moons-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, except for very low choices of the energy penalty during training for which it performs abismal. *Generic* performs quite badly across the board for high enough choices of the energy penalty at evaluation time. *Omni* has small positive effect. *REVISE* performs poorly across the board.\n",
        "- **Cost (distance penalty)**: *Generic* generally does better for higher values, while *ECCo* does better for lower values.\n",
        "- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n",
        "- **Validity**: *ECCo* generally achieves full validity except for very low choices the energy penalty during training and high choices at evaluation time. *Generic* performs poorly for high choices of the energy penalty during evaluation.\n",
        "- **Accuracy**: Largely unaffected although *ECCo* suffers a bit for very low choices the energy penalty during training. *REVISE* suffers a lot in general (around 10 percentage points).\n"
      ],
      "id": "0ede4627"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = CSV.read(\"$(gen_params_dir)/moons/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv\", DataFrame)\n",
        "df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> \n",
        "  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>\n",
        "  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type]) "
      ],
      "id": "0e08002a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#tbl-moons-lambda_energy_exper}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}\n"
      ],
      "id": "d3b38810"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "get_table_inputs(df, \"value\"; alpha=0.9, byvars=\"lambda_energy_exper\", backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs)"
      ],
      "id": "19ddf1d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Results for Moons data by energy penalty.\n",
        "\n",
        ":::\n",
        "\n",
        "#### Circles\n",
        "\n",
        "- **Energy Penalty** (@tbl-circles-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, though primarily for low to medium choices of the energy penalty (<=5) during training. The same goes for *Generic*, which sometimes outperforms *ECCo* (for small energy penalty at evaluation time). *Omni* does alright for lower energy penalty at evaluation time, but loses out for higher choices. *REVISE* performs poorly across the board (except very low choices at evaluation time).\n",
        "- **Cost (distance penalty)**: *ECCo* and *Generic* generally achieve the best results when no cost penalty is used during training. Both *Omni* and *REVISE* are largely unaffected.\n",
        "- **Maximum Iterations**: *ECCo* consistently yields better results for higher numbers of iterations. *Generic* generally does best for a medium number (50). *Omni* is sometimes invalid (**???**).\n",
        "- **Validity**: *ECCo* tends to outperform its *Vanilla* counterpart, though primarily for low to medium choices of the energy penalty (<=5) during training and evaluation. *Vanilla* typically worse across the board.\n",
        "- **Accuracy**: Mostly unaffected, but *REVISE* again consistently some deterioration and *ECCo* deteriorates for high choices of energy penalty during training, reflecting other outcomes above.\n"
      ],
      "id": "44b8397b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = CSV.read(\"$(gen_params_dir)/circles/evaluation/results/ce/objective---lambda_energy_exper---lambda_energy_eval/plausibility_distance_from_target.csv\", DataFrame)\n",
        "df = groupby(df, Not(:run, :std, :mean, :lambda_energy_eval)) |> \n",
        "  gdf -> combine(gdf, :mean => (x -> [(mean(x),std(x))]) => [:value,:std]) |>\n",
        "  df -> sort(df, [:lambda_energy_exper, :objective, :generator_type]) "
      ],
      "id": "78f68b9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#tbl-circles-lambda_energy_exper}\n",
        "\n",
        "::: {.content-hidden unless-format=\"pdf\"}\n"
      ],
      "id": "427b1443"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "get_table_inputs(df, \"value\"; alpha=0.9, byvars=\"lambda_energy_exper\", backend=Val(:latex)) |>\n",
        "    inputs -> tabulate_results(inputs)"
      ],
      "id": "0e290592",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Results for Circles data by energy penalty.\n",
        "\n",
        ":::"
      ],
      "id": "963c632e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.1",
      "path": "/Users/paltmeyer/Library/Jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}