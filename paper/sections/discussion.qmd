# Discussion {#sec-discussion}

We begin the discussion by addressing the direct extensions of the counterfactual training approach in @sec-approach. Then, we look at its broader limitations and challenges in @sec-limitations. 

## Future research {#sec-approach}

***CT is defined only for classification settings.*** Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined by the user, CT requires the output space $\mathcal{Y}$ to be discrete. Thus, it applies to binary and multi-class classification but it is not well-defined for other ML tasks where the change in outcome with respect to a decision threshold $\tau$ cannot be readily quantified. In fact, this is a common restriction in research on CEs and AR that predominantly focuses on classification models. Although other settings have attracted some interest [e.g., regression in @spooner2021counterfactual; @zhao2023counterfactual], there is still no consensus on what constitutes a counterfactual in such settings.

***CT is subject to training instabilities.*** Joint energy-based models are susceptible to instabilities during training [@grathwohl2020your] and even though we depart from the SGLD-based sampling, we still encounter major variability in the outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term in @eq-div, and (2) the underlying counterfactual explainers. For example, @altmeyer2023faithful recognize this to be a challenge for *ECCCo* and so it may have downstream impacts on our proposed method. Still, we find that training instabilities can be largely counteracted with strong regularization, in particular through $\lambda_{\text{reg}}$ penalty on the energy. 

***CT is sensitive to hyperparameter selection.*** As discussed in @sec-hyperparameters, our method requires careful selection of the hyperparameters. All experiments relied on grid-search tuning but we recognize that other algorithms may be more efficient. Notably, counterfactual training is iterative which makes a variety of methods applicable, including Bayesian [e.g., @snoek2012practical] or gradient-based [e.g., @franceschi2017forward] optimization.

## Limitations and challenges {#sec-limitations}

***CT increases the training time of models.***

***Immutable features may have proxies.***

***Interventions on features may have downstream impacts on fairness.***