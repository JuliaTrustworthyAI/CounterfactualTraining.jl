{
  "hash": "7f9920c0fad8e9c46290e9b4def6ecfe",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  quarto-aaai-pdf:\n    keep-tex: true\n    include-in-header:\n      text: |\n        \\usepackage[title]{appendix}\n        \\usepackage{placeins}\n---\n\n# Abstract\n\nWe propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-word decision-making systems, counterfactuals should be (1) plausible with respect to the underlying data and (2) actionable with respect to the user-defined mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals ad-hoc during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable explanations while promoting robustness and preserving high predictive performance.\n\n\n\n# Introduction\n\nToday's prominence of artificial intelligence (AI) has largely been driven by **representation learning**: instead of relying on features and rules hand-crafted by humans, modern machine learning (ML) models are tasked with learning representations directly from the data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Advances in computing have made it possible to provide these models with ever-growing degrees of freedom to achieve this task, which often allows them to outperform traditionally parsimonious models. Unfortunately, in doing so, models learn increasingly complex, sensitive representations that humans can no longer easily interpret.\n\nThe trend towards complexity for the sake of performance has come under scrutiny in recent years. At the very cusp of the deep learning (DL) revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are susceptible to adversarial examples (AEs): perturbed versions of data instances that yield vastly different model predictions despite being semantically indistinguishable from their factual counterparts. Some partial mitigation strategies have been proposed---most notably **adversarial training** [@goodfellow2014explaining]---but truly robust deep learning remains unattainable even for models that are considered \"shallow\" by today's standards [@kolter2023keynote]. \n\nPart of the problem is that the high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case].^[We follow the standard ML convention, where \"degrees of freedom\" refer to the number of parameters estimated from data.] As one example, research on the \"lottery ticket hypothesis\" suggests that modern neural networks can be pruned by up to 90% without losing predictive performance [@frankle2018lottery]. Thus, looking at the predictive performance alone, found solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative and semantically meaningless patterns. This poses two related challenges. Firstly, there is no dependable way to verify if learned representations correspond to meaningful, plausible explanations. Secondly, even if we resolve this challenge, it remains undecided how to ensure that machine learning models can *only* learn valuable explanations. \n\nThe first challenge has attracted an abundance of work on **explainable AI** (XAI), a paradigm that focuses on the development of tools to derive (post-hoc) explanations from complex model representations, aiming to mitigate scenarios in which practitioners deploy opaque models and have to blindly rely on their predictions. On many occasions, this has happened in practice, causing harms to people who were adversely and unfairly affected by automated decision-making (ADM) systems involving opaque models; see, e.g., @oneil2016weapons. Effective XAI tools can also aid in monitoring models and providing recourse, empowering people to turn negative outcomes (e.g., \"loan application rejected\") into positive ones (e.g., \"loan application accepted\"). In line with this, our work builds upon **counterfactual explanations** (CE) proposed by @wachter2017counterfactual; CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce an alternative, more desirable output.\n\nTo our surprise, the second challenge has not yet attracted major research interest. In particular, there has been no concerted effort towards improving the degree to which learned representations promote explanations that are both **interpretable** to and deemed **plausible** by humans. Instead, the typical choice has been to improve the ability of XAI tools to identify the subset of explanations that are plausible and valid for any given model, independent of whether these explanations are compatible with the learned representations [@altmeyer2024faithful]. Fortunately, recent findings indicate that improved \"explanatory capacity\" of a model can arise as a consequence of regularization techniques aimed at other training objectives such as generative capacity, generalization, or robustness [@altmeyer2024faithful; @augustin2020adversarial; @schut2021generating]. Our contribution consolidates these findings within a unified framework.\n\nSpecifically, **we propose Counterfactual Training (CT)**: a novel training regime explicitly geared towards improving the explanatory capacity of models that, in high-level terms, we define as the extent to which valid explanations derived for a model can be deemed plausible with respect to the underlying data and global actionability constraints (we refine this notion in Def. \\ref{def-explainability}). For simplicity, we refer to models with high explanatory capacity as *explainable*. To the best of our knowledge, Counterfactual Training represents the first attempt to achieve more explainable models by employing counterfactual explanations already in the training phase.\n\nThe remainder of this manuscript is structured as follows. @sec-lit presents related work, focusing on the link between AEs and CEs. Then follow our two principal contributions. In @sec-method, we introduce our methodological framework and show theoretically that it can be employed to enforce global actionability constraints. In @sec-experiments, through extensive experiments, we empirically demonstrate that CT substantially improves explainability and positively contributes to the robustness of trained models without sacrificing predictive performance. Finally, in @sec-discussion, we discuss open challenges and conclude that CT is a promising approach towards making opaque models more trustworthy.\n\n\n\n\n# Related Literature {#sec-lit}\n\nTo make the desiderata for our framework more concrete, we follow @augustin2020adversarial in tying explainability to the quality of CEs that can be generated for a given model. The authors show that CEs (understood as minimal input perturbations that yield some desired model prediction) tend to be more meaningful if the underlying model is more robust to adversarial examples. We can make intuitive sense of this finding if we look at adversarial training (AT) through the lens of representation learning with high degrees of freedom. As argued before, learned representations may be sensitive to producing implausible explanations and mispredicting for worst-case counterfactuals (i.e., AEs). Thus, by inducing models to \"unlearn\" susceptiblity to such examples, adversarial training can effectively remove implausible explanations from the solution space.\n\n## Adversarial Examples are Counterfactuals\n\nThe interpretation of the link between explainability through counterfactuals on the one side, and robustness to adversarial examples on the other is backed by empirical evidence. @sauer2021counterfactual demonstrate that using counterfactual images during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, show that augmenting the training data of (image) classifiers can improve generalization performance. Finally, @teney2020learning argue that counterfactual pairs tend to exist in training data. Hence, their approach aims to identify similar input samples with different annotations and ensure that the gradient of the classifier aligns with the vector between such pairs of counterfactual inputs using a cosine distance loss function. \n\nCEs have also been used to improve models in the natural language processing domain. For example, @wu2021polyjuice2 propose *Polyjuice*, a general-purpose CE generator for language models and demonstrate that the augmentation of training data with *Polyjuice* improves robustness in a number of tasks, while @luu2023counterfactual introduce the *Counterfactual Adversarial Training* (CAT) framework that aims to improve generalization and robustness of language models by generating counterfactuals for training samples that are subject to high predictive uncertainty. \n\nThere have also been several attempts at formalizing the relationship between counterfactual explanations and adversarial examples. Pointing to clear similarities in how CEs and AEs are generated, @freiesleben2022intriguing makes the case for jointly studying the opaqueness and robustness problems in representation learning. Formally, AEs can be seen as the subset of CEs for which misclassification is achieved [@freiesleben2022intriguing]. Similarly, @pawelczyk2022exploring show that CEs and AEs are equivalent under certain conditions. \n\nTwo other works are closely related to ours in that they use counterfactuals during training with the explicit goal of affecting certain properties of the post-hoc counterfactual explanations. Firstly, @ross2021learning propose a way to train models that guarantee recourse to a positive target class with high probability. Their approach builds on adversarial training by explicitly inducing susceptibility to targeted AEs for the positive class. Additionally, the method allows for imposing a set of actionability constraints ex-ante. For example, users can specify that certain features are immutable. Secondly, @guo2023counternet are the first to propose an end-to-end training pipeline that includes CEs as part of the training procedure. Their *CounterNet* network architecture includes a predictor and a CE generator, where the parameters of the CE generator are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor. In contrast, we impose no restrictions on the ANN architecture at all. \n\n## Aligning Representations with Explanations \n\nImproving the adversarial robustness of models is not the only path towards aligning representations with plausible explanations. In a closely related work, @altmeyer2024faithful show that explainability can be improved through model averaging and refined model objectives. They propose a way to generate counterfactuals that are maximally faithful to the model in that they are consistent with what the model has learned about the underlying data. Formally, they rely on tools from energy-based modelling [@teh2003energy] to minimize the divergence between the distribution of counterfactuals and the conditional posterior over inputs learned by the model. Their counterfactual explainer, *ECCCo*, yields plausible explanations if and only if the underlying model has learned representations that align with them. The authors find that both deep ensembles [@lakshminarayanan2016simple] and joint energy-based models (JEMs) [@grathwohl2020your] tend to do well in this regard. \n\nOnce again it helps to look at these findings through the lens of representation learning with high degrees of freedom. Deep ensembles are approximate Bayesian model averages, which are particularly effective when models are underspecified by the available data [@wilson2020case]. Averaging across solutions mitigates the aforementioned risk of overrelying on a single locally optimal representation that corresponds to semantically meaningless explanations for the data. Likewise, previous work of @schut2021generating found that generating plausible (\"interpretable\") CEs is almost trivial for deep ensembles that have undergone adversarial training. The case for JEMs is even clearer: they optimize a hybrid objective that induces both high predictive performance and strong generative capacity [@grathwohl2020your], which bears resemblance to the idea of aligning models with plausible explanations and has inspired our CT objective.\n\n\n\n# Counterfactual Training {#sec-method}\n\nThis section introduces the Counterfactual Training framework. CT combines ideas from adversarial training, counterfactual explanations, and energy-based modelling with the explicit goal of producing models whose learned representations align with plausible explanations that further comply with user-defined actionability constraints.\n\nIn the context of counterfactual explanations, plausibility has broadly been defined as the degree to which generated CEs comply with the underlying data-generating process [@altmeyer2024faithful;@guidotti2022counterfactual;@poyiadzi2020face]. Plausibility is a necessary but insufficient condition for using CEs to provide algorithmic recourse (AR) to individuals (negatively) affected by opaque models. An AR recommendations must also be actionable, i.e., possible to attain by the recipient. A plausible CE for a rejected 20-year-old loan applicant, for example, might reveal that their application would have been accepted, if only they had been 20 years older. Ignoring all other features, this would comply with the definition of plausibility if 40-year-old individuals were in fact more credit-worthy on average than young adults. But of course this CE does not qualify for providing actionable recourse to the applicant since *age* is not a (directly) mutable feature. Counterfactual training aims to improve model explainability by aligning models with counterfactuals that meet both desiderata: plausibility and actionability. Formally, we define explainability as follows:\n\n\\begin{definition}[Model Explainability]\n\\label{def-explainability}\nLet $\\mathbf{M}_\\theta: \\mathcal{X} \\mapsto \\mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\\mathcal{X}$ to representations $\\phi(\\mathbf{x};\\theta)$ and finally to the $K$-dimensional output space $\\mathcal{Y}$. Assume that for any given input-output pair $\\{\\mathbf{x},\\mathbf{y}\\}_i$ there exists a counterfactual $\\mathbf{x}^{\\prime} = \\mathbf{x} + \\Delta: \\mathbf{M}_\\theta(\\mathbf{x}^{\\prime}) = \\mathbf{y}^{+} \\neq \\mathbf{y} = \\mathbf{M}_\\theta(\\mathbf{x})$, where $\\arg\\max_y{\\mathbf{y}^{+}}=y^+$ is the index of the target class. \n\nWe say that $\\mathbf{M}_\\theta$ has an \\textbf{explanatory capacity} to the extent that faithfully generated counterfactuals are also plausible and actionable. We define these properties as follows:\n\n\\begin{enumerate}\n    \\item (Faithfulness) $\\int^{A} p_\\theta(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$, where $A$ is some arbitrarily small region around $\\mathbf{x}^{\\prime}$.\n    \\item (Plausibility) $\\int^{A} p(\\mathbf{x}^\\prime|\\mathbf{y}^{+})d\\mathbf{x} \\rightarrow 1$; $A$ as specified above.\n    \\item (Actionability) Perturbations $\\Delta$ are subject to some actionability constraints.\n\\end{enumerate}\nand $p_\\theta(\\mathbf{x}|\\mathbf{y}^{+})$ denotes the conditional posterior distribution over inputs. For simplicity, we refer to a model with high explanatory capacity as \\textbf{explainable} in this manuscript. \n\\end{definition}\n\n`\\noindent`{=latex} The characterization of faithfulness and plausibility in Def. \\ref{def-explainability} is the same as in @altmeyer2024faithful, with adapted notation. Intuitively, plausible counterfactuals are consistent with the data and faithful counterfactuals are consistent with what the model has learned about the input data. Actionability constraints in Def. \\ref{def-explainability} vary and depend on the context in which $\\mathbf{M}_\\theta$ is deployed. In this work, we choose to only consider domain and mutability constraints for individual features $x_d$ for $d=1,...,D$. We also limit ourselves to classification tasks for reasons discussed in @sec-discussion.\n\n## Proposed Objective\n\nLet $\\mathbf{x}_t^\\prime$ for $t=0,...,T$ denote a counterfactual generated through gradient descent over $T$ iterations as originally proposed by @wachter2017counterfactual. In broad terms, searching for CEs using gradient descent entails optimizing some form of an objective that balances (1) the classification loss $\\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}),\\mathbf{y})$, and (2) one or more penalty terms $\\lambda_{i}\\text{cost}_{i}(\\cdot)$. The exact specification of these penalties induces various properties in the counterfactual outcomes, and tends to be the key feature that distinguishes various gradient-based \"generators\" or \"explainers\" in the literature [@altmeyer2023explaining], including all generators used in our experiments. We refer the reader to the supplementary appendix for details.\n\nCT adopts gradient-based CE search during training to generate on-the-fly model explanations $\\mathbf{x}^\\prime$ for training samples. We use the term *nascent* to denote counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$ that are not yet valid where $t$ indicates the last iteration before the label is flipped. We store and use these interim counterfactuals as adversarial examples. Conversely, we consider counterfactuals $\\mathbf{x}_T^\\prime$ as *mature* explanations if they have either exhausted all $T$ iterations or converged in terms reaching a pre-specified threshold, $\\tau$, for the predicted probability of the target class: $\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}^\\prime))[y^+] \\geq \\tau$, where $\\mathcal{S}$ is the softmax function.\n\nFormally, we propose the following counterfactual training objective to train explainable (as in Def. \\ref{def-explainability}) models:\n$$\n\\begin{aligned}\n&\\min_\\theta \\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}),\\mathbf{y}) + \\lambda_{\\text{div}} \\text{div}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta) \\\\+ &\\lambda_{\\text{adv}} \\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y}) + \\lambda_{\\text{reg}}\\text{ridge}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta)\n\\end{aligned}\n$$ {#eq-obj}\nwhere $\\text{yloss}(\\cdot)$ is any classification loss that induces discriminative performance (e.g., cross-entropy). The second and third terms are explained in detail below. For now, they can be summarized as inducing explainability directly and indirectly by penalizing the contrastive divergence, $\\text{div}(\\cdot)$, between mature counterfactuals $\\mathbf{x}_T^\\prime$ and observed samples $\\mathbf{x}^+\\in\\mathcal{X}^+=\\{\\mathbf{x}:y=y^+\\}$ in the target class $y^+$, and the adversarial loss, $\\text{advloss}(.)$, wrt. nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$. Finally, $\\text{ridge}(\\cdot)$ denotes a Ridge penalty ($\\ell_2$-norm) that regularizes the magnitude of the energy terms involved in $\\text{div}(\\cdot)$ [@du2019implicit]. The trade-off between the components are governed through $\\lambda_{\\text{div}}$, $\\lambda_{\\text{adv}}$ and $\\lambda_{\\text{reg}}$.\n\n## Directly Inducing Explainability with Contrastive Divergence\n\n@grathwohl2020your observe that any classifier can be re-interpreted as a joint energy-based model that learns to discriminate output classes conditional on the observed (training) samples from $p(\\mathbf{x})$ and the generated samples from $p_\\theta(\\mathbf{x})$. The authors show that JEMs can be trained to perform well at both tasks by directly maximizing the joint log-likelihood: $\\log p_\\theta(\\mathbf{x},\\mathbf{y})=\\log p_\\theta(\\mathbf{y}|\\mathbf{x}) + \\log p_\\theta(\\mathbf{x})$, where the first term can be optimized using cross-entropy as in @eq-obj. To optimize $\\log p_\\theta(\\mathbf{x})$, they minimize the contrastive divergence between the observed samples from $p(\\mathbf{x})$ and the generated samples from $p_\\theta(\\mathbf{x})$. \n\nA key empirical finding of @altmeyer2024faithful was that JEMs perform well on the plausibility objective in Def. \\ref{def-explainability}. This follows directly if we consider samples drawn from $p_\\theta(\\mathbf{x})$ as counterfactuals --- the JEM objective effectively minimizes the divergence between the conditional posterior and $p(\\mathbf{x}|\\mathbf{y}^{+})$. To generate samples, @grathwohl2020your use Stochastic Gradient Langevin Dynamics (SGLD) with an uninformative prior for initialization but we depart from their methodology. Instead we propose to leverage counterfactual explainers to generate counterfactuals of observed training samples. Specifically, we have:\n$$\n\\text{div}(\\mathbf{x}^+,\\mathbf{x}_T^\\prime,y;\\theta) = \\mathcal{E}_\\theta(\\mathbf{x}^+,y) - \\mathcal{E}_\\theta(\\mathbf{x}_T^\\prime,y)\n$$ {#eq-div}\nwhere $\\mathcal{E}_\\theta(\\cdot)$ denotes the energy function defined as $\\mathcal{E}_\\theta(\\mathbf{x},y)=-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]$, with $y^+$ denoting the index of the randomly drawn target class, $y^+ \\sim p(y)$. Conditional on the target class $y^+$, $\\mathbf{x}_T^\\prime$ denotes a mature counterfactual for a randomly sampled factual from a non-target class generated with a gradient-based CE generator for up to $T$ iterations. Mature counterfactuals are ones that have either reached convergence wrt. the decision threshold $\\tau$ or exhausted $T$.\n\nIntuitively, the gradient of @eq-div decreases the energy of observed training samples (positive samples) while increasing the energy of counterfactuals (negative samples) [@du2019implicit]. As the counterfactuals get more plausible (Def. \\ref{def-explainability}) during training, these opposing effects gradually balance each other out [@lippe2024uvadlc]. \n\nThe departure from SGLD allows us to tap into the vast repertoire of explainers that have been proposed in the literature to meet different desiderata. For example, many methods support domain and mutability constraints. In principle, any existing approach for generating CEs is viable, so long as it does not violate the faithfulness condition. Like JEMs [@murphy2022probabilistic], Counterfactual Training can be considered a form of contrastive representation learning. \n\n## Indirectly Inducing Explainability with Adversarial Robustness\n\nBased on our analysis in @sec-lit, counterfactuals $\\mathbf{x}^\\prime$ can be repurposed as additional training samples [@balashankar2023improving;@luu2023counterfactual] or adversarial examples [@freiesleben2022intriguing;@pawelczyk2022exploring]. This leaves some flexibility with regards to the choice for the $\\text{advloss}(\\cdot)$ term in @eq-obj. An intuitive functional form, but likely not the only sensible choice, is inspired by adversarial training:\n$$\n\\begin{aligned}\n\\text{advloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t\\leq T}^\\prime),\\mathbf{y};\\varepsilon)&=\\text{yloss}(\\mathbf{M}_\\theta(\\mathbf{x}_{t_\\varepsilon}^\\prime),\\mathbf{y}) \\\\\nt_\\varepsilon &= \\max_t \\{t: ||\\Delta_t||_\\infty < \\varepsilon\\}\n\\end{aligned}\n$$ {#eq-adv}\nUnder this choice, we consider nascent counterfactuals $\\mathbf{x}_{t\\leq T}^\\prime$ as AEs as long as the magnitude of the perturbation to any single feature is at most $\\varepsilon$. This is closely aligned with @szegedy2013intriguing who define an adversarial attack as an \"imperceptible non-random perturbation\". Thus, we work with a different distinction between CE and AE than @freiesleben2022intriguing who considers misclassification as the distinguishing feature of adversarial examples. One of the key observations of this work is that we can leverage CEs during training and get AEs essentially for free to reap the aforementioned benefits of adversarial training. \n\n## Encoding Actionability Constraints {#sec-constraints}\n\nMany existing counterfactual explainers support domain and mutability constraints out-of-the-box. In fact, both types of constraints can be implemented for any explainer that relies on gradient descent in the feature space for optimization [@altmeyer2023explaining]. In this context, domain constraints can be imposed by simply projecting counterfactuals back to the specified domain, if the previous gradient step resulted in updated feature values that were out-of-domain. Similarly, mutability constraints can be enforced by setting partial derivatives to zero to ensure that features are only perturbed in the allowed direction, if at all. \n\nSince actionability constraints are binding at test time, we should also impose them when generating $\\mathbf{x}^\\prime$ during each training iteration to inform model representations. Through their effect on $\\mathbf{x}^\\prime$, both types of constraints influence model outcomes via @eq-div. Here it is crucial that we avoid penalizing implausibility that arises due to mutability constraints. For any mutability-constrained feature $d$ this can be achieved by enforcing $\\mathbf{x}^+[d] - \\mathbf{x}^\\prime[d]:=0$ whenever perturbing $\\mathbf{x}^\\prime[d]$ in the direction of $\\mathbf{x}^+[d]$ would violate mutability constraints. Specifically, we set $\\mathbf{x}^+[d] := \\mathbf{x}^\\prime[d]$ if:\n\n1. Feature $d$ is strictly immutable in practice.\n2. $\\mathbf{x}^+[d]>\\mathbf{x}^\\prime[d]$, but $d$ can only be decreased in practice.\n3. $\\mathbf{x}^+[d]<\\mathbf{x}^\\prime[d]$, but $d$ can only be increased in practice.\n\n`\\noindent`{=latex} From a Bayesian perspective, setting $\\mathbf{x}^+[d] := \\mathbf{x}^\\prime[d]$ can be understood as assuming a point mass prior for $p(\\mathbf{x}^+)$ with respect to feature $d$. Intuitively, we think of this as ignoring implausibility costs with respect to immutable features, which effectively forces the model to instead seek plausibility with respect to the remaining features. This can be expected to result in lower overall sensitivity to immutable features, which we investigate empirically in @sec-experiments. Under certain conditions, this result holds theoretically:^[For the proof, see the supplementary appendix.]\n\n\\begin{proposition}[Protecting Immutable Features]\n\\label{prp-mtblty}\nLet $f_\\theta(\\mathbf{x})=\\mathcal{S}(\\mathbf{M}_\\theta(\\mathbf{x}))=\\mathcal{S}(\\Theta\\mathbf{x})$ denote a linear classifier with softmax activation $\\mathcal{S}$ where $y\\in\\{1,...,K\\}=\\mathcal{K}$ and $\\mathbf{x} \\in \\mathbb{R}^D$. Assume multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.\n\\end{proposition}\n\n\n\n# Experiments {#sec-experiments}\n\nWe seek to answer the following three research questions:\n\n1. To what extent does the CT objective in Equation 1 induce models to learn plausible explanations?\n2. To what extent does the CT objective produce more favorable algorithmic recourse outcomes in the presence of actionability constraints?\n3. What are the effects of hyperparameter selection wrt. the CT objective?\n\n## Experimental Setup\n\nOur key outcome of interest is improvement in explainability (Def. \\ref{def-explainability}). Thus, we focus primarily on the plausibility and cost of faithfully generated counterfactuals at test time^[Additional metrics such as validity and redundancy are reported in the appendix.]. To measure the cost, we follow the standard convention of using distances ($\\ell_1$-norm) between factuals and counterfactuals as a proxy. For plausibility, we assess how similar CEs are to the observed samples in the target domain, $\\mathbf{X}^+\\subset\\mathcal{X}^+$. We rely on the metric used by @altmeyer2024faithful,\n$$\n\\text{IP}(\\mathbf{x}^\\prime,\\mathbf{X}^+) = \\frac{1}{\\lvert\\mathbf{X}^+\\rvert}\\sum_{\\mathbf{x} \\in \\mathbf{X}^+} \\text{dist}(\\mathbf{x}^{\\prime},\\mathbf{x})\n$$ {#eq-impl-dist}\nand introduce a novel divergence metric,\n$$\n\\text{IP}^*(\\mathbf{X}^\\prime,\\mathbf{X}^+) = \\text{MMD}(\\mathbf{X}^\\prime,\\mathbf{X}^+)\n$$ {#eq-impl-div}\nwhere $\\mathbf{X}^\\prime$ denotes a collection of counterfactuals and $\\text{MMD}(\\cdot)$ is the unbiased estimate of the squared population maximum mean discrepancy, proposed by @gretton2012kernel. The metric in @eq-impl-div is equal to zero if and only if the two distributions are exactly the same, $\\mathbf{X}^\\prime=\\mathbf{X}^+$.\n\nWe also assess the predictive performance of models using standard metrics, including robust accuracy estimated on adversarially perturbed data using FGSM [@goodfellow2014explaining].\n\nWe run experiments with three gradient-based generators: *Generic* of @wachter2017counterfactual as a simple baseline approach, *REVISE* [@joshi2019realistic] that aims to generate plausible counterfactuals using a surrogate Variational Autoencoder (VAE), and *ECCo*---the generator of @altmeyer2024faithful but without the conformal prediction component---as a method that directly targets both faithfulness and plausibility of the counterfactuals.\n\nWe make use of nine classification datasets common in the CE/AR literature. Four of them are synthetic with two classes and different characteristics: linearly separable clusters (*LS*), overlapping clusters (*OL*), concentric circles (*Circ*), and interlocking moons (*Moon*). These datasets are generated using the library of @altmeyer2023explaining and we present them in the supplementary appendix. Next, we have four real-world binary tabular datasets from the domain of economics: *Adult* (a.k.a. Census data) of @becker1996adult2, California housing (*CH*) of @pace1997sparse, Default of Credit Card Clients (*Cred*) of @yeh2016default, and Give Me Some Credit (*GMSC*) from @kaggle2011give2. Finally, for the convenience of illustration, we use the 10-class *MNIST* dataset [@lecun1998mnist].\n\nTo assess CT, we investigate the improvements in performance metrics when using it on top of a weak baseline (BL): a multilayer perceptron (*MLP*). This is the best way to get a clear picture of the effectiveness of CT, and it is consistent with evaluation practices in the related literature [@goodfellow2014explaining;@ross2021learning;@teney2020learning].\n\n## Experimental Results\n\nOur main quantitative results for the *MLP* are summarised in @tbl-main, which shows average outcomes along with bootstrapped standard errors for key metrics. The following example illustrates how to read @tbl-main.\n\n![Illustration of how CT improves model explainability: (a) conventional training, all mutable; (b) CT, all mutable; (c) conventional, *age* immutable; (d) CT, *age* immutable. The linear decision boundary is shown in green along with training data colored according to ground-truth labels: $y^-=1$ (blue) and $y^+=2$ (orange). Stars indicate counterfactuals in the target class.](/paper/figures/poc.svg){#fig-poc fig-env=\"figure*\"}\n\n### Synthetic Example: Prediction of Credit Defaults\n\n@fig-poc presents results for a linear classifier fitted to *LS*, which complies with the data assumptions in Proposition \\ref{prp-mtblty}. The four panels show the outcomes for different training procedures: in panels (a) and (c) we have trained the models conventionally, while in panels (b) and (d) we have applied CT. For illustrative purposes, suppose the first feature represents *debt* (mutable) and the second feature represents *age* (immutable) of loan applicants seeking counterfactual explanations for moving to the target class: loan provided (orange).\n\nIn all four cases, it is possible to generate valid counterfactuals (stars) for unsuccessful applicants (blue). They cross the decision boundary (green) into the target class, but their quality differs. In panel (a), they are not plausible: they do not comply with the distribution of the factuals in $y^+$ to the point where they form a clearly discernible cluster. In panel (b), they are highly plausible, meeting the first objective of Def. \\ref{def-explainability}. This difference in outcomes is quantified for the non-linear MLP in the first two columns of @tbl-main as the $\\%$-reduction in implausibility: it is substantial and statistically significant for *LS* across both metrics, the distance-based $IP$ (29%) and divergence-based $IP^{*}$ (55%).\n\nIn panel (c) of @fig-poc, the CEs involve substantial reductions in *debt* for younger applicants. By comparison, counterfactual paths are shorter on average in panel (d) where we have protected the immutable *age* as described in @sec-constraints. Due to the classifier's lower sensitivity to *age*, recommendations with respect to *debt* are much more homogenous and do not unfairly punish younger individuals. These counterfactuals are also plausible with respect to the mutable feature, despite requiring smaller debt reductions on average, resulting in smaller costs to individuals. This result is quantified for the non-linear case in column three of @tbl-main, which shows the $\\%$-reduction in costs averaged across valid counterfactuals. Once again, the impact of CT is statistically significant and substantial (14%). Thus, we consider the model in panel (d) as the most explainable according to Def. \\ref{def-explainability}. In the following, we present the results for all remaining datasets.\n\n### Plausibility. {#sec-plaus}\n\nWe find that CT generally leads to substantial and statistically significant improvements in plausibility: average reductions in $IP$ range from around 7% for *MNIST* to almost 60% for *Circ*; for the real-world tabular datasets they are around 12% for both *CH* and *Cred* and almost 25% for *GMSC*; for *Adult* and *OL* we find no significant impact of CT on $IP$. Reductions in $IP^{*}$ are even more substantial and generally statistically significant, although the average degree of uncertainty is higher than for $IP$: average reductions range from around 20% (*Moons*) to almost 90% (*Circ*). The only negative findings for *OL* and *MNIST* are statistically insignificant and, in the latter case, do not align with qualitative findings, which are much more plausible for CT (@fig-mnist).\n\n![Explanations for *MNIST* for BL (top) and CT (bottom). First columns is a randomly chosen factual 0. Columns 2 through 5 are *ECCo* counterfactuals for that factual in target classes 1 to 4. Columns 6 trough 10 show integrated gradients averaged over all test images in the corresponding classes (5-9).](/paper/figures/mnist_body.png){#fig-mnist}\n\n### Actionability. {#sec-act}\n\nWe also find that CT can reduce sensitivity to immutable, protected features and thus lead to less costly counterfactual outcomes as illustrated in @fig-poc. In column three of @tbl-main, we impose mutability constraints on selected features and compute the reduction in average costs of counterfactuals associated with CT compared to the weak baseline: for synthetic datasets, we always protect the first feature; for all real-world tabular datasets we could identify and protect an *age* variable; for *MNIST*, we protect the five top and bottom rows of digits. Reductions in costs are overwhelmingly positive and significant of up to nearly 60% for *GMSC*. While the estimated cost reductions for *Adult* and *MNIST* are not significant, @fig-mnist demonstrates that CT does have the expected effect: sensitivity to protected features as proxied by integrated gradients is drastically reduced^[Additional findings for integrated gradients are reported in the appendix.]. In the case of *Cred*, average costs increase, likely because any potential benefits from protecting the *age* are outweighed by the increase in costs required for greater plausibility.\n\n### Predictive Performance. {#sec-pred} \n\nTest accuracy for CT is virtually identical to the baseline for *Adult*, *Circ*, *LS*, *Moon*, and *OL*, and even slightly improved for *Cred*. Exceptions to this general pattern are *MNIST*, *CH*, and *GMSC*, for which we observe a reduction in test accuracy of 2, 5, and 15 percentage points respectively. When looking at robust test accuracies (Acc.$^*$) for these datasets in particular, we find that CT strongly outperforms the baseline. In fact, we observe that CT improves adversarial robustness on all datasets. \n\n<!-- Final Results -->\n<!---->\n<!-- Columns 1-3 show mean outcome +- TWO (2) bootstrapped standard errors. -->\n<!---->\n<!-- - IP: significant and in many cases substantial reduction in implausibility, except for OL and Adult (both insfignficant) -->\n<!-- - IP*: significant and in many cases substantial reduction (up to 90%) in implausibility, except for MNIST and OL. -->\n<!-- - Cost: significant and in some cases substantial reduction, except for Adult, MNIST (both insignificant) and Credit (significant increase in costs). -->\n\n::: {#tbl-main}\n\n```{=latex}\n\\begin{table*}\n\\input{tables/main.tex}\n\\end{table*}\n```\n\nKey performance metrics and bootstrapped standard errors for all datasets. **Plausibility** (columns 1-2): percentage reduction in implausibility for $IP$ and $IP^*$, respectively; **Actionability** (column 3): percentage reduction in costs with protected features. **Accuracy** (columns 4-7): test accuracies and robust accuracies ($\\text{Acc}^*$) for CT and the baseline (BL). Counterfactual outcomes in columns 1-3 are aggregated across bootstrap samples and varying degrees of the energy penalty $\\lambda_{\\text{egy}}$ used for *ECCo* at test time. Standard errors for accuracy are bootstrapped from the test set.\n:::\n\n### Hyperparameter settings. {#sec-hyperparameters}\nWe test the impact of three types of hyperparameters. Here we focus on the highlights; full results are available in the supplementary appendix.  \n`\\indent`{=latex} First, we note that CT is highly sensitive to the choice of a CE generator and its hyperparameters but (1) there are manageable patterns, and (2) we can usually identify settings that improve either plausibility or cost, and often both of them at the same time. For example, *REVISE* tends to perform the worst, most likely because it uses a surrogate VAE to generate counterfactuals which impedes faithfulness [@altmeyer2024faithful]. Increasing $T$, the maximum number of steps, generally yields better outcomes because more CEs can mature in each training epoch. The impact of $\\tau$, the required decision threshold is more difficult to predict. On \"harder\" datasets it may be difficult to satisfy high $\\tau$ for any given sample (i.e., also factuals) and so increasing this threshold does not seem to correlate with better outcomes. In fact, $\\tau=0.5$ generally leads to optimal results as it is associated with high proportions of mature counterfactuals.  \n`\\indent`{=latex} Second, the strength of the energy regularization, $\\lambda_{\\text{reg}}$ is highly impactful and leads to poor performance in terms of decreased plausibility and increased costs if insufficiently high. The sensitivity with respect to $\\lambda_{\\text{div}}$ and $\\lambda_{\\text{adv}}$ is much less evident. While high values of $\\lambda_{\\text{reg}}$ may increase the variability in outcomes when combined with high values of $\\lambda_{\\text{div}}$ or $\\lambda_{\\text{adv}}$, this effect is not very pronounced.   \n`\\indent`{=latex} Third, the effectiveness and stability of CT is positively associated with the number of counterfactuals generated during each training epoch. A higher number of training epochs is also beneficial. Interestingly, we observed desired improvements when CT was combined with conventional training and applied only for the final 50% of epochs of the complete training process. Put differently, CT can improve the explainability of models in a fine-tuning manner. \n\n\n\n# Conclusions {#sec-discussion}\n\nAs our results indicate, counterfactual training produces models that are more explainable. Nonetheless, these advantages come at the cost of two important limitations.  \n\n*Interventions on features have implications for fairness.* We provide a tool that allows practitioners to modify the sensitivity of a model with respect to certain features. Model owners can use our solution to support the fair and equitable treatment of decision subjects, but they could also misuse it by enforcing explanations based on features that are more difficult to modify by some (group of) individuals. When used irresponsibly, CT could result in an unfairly assigned burden of recourse [@sharma2020certifai], threatening the equality of opportunity [@bell2024fairness]. Additionally, CT requires mutability constraints for the features considered by the model. Even if all immutable features are protected, there may exist proxies that are mutable, and hence should not be protected, but preserve sufficient information about the principals to hinder these protections. Deciding on actionability is still a major open challenge in the AR literature [@venkatasubramanian2020philosophical] impacting the capacity of CT to fulfill its intended goal.\n\n*CT increases the training time of models.* Just like adversarial training, CT is more resource-intensive than conventional training regimes. Higher numbers of CEs per iteration improve the quality of learned representations, but they also increase the number of computations. As our codebase is not performance optimized, grids of 270 settings took up to four hours for more demanding datasets in our experiments, when ran on 34 2GB CPUs (see supplementary appendix). Other than optimization, three factors mitigate this effect: (1) CT yields itself to parallel execution; (2) it amortizes the cost of CEs for the training samples; (3) it can be used to fine-tune conventionally-trained models.  \n\nWe also highlight several important directions for future research. Firstly, it is an interesting challenge to extend CT beyond classification settings. Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined, CT requires the output space $\\mathcal{Y}$ to be discrete. Thus, it does not apply to ML tasks where the change in outcome cannot be readily discretized. Focus on classification is a common choice in research on CEs and AR. Other settings have attracted some interest, e.g., regression [@spooner2021counterfactual], but there is little consensus how to robustly extend the notion of CEs.  \nSecondly, CT is susceptible to training instabilities. This problem has been recognized for JEMs [@grathwohl2020your] and even though we depart from the SGLD sampling, we still encounter variability in outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term, $\\text{div}(\\cdot)$, in @eq-div, and (2) the underlying explainers. We find several promising ways to mitigate this problem: regularizing energy ($\\lambda_{\\text{reg}}$), generating sufficiently many counterfactuals during each epoch, and including only mature counterfactuals in $\\text{div}(\\cdot)$.  \nFinally, we believe that it is possible to substantially improve hyperparameter selection procedures, and thus performance. In this work, we have relied exclusively on grid searches for this task. Future work on CT could benefit from investigating more sophisticated approaches. Notably, our method is iterative which makes methods such as Bayesian or gradient-based optimization applicable [@bischl2023hyperparameter].\n\n\n\nTo conclude, state-of-the-art machine learning models are prone to learning complex representations that cannot be interpreted by humans. Existing explainability solutions cannot guarantee that explanations agree with the model's learned representation of data. As a step towards addressing this challenge, we introduced counterfactual training, a novel training regime that incentivizes highly-explainable models. Our approach leads to explanations that are both plausible (compliant with the underlying data-generating process) and actionable (compliant with user-specified mutability constraints), and thus meaningful to their recipients. Through extensive experiments, we demonstrated that CT satisfies its objective while promoting robustness and preserving the predictive performance of models. It can also be used to fine-tune conventionally-trained models and achieve similar gains. Thus, our work showcases that it is practical to simultaneously improve models and their explanations.\n\n\n\n\n# References {-}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "paper_files/figure-pdf"
    ],
    "filters": []
  }
}