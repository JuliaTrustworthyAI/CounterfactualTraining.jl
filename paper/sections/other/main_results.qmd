
```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)

using CTExperiments
using CTExperiments.CairoMakie
using CTExperiments.CSV
using CTExperiments.DataFrames
using CTExperiments.Plots
using CTExperiments.PrettyTables
using CTExperiments.StatsBase
using Random
using Serialization

using DotEnv
DotEnv.load!()
```

```{julia}
res_dir = joinpath(ENV["FINAL_GRID_RESULTS"],"single")
mtbl_dir = joinpath(ENV["FINAL_GRID_RESULTS"],"mutability")
```

\FloatBarrier

# Details on Main Experiments {#sec-app-main .appendix}

## Final Hyperparameters

As discussed @sec-experiments, CT is sensitive to certain hyperparameter choices. We study the effect of many hyperparameters extensively in @sec-app-grid. For the main results, we tune a small set of key hyperparameters (@sec-app-tune). The final choices for the main results are presented for each data set in @tbl-final-params along with training, test and batch sizes.

::: {#tbl-final-params}

::: {.content-hidden unless-format="pdf"}

```{julia}
#| output: asis

df = final_params(res_dir)
float_cols = ["decision_threshold", "lambda_energy_reg"]
get_table_inputs(df, nothing; backend=Val(:latex)) |>
    inputs -> tabulate_results(inputs; wrap_table=false, table_type=:tabular, formatters=PrettyTables.ft_round(2, findall([x in float_cols for x in names(df)])))
```

:::

Final hyperparameters used for the main results presented in @sec-experiments. Any hyperparameter not shown here is set to its default value (@nte-train-default).

:::

## Final Results {#sec-app-final-results}

Plus/minus two standard deviations of bootstrap estimates. 

<!-- Standard deviation of bootstrap is the standard error. -->


```{julia}
#| eval: false

df_mtbl = aggregate_ce_evaluation(
    mtbl_dir; 
    ratio=true, 
    rebase=false, 
    y="distance", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = final_table(res_dir; tbl_mtbl=df_mtbl)
Serialization.serialize("paper/experiments/output/final_table.jls", df)
```

```{julia}
#| eval: false

df = Serialization.deserialize("paper/experiments/output/final_table.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
for fname in ["paper/quarto_aaai/tables/main.tex","paper/preprint/tables/main.tex"]
    tabulate_results(
        inputs; 
        table_type=:tabular, 
        save_name=fname, 
        wrap_table=false, 
    )
end
```

### Robust Performance Plots

```{julia}
#| eval: false

plt = CTExperiments.plot_performance(res_dir, eps=range(0.0, 0.1, length=10) |> collect, adversarial=true, byvars=["objective", "eps"], drop_synthetic=false)
plt_out = draw(plt, figure=(size=(1000, 150),), axis=(yticks=[0.0,0.5,1.0], limits=(nothing, (0,1)), xticklabelsvisible=false, xticksvisible=false))
save("paper/figures/acc.png", plt_out; px_per_unit=3)
```

### Confidence Intervals

```{julia}
#| eval: false

lvls = [0.90, 0.95, 0.99]

# Mutability:
y = "distance"
df_mtbl = aggregate_ce_evaluation(
    mtbl_dir; 
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
    drop_models=["linear"]
)
df_mtbl.measure .= y

# Distance-based:
y = "plausibility_distance_from_target"
df_ip = aggregate_ce_evaluation(
    res_dir;
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
) 
df_ip.measure .= y 

# Divergence-based:
y = "mmd"
df_mmd = aggregate_ce_evaluation(
    res_dir;
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
)
df_mmd.measure .= y
df = vcat(df_mtbl, df_ip, df_mmd)
Serialization.serialize("paper/experiments/output/ci.jls", df)
```

::: {#tbl-ci}

```{julia}
#| output: asis

df = Serialization.deserialize("paper/experiments/output/ci.jls")
df = select(df, :variable, :measure, :dataset, :full, :vanilla, "99.0%")
df.variable = (x -> LatexCell(replace(x.data, " \$(-%)\$" => ""))).(df.variable)
sort!(df, [:measure, :dataset])
df = select(df, Not(:measure))
df = transform(df, "99.0%" => AsTable) |>
    df -> select(df, Not("99.0%"))
rename!(df, :dataset => :data, :x1 => :LB, :x2 => :UB, :full => :CT, :vanilla => :BL)
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
    sort_by_data = false,
    hlines = :none,
)

```

Mean outcomes for **CT** and **BL** along with bootstrapped confidence intervals (99%) for difference in mean outcomes grouped by dataset and evaluation metric. Column **LB** and **UB** show the lower and upper bound of the intervals, respectively, and computed using the percentile method. The underlying counterfactual evaluations are the same as the ones used to produce @tbl-main. 

:::

### Qualitative Findings for Image Data

```{julia}
#| eval: false

Random.seed!(42)    # change seed for different outcome
overwrite = false   # set to `true` if you want to overwrite the file
fname = joinpath(res_dir,"mlp/mnist/grid_config.toml")
output_dir = mkpath("paper/experiments/output/extra/")
gen = CTExperiments.GeneratorParams(lambda_energy=25.0)     # instead of a high penalty, you can also use e.g. 1.0 and many iterations e.g 1000
conv = "max_iter"
cfg = EvaluationConfig(
    grid_file=fname, 
    counterfactual_params=(conv=conv, generator_params=gen), 
    save_dir=output_dir,
    test_time=true
)
generate_factual_target_pairs(cfg; overwrite, nce=10)
plts = plot_ce(MNIST(), cfg; byvars="objective")
savefig(plts["objective"]["full"]["ecco"], "paper/figures/mnist_mlp.png")
savefig(plts["objective"]["vanilla"]["ecco"], "paper/figures/mnist_mlp_vanilla.png")
```



@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla).

::: {layout="[10,-2,10]" layout-valign="top"}
![Counterfactual images for *MLP* with counterfactual training. Factual images are shown on the diagonal, with the corresponding counterfactual for each target class (columns) in that same row. The underlying generator, *ECCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}

![The same setup, factuals, model architecture and generator as in @fig-mnist, but the model was trained conventionally.](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}
:::

### Integrated Gradients

:::{#tbl-ig}

```{julia}
#| output: asis

df = Serialization.deserialize(joinpath(mtbl_dir, "ig.jls")) |>
    df -> transform(df, [:mean, :se] => ByRow((m, s) -> LatexCell("$(round(m, digits=2)) \\pm $(round(s, digits=2))")) => :mean) |>
    df -> select(df, Not(:se)) |>
    df -> DataFrames.unstack(df, :objective, :mean) |>
    df -> rename(df, :full => :CT, :vanilla => :BL) |>
    df -> transform(df, :data => ByRow(d  -> CTExperiments.format_header(d)) => :data)
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
al = [
    "l", 
    "S[table-format=2.2(3.2)]", 
    "S[table-format=2.2(2.2)]"
] 
tabulate_results(
    inputs,
    al
    ; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Integrated gradients.

:::

### Costs


```{julia}
#| eval: false

df = final_table(
    res_dir; 
    ce_var=["distance"], 
    agg_further_vars=[["run", "lambda_energy_eval"]], 
    total_uncertainty=false, 
)
Serialization.serialize("paper/experiments/output/costs.jls", df)
```

::: {#tbl-costs}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/costs.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
al = [
    "l", 
    "S[table-format=2.2(1.2)]", 
]
tabulate_results(
    inputs, al; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Costs

:::

### Validity

```{julia}
#| eval: false

# Unconstrained:
df = aggregate_ce_evaluation(
    res_dir; 
    ratio=false, 
    rebase=false, 
    y="validity", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = select(df, Not(:se, :variable)) |> df -> unstack(df, :objective, :mean)
rename!(df, :dataset => :data, :Full => :CT, :Vanilla => :BL)
Serialization.serialize("paper/experiments/output/validity.jls", df)

# Mutability constrained:
df = aggregate_ce_evaluation(
    mtbl_dir; 
    ratio=false, 
    rebase=false, 
    y="validity", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = select(df, Not(:se, :variable)) |> df -> unstack(df, :objective, :mean)
rename!(df, :dataset => :data, :Full => :CT, :Vanilla => :BL)
Serialization.serialize("paper/experiments/output/validity_mtbl.jls", df)
```

::: {#tbl-val}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/validity.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Validity

:::

::: {#tbl-val-mtbl}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/validity_mtbl.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Validity

:::
