# Discussion {#sec-discussion}

We first address the direct extensions of CT in @sec-approach. Then, we look at its limitations and challenges in @sec-limitations. 

## Future Research {#sec-approach}

***CT is defined only for classification settings.*** Our formulation relies on the distinction between non-target class(es) $y^{-}$ and target class(es) $y^{+}$ to generate counterfactuals through @eq-obj. While $y^{-}$ and $y^{+}$ can be arbitrarily defined, CT requires the output space $\mathcal{Y}$ to be discrete. Thus, it does not apply to ML tasks where the change in outcome cannot be readily quantified. Focus on classification models is a common restriction in research on CEs and AR. Other settings have attracted some interest (e.g., regression in [@spooner2021counterfactual; @zhao2023counterfactual]), but there is little consensus how to robustly extend the notion of counterfactuals.

***CT is subject to training instabilities.*** Joint energy-based models are susceptible to instabilities during training [@grathwohl2020your] and even though we depart from the SGLD-based sampling, we still encounter major variability in the outcomes. CT is exposed to two potential sources of instabilities: (1) the energy-based contrastive divergence term in @eq-div, and (2) the underlying counterfactual explainers. For example, @altmeyer2023faithful recognize this to be a challenge for *ECCCo* and so it may have downstream impacts on our proposed method. Still, we find that training instabilities can be successfully mitigated by regularizing energy ($\lambda_{\text{reg}}$), generating a sufficiently large number of counterfactuals during each training epoch, and including only mature counterfactuals for contrastive divergence.

***CT is sensitive to hyperparameter selection.*** Our method benefits from tuning certain key hyperparameters (see @sec-hyperparameters). In this work, we have relied exclusively on grid search for this task. Future work on CT could benefit from investigating more sophisticated approaches towards hyperparameter tuning. Notably, CT is iterative which makes a variety of methods applicable, including Bayesian [e.g., @snoek2012practical] or gradient-based [e.g., @franceschi2017forward] optimization.

## Limitations and Challenges {#sec-limitations}

***CT increases the training time of models.*** Counterfactual training promotes explainability through CEs and robustness through AEs at the cost of longer training times compared to conventional training regimes. While higher numbers of iterations and counterfactuals per iteration positively impact the quality of found solutions, they also increase the required amount of computations. We find that relatively small grids with 270 settings can take almost four hours for more demanding datasets on a high-performance computing cluster with 34 2GB CPUs^[See supplementary appendix for computational details.]. However, there are three factors that attenuate the impact of this limitation. First, CT provides counterfactual explanations for the training samples essentially for free, which may be beneficial in many ADM systems. Second, we find that CT can retain its value when used as a "fine-tuning" training regime for conventionally-trained models. Third, in principle, CT yields itself to parallel execution, which we have leveraged for our own experiments.

***Immutable features may have proxies.*** We propose an approach to protect immutable features and thus increase the actionability of the generated CEs. However, it requires that model owners define the mutability constraints for (all) features considered by the model. Even with sufficient domain knowledge to protect all immutable features, there may exist proxies that are theoretically mutable (and hence should not be protected) but preserve enough information about the principals to hinder the protections. As an example, consider the Adult dataset used in our experiments where the mutable education status is a proxy for the immutable age, in that the attainment of degrees is correlated with age. Delineating actionability is a major undecided challenge in the AR literature [see, e.g., @venkatasubramanian2020philosophical] impacting the capacity of CT to increase the explainability of the model.

***Interventions on features may impact fairness downstream.***  Related to the point above, we provide a tool that allows practitioners to modify the sensitivity of a model with respect to certain features, which may have implication for the fair and equitable treatment of individuals subject to automated decisions. As protecting a set of features leads the model to assign higher relative importance to unprotected features, model owners could misuse our solution by enforcing explanations based on features that are more difficult to modify by some (group of) individuals. For example, consider again the Adult dataset where features such as workclass or education may be more difficult to change for underpriviledged groups. When applied irresponsibly, CT could result in an unfairly assigned burden of recourse [e.g., @sharma2020certifai], threatening the equality of opportunity in the system [@bell2024fairness] and potentially reinforcing social segregation [@gao2023impact]. Still, as the referenced publications indicate, such phenomena are not specific to CT; all types of ADM solutions without strong external protections have been recognized to promote harmful power dynamics [@maas2023machine].