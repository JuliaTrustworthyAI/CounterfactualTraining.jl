{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "projectdir = splitpath(pwd()) |>\n",
        "    ss -> joinpath(ss[1:findall([s == \"CounterfactualTraining.jl\" for s in ss])[1]]...) \n",
        "cd(projectdir)\n",
        "\n",
        "using CTExperiments\n",
        "using CTExperiments.CounterfactualExplanations\n",
        "using CTExperiments.CounterfactualTraining\n",
        "using CTExperiments.Flux\n",
        "using CTExperiments.Plots\n",
        "using CTExperiments.TaijaPlotting\n",
        "using Plots.PlotMeasures"
      ],
      "id": "175b65be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Protecting Mutability Constraints with Linear Classifiers {#sec-app-constraints}\n",
        "\n",
        "In @sec-constraints we explain that to avoid penalizing implausibility that arises due to mutability constraints, we impose a point mass prior on $p(\\mathbf{x})$ for the corresponding feature. We argue in @sec-constraints that this approach induces models to be less sensitive to immutable features and demonstrate this empirically in @sec-experiments. Below we derive the analytical results in @thm-mtblty.\n",
        "\n",
        "::: {.proof}\n",
        "\n",
        "Let $d_{\\text{mtbl}}$ and $d_{\\text{immtbl}}$ denote some mutable and immutable feature, respectively. Suppose that $\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}}$ and $\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}}$, where $\\mu_{k,d}$ denotes the conditional sample mean of feature $d$ in class $k$. In words, we assume that the immutable feature tends to take lower values for samples in the non-target class $y^-$ than in the target class $y^+$. We assume the opposite to hold for the mutable feature.\n",
        "\n",
        "Assuming multivariate Gaussian class densities with common diagonal covariance matrix $\\Sigma_k=\\Sigma$ for all $k \\in \\mathcal{K}$, we have for the log likelihood ratio between any two classes $k,m \\in \\mathcal{K}$ [@hastie2009elements]:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})}=\\mathbf{x}^\\intercal \\Sigma^{-1}(\\mu_{k}-\\mu_{m})  + \\text{const}\n",
        "$$ {#eq-loglike}\n",
        "\n",
        "By independence of $x_1,...,x_D$, the full log-likelihood ratio decomposes into:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\frac{\\mu_{k,d}-\\mu_{m,d}}{\\sigma_{d}^2} x_{d} + \\text{const}\n",
        "$$ {#eq-loglike-decomp}\n",
        "\n",
        "By the properties of our classifier (*multinomial logistic regression*), we have:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p(k|\\mathbf{x})}{p(m|\\mathbf{x})} = \\sum_{d=1}^D \\left( \\theta_{k,d} - \\theta_{m,d} \\right)x_d + \\text{const}\n",
        "$$ {#eq-multi}\n",
        "\n",
        "where $\\theta_{k,d}=\\Theta[k,d]$ denotes the coefficient on feature $d$ for class $k$. \n",
        "\n",
        "Based on @eq-loglike-decomp and @eq-multi we can identify that $(\\mu_{k,d}-\\mu_{m,d}) \\propto (\\theta_{k,d} - \\theta_{m,d})$ under the assumptions we made above. Hence, we have that $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$ and $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$\n",
        "\n",
        "Let $\\mathbf{x}^\\prime$ denote some randomly chosen individual from class $y^-$ and let $y^+ \\sim p(y)$ denote the randomly chosen target class. Then the partial derivative of the contrastive divergence penalty [@eq-div] with respect to coefficient $\\theta_{y^+,d}$ is equal to \n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left(\\text{div}(\\mathbf{x},\\mathbf{x^\\prime},\\mathbf{y};\\theta)\\right) = \\frac{\\partial}{\\partial\\theta_{y^+,d}} \\left( \\left(-\\mathbf{M}_\\theta(\\mathbf{x})[y^+]\\right) - \\left(-\\mathbf{M}_\\theta(\\mathbf{x}^\\prime)[y^+]\\right) \\right) = x_{d}^\\prime - x_{d}\n",
        "$$ {#eq-grad}\n",
        "\n",
        "and equal to zero everywhere else.\n",
        "\n",
        "Since $(\\mu_{y^-,d_{\\text{immtbl}}} < \\mu_{y^+,d_{\\text{immtbl}}})$ we are more likely to have $(x_{d_{\\text{immtbl}}}^\\prime - x_{d_{\\text{immtbl}}}) < 0$ than vice versa at initialization. Similarly, we are more likely to have $(x_{d_{\\text{mtbl}}}^\\prime - x_{d_{\\text{mtbl}}}) > 0$ since $(\\mu_{y^-,d_{\\text{mtbl}}} > \\mu_{y^+,d_{\\text{mtbl}}})$.\n",
        "\n",
        "This implies that if we do not protect feature $d_{\\text{immtbl}}$, the contrastive divergence penalty will push down on $\\theta_{y^-,d_{\\text{immtbl}}}$ thereby exacerbating the existing effect $(\\theta_{y^-,d_{\\text{immtbl}}} - \\theta_{y^+,d_{\\text{immtbl}}}) < 0$. In words, not protecting the immutable feature would have the undesirable effect of making the classifier more sensitive to this feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for lower values of $d_{\\text{immtbl}}$. \n",
        "\n",
        "By the same rationale, the contrastive divergence penalty can generally be expected to push up on $\\theta_{y^-,d_{\\text{mtbl}}}$ exacerbating $(\\theta_{y^-,d_{\\text{mtbl}}} - \\theta_{y^+,d_{\\text{mtbl}}}) > 0$. In words, this has the effect of making the classifier more sensitive to the mutable feature, in that it would be more likely to predict class $y^-$ as opposed to $y^+$ for higher values of $d_{\\text{mtbl}}$.\n",
        "\n",
        "Thus, our proposed approach of protecting feature $d_{\\text{immtbl}}$ has the net affect of decreasing the classifier's sensitivity to the immutable feature relative to the mutable feature (i.e. no change in sensitivity for $d_{\\text{immtbl}}$ relative to increased sensitivity for $d_{\\text{mtbl}}$).\n",
        "\n",
        ":::\n",
        "\n",
        "We provide an illustrative example in @exm-grad.\n",
        "\n",
        "::: {#exm-grad}\n",
        "\n",
        "## Prediction of Consumer Credit Default\n",
        "\n",
        "Suppose now that $d_{\\text{immtbl}}$ represents an individual's *age* and $d_{\\text{mtbl}}$ represents an individual's existing level of credit card debt. Assume that these two features are independent, the class conditional densities are Gaussian and we use no other features to predict the risk of individuals defaulting on a consumer loan using a linear classifier. We have simulated this scenario using synthetic data in @fig-mtblty.\n",
        "\n",
        "In panel (a) of @fig-mtblty, we have trained the linear classifier using counterfactual training, treating both features as mutable. The linear decision boundary is roughly equally sensitive to both *age* and existing levels of *debt*. \n",
        "\n",
        "Conversely, in panel (b) of @fig-mtblty, we have trained the same classifier using counterfactual training, but this time treating *age* as an immutable feature. The result is a new decision boundary that has tilted in favor of higher sensitivity to the mutable feature (existing *debt*) and lower sensitivity to the immutable feature (*age*).\n",
        "\n",
        ":::\n",
        "\n",
        "![Visual illustration of the effect of imposing mutability constraints. See @exm-grad for details.](/paper/figures/app_mtblty.svg){#fig-mtblty}\n",
        "\n",
        "::: {.callout-warning}\n",
        "\n",
        "\\@Cynthia, \\@Arie, I have tentatively phrased the above in terms of a theorem and proof. This is something I've so far shied away from because I feel a bit out of my depth when it comes to mathematical proofs. The above makes intuitive sense to me, but I don't know for sure if it's correct. \n",
        "\n",
        ":::\n"
      ],
      "id": "51454e0e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "using CTExperiments: \n",
        "    get_ce_data, \n",
        "    train_val_split, \n",
        "    build_model, \n",
        "    LinearModel, \n",
        "    get_input_encoder\n",
        "\n",
        "\n",
        "# Callback:\n",
        "using CTExperiments: get_log_reg_params, get_decision_boundary\n",
        "\n",
        "function plot_db(model, ces; _xlab=\"Debt\", _ylab=\"Age\")\n",
        "\n",
        "    x = (ce -> ce.counterfactual).(ces) |> xs -> reduce(hcat, xs)\n",
        "    x0 = (ce -> ce.factual).(ces) |> xs -> reduce(hcat, xs)\n",
        "\n",
        "    xlab = _constraint[1] == \"both\" ? \"$(_xlab) (mutable)\" : \"$(_xlab) (immutable)\"\n",
        "    ylab = _constraint[2] == \"both\" ? \"$(_ylab) (mutable)\" : \"$(_ylab) (immutable)\"\n",
        "\n",
        "    # Data and decision boundary:\n",
        "    coeff = get_log_reg_params(model)\n",
        "    db = get_decision_boundary(coeff)\n",
        "    plt = Plots.scatter(\n",
        "        ce_data, \n",
        "        # label=[\"1=Default\" \"2=No Default\"], \n",
        "        # legend_position=:topright, \n",
        "        xlab=xlab, ylab=ylab,  \n",
        "        axis=nothing, \n",
        "        legend=false,\n",
        "        title=_title\n",
        "    )\n",
        "    Plots.abline!(plt, db.slope, db.intercept; lw=5, label=\"Dec. Boundary\")\n",
        "\n",
        "    # Counterfactuals:\n",
        "    if !any(isnothing.(x))\n",
        "        yhat = [argmax(y) for y in eachcol(model(x))]\n",
        "        yhat0 = [argmax(y) for y in eachcol(model(x0))]\n",
        "        if any(yhat.==2)\n",
        "\n",
        "            # Paths:\n",
        "            u = []\n",
        "            v = []\n",
        "            for (i,ce) in enumerate(eachcol(x))\n",
        "                Δ = ce - x0[:,i]\n",
        "                push!(u, Δ[1])\n",
        "                push!(v, Δ[2])\n",
        "            end\n",
        "            Plots.quiver!(x0[1,yhat.==2], x0[2,yhat.==2], quiver=(u[yhat.==2], v[yhat.==2]), color=1)\n",
        "            \n",
        "            # End points:\n",
        "            Plots.scatter!(x[1,yhat.==2], x[2,yhat.==2], label=[\"CE (y⁺=1)\" \"CE (y⁺=2)\"], ms=15, shape=:star, color=yhat[yhat.==2], group=yhat[yhat.==2], mscolor=yhat0[yhat.==2])\n",
        "\n",
        "        end\n",
        "    end\n",
        "    display(plt)\n",
        "end\n",
        "\n",
        "# Data:\n",
        "specs = [\n",
        "    (\"(a)\", VanillaObjective(needs_ce=true), [\"both\", \"both\"]),\n",
        "    (\"(b)\", EnergyDifferentialObjective(lambda=[1.0,1.0,0.0]), [\"both\", \"both\"]),\n",
        "    (\"(c)\", VanillaObjective(needs_ce=true), [\"both\", \"none\"]),\n",
        "    (\"(d)\", EnergyDifferentialObjective(lambda=[1.0,1.0,0.0]), [\"both\", \"none\"]),\n",
        "]"
      ],
      "id": "8d5e9682",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "using CounterfactualExplanations.Convergence\n",
        "using Random\n",
        "Random.seed!(42)\n",
        "\n",
        "models = []\n",
        "plts = []\n",
        "\n",
        "for (title, obj, constraint) in specs\n",
        "\n",
        "    # Globals for the callback:\n",
        "    global _title = title\n",
        "    global _constraint = constraint\n",
        "\n",
        "    # Data:\n",
        "    data = LinearlySeparable(\n",
        "        n_train=500,\n",
        "        batchsize=50,\n",
        "        mutability=constraint\n",
        "    )\n",
        "    global ce_data = get_ce_data(data)\n",
        "    val_size = data.n_validation / (data.n_validation + data.n_train)\n",
        "    train_set, val_set, _ = train_val_split(data, ce_data, val_size)\n",
        "\n",
        "    # Model:\n",
        "    nin = size(first(train_set)[1], 1)\n",
        "    nout = size(first(train_set)[2], 1)\n",
        "    model = build_model(LinearModel(), nin, nout)\n",
        "\n",
        "    # Objective:\n",
        "    generator = GenericGenerator()\n",
        "    opt_state = Flux.setup(Descent(), model)\n",
        "    conv = MaxIterConvergence()\n",
        "\n",
        "    model, logs = counterfactual_training(\n",
        "        obj,\n",
        "        model,\n",
        "        generator,\n",
        "        train_set,\n",
        "        opt_state;\n",
        "        val_set = val_set,\n",
        "        nepochs = 50,\n",
        "        mutability = Symbol.(constraint),\n",
        "        callback = plot_db,\n",
        "        nce=50,\n",
        "        convergence=conv, \n",
        "    )\n",
        "\n",
        "    push!(models, model)\n",
        "    push!(plts, current())\n",
        "end"
      ],
      "id": "ba6f1d05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt = Plots.plot(\n",
        "    plts..., \n",
        "    layout=(1,4), \n",
        "    size=(1150,250),  \n",
        "    left_margin = 10mm, \n",
        "    bottom_margin = 5mm,\n",
        "    top_margin = 3mm,\n",
        "    right_margin = 10mm,\n",
        ")\n",
        "display(plt)\n",
        "Plots.savefig(plt, \"paper/figures/poc.svg\")"
      ],
      "id": "3a1b48c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "plts = []\n",
        "titles = [\"(a)\", \"(b)\"]\n",
        "_xlab = \"Existing Debt\"\n",
        "_ylab = \"Age\"\n",
        "for (i,model) in enumerate(models)\n",
        "    coeff = get_log_reg_params(model)\n",
        "    db = get_decision_boundary(coeff)\n",
        "    xlab = constraints[i][1] == \"both\" ? \"$(_xlab) (mutable)\" : \"$(_xlab) (immutable)\"\n",
        "    ylab = constraints[i][2] == \"both\" ? \"$(_ylab) (mutable)\" : \"$(_ylab) (immutable)\"\n",
        "    plt = Plots.scatter(ce_data; xlab=xlab, ylab=ylab, bottom_margin = 5mm, left_margin = 5mm, label=[\"Default\" \"No Default\"], title=titles[i])\n",
        "    Plots.abline!(plt, db.slope, db.intercept; lw=5, label=\"Dec. Boundary\")\n",
        "\n",
        "    push!(plts, plt)\n",
        "end\n",
        "\n",
        "plt = Plots.plot(plts..., layout=(1,2), size=(600,300))\n",
        "Plots.savefig(plt, \"paper/figures/app_mtblty.svg\")"
      ],
      "id": "6e489ccd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.1",
      "path": "/Users/paltmeyer/Library/Jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}