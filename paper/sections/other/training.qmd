```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)

using CTExperiments
using CTExperiments.CSV
using CTExperiments.DataFrames
using CTExperiments.StatsBase
using Logging

using DotEnv
DotEnv.load!()
```

## Training Hyperparameters {#sec-app-training}

@nte-train-default presents the default hyperparameters used during training. 

::: {#nte-train-default .callout-note}

## Training Phase

```{julia}
#| output: asis

dict = Logging.with_logger(Logging.NullLogger()) do 
    CTExperiments.to_dict(Experiment()) 
end
println(CTExperiments.dict_to_quarto_markdown(dict, filter_empty=false))
```

:::

## Evaluation Details {#sec-app-eval}

For all of our evaluations, we proceed as follows: for each experiment setting we generate multiple counterfactuals ("No. Counterfactuals"), randomly choosing the factual and target class each time (@nte-eval-default). We do this across multiple rounds ("No. Runs") with different random seeds to account for stochasticity (@nte-eval-default). This is in line with standard practice in the related literature on CE. @nte-eval-default presents the default hyperparameters used during evaluation. For our final results presented in the main paper, we rely on held out test sets to sample factuals (and outputs for our performance metrics). For tuning purposes we rely on training or validation sets. 

### Robust Accuracy

To evaluate robust accuracy (Acc.$^*$), we use the Fast Gradient Sign Method (FGSM) to perturb test samples [@goodfellow2014explaining]. For the main results, we have set the perturbation size to $\epsilon=0.03$. We have also tested other perturbation sizes, as well as randomly perturbed data. Although not reported here, we have consistently found strong outperformance of CT compared to the weak baseline. 

::: {#nte-eval-default .callout-note}

## Evaluation Phase

```{julia}
#| output: asis

dict = Logging.with_logger(Logging.NullLogger()) do  
    CTExperiments.to_dict(EvaluationConfig(ExperimentGrid()))
end
println(CTExperiments.dict_to_quarto_markdown(dict, filter_empty=false))
```

:::