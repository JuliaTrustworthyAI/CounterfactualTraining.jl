---
execute: 
  eval: false
---

```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)
```

```{julia}
using CTExperiments
using CTExperiments.CounterfactualExplanations
using CTExperiments.CounterfactualTraining
using CTExperiments.Flux
using CTExperiments.Plots
using CTExperiments.TaijaPlotting
```

## Details on Encoding Mutability Constraints {#sec-app-constraints}

In @sec-constraints we explained that to avoid penalizing implausibility that arises due to mutability constraints for any feature $d$, we impose a point mass prior on $p(\mathbf{x})$ for that features. This is best explained through a simple example involving a binary linear classifier.

Let $\mathbf{M}_\theta(\mathbf{x})=\Theta\mathbf{x}$, with $\mathbf{x} \in \mathbb{R}^2$ and $y \in \{0,1\}$. Then for any given counterfactual with $y^+ \sim p(y)=1$ we have:

$$
\nabla_\theta \left(\text{div}(\mathbf{x},\mathbf{x^\prime},\mathbf{y};\theta)\right) = \nabla_\theta \left(-\mathbf{M}_\theta(\mathbf{x})[y^+]\right) - \nabla_\theta \left(-\mathbf{M}_\theta(\mathbf{x}^\prime)[y^+]\right) = \begin{pmatrix} 0 & 0 \\ \mathbf{x}^\prime[1] - \mathbf{x}[1] & \mathbf{x}^\prime[2] - \mathbf{x}[2] \end{pmatrix}
$$ {#eq-grad}

For the given counterfactual, contrastive divergence pushes down (up) on the coefficient $\Theta[y^+,d]$ by the amount that feature $d$ that is lower (higher) for counterfactuals than samples. This has the intended effect of aligning counterfactual explanations with the data, as discussed above. 

::: {#exm-grad}

Let feature $d=1$ in @eq-grad represents *age* and our model predicts the likelihood of credit default: $y \in \{0:=\text{'no default'},1:=\text{'default'}\}$. Then if $\mathbf{x}^\prime[1] < \mathbf{x}[1]$, contrastive divergence will push down on the corresponding coefficient, $\Theta[\text{'default'},\text{'age'}]$. Intuitively, this has the effect of decreasing the effect of *age* that the model attributes to the log odds of default, i.e. the model learns to predict higher default risk for younger individuals. 

If $\Theta[y^+,d]<0$ even in absence of this force, then contrastive divergence exacerbates the existing effect. But if *age* is immutable, then this corresponds to penalizing younger individuals for implausibility that arises due to a feature that they can change about themselves. This is precisely what we want to avoid, and therefore we impose $\mathbf{x}^\prime[1] - \mathbf{x}[1]:=0$ in such cases. In doing so, we induce lower sensitivity with respect to *age* compared to other mutable features (@fig-mtblty).

:::

![Visual illustration of the effect of imposing mutability constraints.](/paper/figures/app_mtblty.svg){#fig-mtblty}

::: {.callout-warning}

\@Cynthia, \@Arie, the above still needs to be polished. I'm not sure how to demonstrate/prove this last point analytically.

:::

```{julia}
using CTExperiments: 
    get_ce_data, 
    train_val_split, 
    build_model, 
    LinearModel, 
    get_input_encoder


# Callback:
using CTExperiments: get_log_reg_params, get_decision_boundary

function plot_db(model, x)
    coeff = get_log_reg_params(model)
    db = get_decision_boundary(coeff)
    plt = Plots.scatter(ce_data)
    yhat = [argmax(y) for y in eachcol(model(x))]
    Plots.scatter!(x[1,:], x[2,:], label="Counterfactuals", ms=15, shape=:star, color=yhat)
    Plots.abline!(plt, db.slope, db.intercept; lw=5, label="Dec. Boundary")
    display(plt)
end

# Data:
constraints = [
    ["both", "both"],
    ["none", "both"],
    ["both", "none"]
]
```

```{julia}
using CounterfactualExplanations.Convergence
using Random
Random.seed!(42)

models = []

for constraint in constraints
    # Data:
    data = LinearlySeparable(
        n_train=500,
        batchsize=50,
        mutability=constraint
    )
    ce_data = get_ce_data(data)
    val_size = data.n_validation / (data.n_validation + data.n_train)
    train_set, val_set, _ = train_val_split(data, ce_data, val_size)

    # Model:
    nin = size(first(train_set)[1], 1)
    nout = size(first(train_set)[2], 1)
    model = build_model(LinearModel(), nin, nout)

    # Objective:
    obj = EnergyDifferentialObjective(lambda=[1.0,1.0,0.0])
    generator = GenericGenerator()
    opt_state = Flux.setup(Descent(), model)
    conv = MaxIterConvergence()

    model, logs = counterfactual_training(
        obj,
        model,
        generator,
        train_set,
        opt_state;
        val_set = val_set,
        nepochs = 50,
        mutability = Symbol.(constraint),
        callback = plot_db,
        nce=50,
        convergence=conv, 
    )

    push!(models, model)
end
```

```{julia}
using Plots.PlotMeasures
plts = []
for (i,model) in enumerate(models)
    coeff = get_log_reg_params(model)
    db = get_decision_boundary(coeff)
    xlab = constraints[i][1] == "both" ? "mutable" : "immutable"
    ylab = constraints[i][2] == "both" ? "mutable" : "immutable"
    plt = Plots.scatter(ce_data; xlab=xlab, ylab=ylab, bottom_margin = 5mm, left_margin = 5mm)
    Plots.abline!(plt, db.slope, db.intercept; lw=5, label="Dec. Boundary")

    push!(plts, plt)
end

Plots.plot(plts..., layout=(1,3), size=(900,300))
```