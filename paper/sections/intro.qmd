# Introduction

Today's prominence of artificial intelligence (AI) has largely been driven by **representation learning**: instead of relying on features and rules that are carefully hand-crafted by humans, modern machine learning (ML) models are tasked with learning representations directly from data, guided by narrow objectives such as predictive accuracy [@goodfellow2016deep]. Modern advances in computing have made it possible to provide such models with ever-growing degrees of freedom to achieve that task, which frequently allows them to outperform traditionally more parsimonious models. Unfortunately, in doing so, models learn increasingly complex and highly sensitive representations that humans can no longer easily interpret.

The trend towards complexity for the sake of performance has come under serious scrutiny in recent years. At the very cusp of the deep learning (DL) revolution, @szegedy2013intriguing showed that artificial neural networks (ANN) are sensitive to adversarial examples (AEs): perturbed versions of data instances that yield vastly different model predictions despite being "imperceptible" in that they are semantically indifferent from their factual counterparts. Even though some partially effective mitigation strategies have been proposed---most notably **adversarial training** [@goodfellow2014explaining]---truly robust deep learning remains unattainable even for models that are considered "shallow" by today's standards [@kolter2023keynote]. 

Part of the problem is that the high degrees of freedom provide room for many solutions that are locally optimal with respect to narrow objectives [@wilson2020case].^[We follow the standard ML convention, where "degrees of freedom" refer to the number of parameters estimated from data.] Indeed, recent work on the so-called "lottery ticket hypothesis" suggests that modern neural networks can be pruned by up to 90% while preserving their predictive performance [@frankle2018lottery]. Similarly, @zhang2021understanding showed that state-of-the-art neural networks are expressive enough to fit randomly labeled data. Thus, looking at the predictive performance alone, the solutions may seem to provide compelling explanations for the data, when in fact they are based on purely associative, semantically meaningless patterns. This poses two challenges. Firstly, there is no dependable way to verify if representations correspond to meaningful, plausible explanations. Secondly, even if we could resolve the first challenge, it remains undecided how to ensure that models can *only* learn valuable explanations. 

The first challenge has attracted an abundance of research on **explainable AI** (XAI), a paradigm that focuses on the development of tools to derive (post-hoc) explanations from complex model representations. Such explanations should mitigate a scenario in which practitioners deploy opaque models and blindly rely on their predictions. On countless occasions, this has happened in practice and caused real harms to people who were adversely and unfairly affected by automated decision-making (ADM) systems involving opaque models (see, e.g., [@oneil2016weapons]). Effective XAI tools can aid us in monitoring models and providing recourse to individuals to turn negative outcomes (e.g., "loan application rejected") into positive ones (e.g., "application accepted"). In line with this, our work builds upon **counterfactual explanations** (CE) proposed by @wachter2017counterfactual as an effective approach to achieve this goal. CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce a desired output.

To our surprise, the second challenge has not yet attracted major research interest. Specifically, there has been no concerted effort towards improving the "explanatory capacity" of models, i.e., the degree to which learned representations correspond to explanations that are **interpretable** and deemed **plausible** by humans (see Def. \ref{def-explainability}). Instead, the choice has generally been to improve the ability of XAI tools to identify the subset of explanations that are both plausible and valid for any given model, independent of whether the learned representations are in fact compatible with plausible explanations [@altmeyer2024faithful]. Fortunately, recent findings indicate that improved explanatory capacity can arise as a consequence of regularization techniques aimed at other training objectives such as generative capacity, generalization, or robustness [@altmeyer2024faithful; @augustin2020adversarial; @schut2021generating]. As further discussed in @sec-lit, our work consolidates these findings within a single objective.

**Specifically, we introduce Counterfactual Training (CT)**: a novel training regime explicitly geared towards improving the explainability of models. In high-level terms, we define this concept as as the extent to which valid explanations derived for an opaque model are also deemed plausible with respect to the underlying data and the global actionability constraints. To the best of our knowledge, our framework represents the first attempt to address this challenge by employing counterfactual explanations already in the training phase.

 The remainder of this manuscript is structured as follows. @sec-lit presents related work, focusing on the link between AEs and CEs. Then follow our two principal contributions. In @sec-method, we introduce our methodological framework and show theoretically that it can be employed to enforce global actionability constraints. In @sec-experiments, through extensive experiments, we demonstrate that CT substantially improves explainability and positively contributes to the robustness of trained models without sacrificing predictive performance. Finally, in @sec-discussion, we discuss the challenges and conclude that CT is a promising approach towards making opaque models more trustworthy.

