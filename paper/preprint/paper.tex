% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Latin Modern Roman}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage[title]{appendix}
\usepackage{placeins}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{siunitx}
\sisetup{uncertainty-mode = separate}
\DeclareMathSizes{10}{9}{7}{6.5}
\usepackage{enumitem}
\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{placeins}
\usepackage{siunitx}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Counterfactual Training: Teaching Models Plausible and Actionable Explanations},
  pdfkeywords={Counterfactual Training, Counterfactual
Explanations, Algorithmic Recourse, Explainable AI, Representation
Learning},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\newcommand{\runninghead}{}
\renewcommand{\runninghead}{Counterfactual Training }
\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations\thanks{A Preprint.}}
\def\asep{\\\\\\ } % default: all authors on same column
\def\asep{\And }
\author{}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be plausible with
respect to the underlying data and actionable with respect to the
feature mutability constraints. Much existing research has therefore
focused on developing post-hoc methods to generate counterfactuals that
meet these desiderata. In this work, we instead hold models directly
accountable for the desired end goal: counterfactual training employs
counterfactuals during the training phase to minimize the divergence
between learned representations and plausible, actionable explanations.
We demonstrate empirically and theoretically that our proposed method
facilitates training models that deliver inherently desirable
counterfactual explanations and exhibit greatly improved adversarial
robustness.
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
Counterfactual Training \sep Counterfactual
Explanations \sep Algorithmic Recourse \sep Explainable AI \sep 
Representation Learning

\pagebreak

\twocolumn

\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by the success of representation learning with high degrees of
freedom: instead of relying on features and rules hand-crafted by
humans, modern machine learning (ML) models are tasked with learning
highly complex representations directly from the data, guided by narrow
objectives such as predictive accuracy
(\citeproc{ref-goodfellow2016deep}{Goodfellow, Bengio, and Courville
2016}). These models tend to be so complex that humans cannot easily
interpret their decision logic.

Counterfactual explanations (CE) have become a key part of the broader
explainable AI (XAI) toolkit
(\citeproc{ref-molnar2022interpretable}{Molnar 2022}) that can be
applied to make sense of this complexity. Originally proposed by
Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}), CEs prescribe minimal
changes for factual inputs that, if implemented, would prompt some
fitted model to produce an alternative, more desirable output. This is
useful and necessary to not only understand how opaque models make their
predictions, but also to provide algorithmic recourse (AR) to
individuals subjected to them: a retail bank, for example, could use CE
to provide meaningful feedback to unsuccessful loan applicants that were
rejected based on an opaque automated decision-making (ADM) system
(Figure~\ref{fig-poc}).

For such feedback to be meaningful, counterfactual explanations need to
fulfill certain desiderata (\citeproc{ref-verma2020counterfactual}{Verma
et al. 2022}; \citeproc{ref-karimi2020survey}{Karimi et al.
2021})---they should be faithful to the model
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}), plausible
(\citeproc{ref-joshi2019realistic}{Joshi et al. 2019}) and actionable
(\citeproc{ref-ustun2019actionable}{Ustun, Spangher, and Liu 2019}).
Plausibility is typically understood as counterfactuals being
\emph{in-domain}: unsuccessful loan applicants that implement the
provided recourse should end up with credit profiles that are genuinely
similar to that of individuals who have successfully repaid their loans
in the past. Actionable explanations comply with practical constraints:
a young, unsuccessful loan applicant cannot increase their age in an
instance.

Existing state-of-the-art (SOTA) approaches in the field have largely
focused on designing model-agnostic CE methods that identify subsets of
counterfactuals, which comply with specific desiderata. This is
problematic, because the narrow focus on any specific desideratum can
adversely affect others: it is possible, for example, to generate
plausible counterfactuals for models that are also highly vulnerable to
implausible, possibly adversarial counterfactuals
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}). In this
work, we therefore embrace the paradigm that models (as opposed to
explanation methods) should be held accountable for explanations that
are plausible and actionable. While previous work has shown that at
least plausibility can be indirectly achieved through existing
techniques aimed at models' generative capacity, generalization and
robustness (\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024};
\citeproc{ref-augustin2020adversarial}{Augustin, Meinke, and Hein 2020};
\citeproc{ref-schut2021generating}{Schut et al. 2021}), we directly
incorporate both plausibility and actionability in the training
objective of models to improve their overall explanatory capacity.

Specifically, we propose \textbf{counterfactual training (CT)}: a novel
training regime that leverages counterfactual explanations on-the-fly to
ensure that differentiable models learn plausible and actionable
explanations for the underlying data, while at the same time also being
more robust to adversarial examples (AE). Figure~\ref{fig-poc}
illustrates the outcomes of CT compared to a conventionally trained
model. First, in panel (a), faithful and valid counterfactuals end up
near the decision boundary forming a clearly distinguishable cluster in
the target class (orange). In panel (b), CT is applied to the same
underlying linear classifier architecture resulting in much more
plausible counterfactuals. In panel (c), the classifier is again trained
conventionally and we have introduced a mutability constraint on the
\emph{age} feature at test time---counterfactuals are valid but the
classifier is roughly equally sensitive to both features. By contrast,
the decision boundary in panel (d) has titled, making the model trained
with CT relatively less sensitive to the immutable \emph{age} feature.
To achieve these outcomes, CT draws inspiration from the literature on
contrastive and robust learning: we contrast faithful CEs with
ground-truth data while protecting immutable features, and capitalize on
methodological links between CE and AE by penalizing the model's
adversarial loss on interim (\emph{nascent}) counterfactuals. To the
best of our knowledge, CT represents the first venture in this direction
with promising empirical and theoretical results.

The remainder of this manuscript is structured as follows.
Section~\ref{sec-lit} presents related work, focusing on the links to
contrastive and robust learning. Then follow our two principal
contributions. In Section~\ref{sec-method}, we introduce our
methodological framework and show theoretically that it can be employed
to respect global actionability constraints. In our experiments
(Section~\ref{sec-experiments}), we find that thanks to counterfactual
training, (1) the implausibility of CEs decreases by up to 90\%; (2) the
cost of reaching valid counterfactuals with protected features decreases
by 19\% on average; and (3) models' adversarial robustness improves
across the board. Finally, we discuss open challenges in
Section~\ref{sec-discussion} and conclude in
Section~\ref{sec-conclusion}.

\begin{figure*}

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}

}

\caption{\label{fig-poc}Counterfactual explanations (stars) for linear
classifiers trained under different regimes on synthetic data: (a)
conventional training, all mutable; (b) CT, all mutable; (c)
conventional, \emph{age} immutable; (d) CT, \emph{age} immutable. The
linear decision boundary is shown in green along with training data
colored according to ground-truth labels: \(y^-=\text{"loan withheld"}\)
(blue) and \(y^+=\text{"loan provided"}\) (orange). Class and feature
annotations (\emph{debt} and \emph{age}) are for illustrative purposes.}

\end{figure*}%

\section{Related Literature}\label{sec-lit}

To make the desiderata for our framework more concrete, we follow
previous work in tying the explanatory capacity of models to the quality
of CEs that can be generated for them
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024};
\citeproc{ref-augustin2020adversarial}{Augustin, Meinke, and Hein
2020}). For simplicity, we refer to ``explanatory capacity'' as
``explainability'' in the rest of this manuscript (see Def.
\ref{def-explainability}).

\subsection{Explainability and Contrastive
Learning}\label{explainability-and-contrastive-learning}

In a closely related work, Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}) show that model averaging
and, in particular, contrastive model objectives can produce more
explainable and hence trustworthy models. The authors propose a way to
generate counterfactuals that are maximally faithful in that they are
consistent with what models have learned about the underlying data.
Formally, they rely on tools from energy-based modelling
(\citeproc{ref-teh2003energy}{Teh et al. 2003}) to minimize the
contrastive divergence between the distribution of counterfactuals and
the conditional posterior over inputs learned by a model. Their
algorithm, \emph{ECCCo}, yields plausible counterfactual explanations if
and only if the underlying model has learned representations that align
with them. The authors find that both deep ensembles
(\citeproc{ref-lakshminarayanan2016simple}{Lakshminarayanan, Pritzel,
and Blundell 2017}) and joint energy-based models (JEMs)
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}), a form of
constrastive learning, tend to do well in this regard.

It helps to look at these findings through the lens of representation
learning with high degrees of freedom. Deep ensembles are approximate
Bayesian model averages, which are particularly effective when models
are underspecified by the available data
(\citeproc{ref-wilson2020case}{Wilson 2020}). Averaging across solutions
mitigates the risk of overrelying on a single locally optimal
representation that corresponds to semantically meaningless
explanations. Likewise, previous work of Schut et al.
(\citeproc{ref-schut2021generating}{2021}) found that generating
plausible (``interpretable'') CEs is almost trivial for deep ensembles
that have undergone adversarial training. The case for JEMs is even
clearer: they optimize a hybrid objective that induces both high
predictive performance and strong generative capacity
(\citeproc{ref-grathwohl2020your}{Grathwohl et al. 2020}), which
resembles the idea of aligning models with plausible explanations and
has inspired CT.

\subsection{Explainability and Robust
Learning}\label{explainability-and-robust-learning}

Augustin, Meinke, and Hein
(\citeproc{ref-augustin2020adversarial}{2020}) show that CEs tend to be
more meaningful (``explainable'') if the underlying model is more robust
to adversarial examples. Once again, we can make intuitive sense of this
finding if we look at adversarial training (AT) through the lens of
representation learning with high degrees of freedom: highly complex and
flexible models may learn representations that make them sensitive to
implausible or even adversarial examples
(\citeproc{ref-szegedy2013intriguing}{Szegedy et al. 2014}). Thus, by
inducing models to ``unlearn'' susceptibility to such examples,
adversarial training can effectively remove implausible explanations
from the solution space.

This interpretation of the link between explainability through
counterfactuals on the one side, and robustness to adversarial examples
on the other is backed by empirical evidence. Sauer and Geiger
(\citeproc{ref-sauer2021counterfactual}{2021}) demonstrate that using
counterfactual images during classifier training improves model
robustness. Similarly, Abbasnejad et al.
(\citeproc{ref-abbasnejad2020counterfactual}{2020}) argue that
counterfactuals represent potentially useful training data in machine
learning, especially in supervised settings where inputs may be
reasonably mapped to multiple outputs. They, too, show that augmenting
the training data of (image) classifiers can improve generalization
performance. Finally, Teney, Abbasnedjad, and Hengel
(\citeproc{ref-teney2020learning}{2020}) argue that counterfactual pairs
tend to exist in training data. Hence, their approach aims to identify
similar input samples with different annotations and ensure that the
gradient of the classifier aligns with the vector between such pairs of
counterfactual inputs using a cosine distance loss function.

CEs have also been used to improve models in the natural language
processing domain. For example, Wu et al.
(\citeproc{ref-wu2021polyjuice}{2021}) propose \emph{Polyjuice}, a
general-purpose CE generator for language models and demonstrate that
the augmentation of training data with \emph{Polyjuice} improves
robustness in a number of tasks, while Luu and Inoue
(\citeproc{ref-luu2023counterfactual}{2023}) introduce the
\emph{Counterfactual Adversarial Training} (CAT) framework that aims to
improve generalization and robustness of language models by generating
counterfactuals for training samples that are subject to high predictive
uncertainty.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples. Pointing
to clear similarities in how CEs and AEs are generated, Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}) makes the case for
jointly studying the opaqueness and robustness problems in
representation learning. Formally, AEs can be seen as the subset of CEs
for which misclassification is achieved
(\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022}). Similarly,
Pawelczyk et al. (\citeproc{ref-pawelczyk2022exploring}{2022}) show that
CEs and AEs are equivalent under certain conditions.

Two other works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of the post-hoc counterfactual explanations. Firstly,
Ross, Lakkaraju, and Bastani (\citeproc{ref-ross2021learning}{2024})
propose a way to train models that guarantee recourse to a positive
target class with high probability. Their approach builds on adversarial
training by explicitly inducing susceptibility to targeted AEs for the
positive class. Additionally, the method allows for imposing a set of
actionability constraints ex-ante. For example, users can specify that
certain features are immutable. Secondly, Guo, Nguyen, and Yadav
(\citeproc{ref-guo2023counternet}{2023}) are the first to propose an
end-to-end training pipeline that includes CEs as part of the training
procedure. Their \emph{CounterNet} network architecture includes a
predictor and a CE generator, where the parameters of the CE generator
are learnable. Counterfactuals are generated during each training
iteration and fed back to the predictor. In contrast, we impose no
restrictions on the ANN architecture at all.

\section{Counterfactual Training}\label{sec-method}

This section introduces the counterfactual training framework, applying
ideas from contrastive and robust learning to counterfactual
explanations. CT produces models whose learned representations align
with plausible explanations that comply with user-defined actionability
constraints.

Counterfactual explanations are typically generated by solving
variations of the following optimization problem,
\begin{equation}\phantomsection\label{eq-general}{
\begin{aligned}
\min_{\mathbf{X}^\prime \in \mathcal{X}^D} \left\{  {\text{yloss}(\mathbf{M}_\theta(\mathbf{x}^{\prime}),\mathbf{y}^+)}+ \lambda {\text{reg}(\mathbf{x}^{\prime}) }  \right\} 
\end{aligned}
}\end{equation} where
\(\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}\) denotes a
classifier, \(\mathbf{x}^{\prime}\) denotes the counterfactual with
\(D\) features and \(\mathbf{y}^+\in\mathcal{Y}\) denotes some target
class. The \(\text{yloss}(\cdot)\) function quantifies the discrepancy
between current model predictions for \(\mathbf{x}^{\prime}\) and the
target class (a conventional choice is cross-entropy). Finally, we use
\(\text{reg}(\cdot)\) to denote any form of regularization used to
induce certain properties on the counterfactual. In their seminal paper,
Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) propose regularizing
the distance between counterfactuals and their original factual values
to ensure that individuals seeking recourse through CE face minimal
costs in terms of feature changes. Different variations of
Equation~\ref{eq-general} have been proposed in the literature to
address many desiderata including the ones discussed above
(faithfulness, plausibility and actionability). Like Wachter,
Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}), most of these
approaches rely on gradient descent to optimize
Equation~\ref{eq-general}. For more details on the approaches tested in
this work, we refer the reader to the supplementary appendix. In the
following, we describe in detail how counterfactuals are generated and
used in counterfactual training.

\subsection{Proposed Training
Objective}\label{proposed-training-objective}

The goal of CT is to improve model explainability by aligning models
with faithful explanations that are plausible and actionable. Formally,
we define explainability as follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$, where $\arg\max_y{\mathbf{y}^{+}}=y^+$ is the index of the target class. 

We say that $\mathbf{M}_\theta$ has an \textbf{explanatory capacity} to the extent that faithfully generated, valid counterfactuals are also plausible and actionable. We define these properties as:

\begin{itemize}
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ is an arbitrarily small region around $\mathbf{x}^{\prime}$.
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ as specified above.
    \item (Actionability) Perturbations $\Delta$ may be subject to some actionability constraints.
\end{itemize}
Here, $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior distribution over inputs. For simplicity, we refer to a model with high explanatory capacity as \textbf{explainable} in this manuscript. 
\end{definition}

The characterization of faithfulness and plausibility in Def.
\ref{def-explainability} follows Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}), with adapted notation.
Intuitively, plausible counterfactuals are consistent with the data and
faithful counterfactuals are consistent with what the model has learned
about the input data. Ac tionability constraints in Def.
\ref{def-explainability} vary and depend on the context in which
\(\mathbf{M}_\theta\) is deployed. In this work, we choose to only
consider domain and mutability constraints for individual features
\(x_d\) for \(d=1,...,D\). We also limit ourselves to classification
tasks for reasons discussed in Section~\ref{sec-discussion}.

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
generated through gradient descent over \(T\) iterations as originally
proposed by Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}). CT adopts
gradient-based CE search in training to generate on-the-fly model
explanations \(\mathbf{x}^\prime\) for the training samples. We use the
term \emph{nascent} to denote interim counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) that have not yet converged. As we
explain below, these nascent counterfactuals can be stored and
repurposed as adversarial examples. Conversely, we consider
counterfactuals \(\mathbf{x}_T^\prime\) as \emph{mature} explanations if
they have either exhausted all \(T\) iterations or converged by reaching
a pre-specified threshold, \(\tau\), for the predicted probability of
the target class:
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.

Formally, we propose the following counterfactual training objective to
train explainable (as in Def. \ref{def-explainability}) models,
\begin{equation}\phantomsection\label{eq-obj}{
\begin{aligned}
&\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) \\+ &\lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) + \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{aligned}
}\end{equation} where \(\text{yloss}(\cdot)\) is any classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms are explained in detail below. For now, they can
be summarized as inducing explainability directly and indirectly by
penalizing (1) the contrastive divergence, \(\text{div}(\cdot)\),
between mature counterfactuals \(\mathbf{x}_T^\prime\) and observed
samples \(\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}\) in the
target class \(y^+\), and (2) the adversarial loss,
\(\text{advloss}(.)\), wrt. nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\). Finally, \(\text{ridge}(\cdot)\)
denotes a Ridge penalty (\(\ell_2\)-norm) that regularizes the magnitude
of the energy terms involved in \(\text{div}(\cdot)\)
(\citeproc{ref-du2019implicit}{Du and Mordatch 2020}). The trade-offs
between these components are adjusted through \(\lambda_{\text{div}}\),
\(\lambda_{\text{adv}}\) and \(\lambda_{\text{reg}}\). The full training
regime is sketched out in Algorithm \ref{alg-experiment}.

\begin{algorithm}[h]
  \caption{Counterfactual Training}
    \label{alg-experiment}
    \begin{algorithmic}[1]
    \REQUIRE Training dataset $\mathcal{D}$, initialize model $\mathbf{M}_{\theta}$
    \WHILE{not converged}
        \STATE Sample $\mathbf{x}$ and $\mathbf{y}$ from dataset $\mathcal{D}$.
        \STATE Sample $\mathbf{x}^{\prime}_0$, $\mathbf{y}^+$ and $\mathbf{x}^+$.
        \FOR{$t = 1$ to $T$}
            \STATE Backpropagate $\nabla_{\mathbf{x}^\prime}$ through Equation \ref{eq-general}. Store $\mathbf{x}_t^\prime$.
        \ENDFOR
        \STATE Backpropagate $\nabla_{\theta}$ through Equation \ref{eq-obj}.
    \ENDWHILE
    \RETURN $\mathbf{M}_\theta$
    \end{algorithmic}
\end{algorithm}

\subsection{Directly Inducing Explainability with Contrastive
Divergence}\label{directly-inducing-explainability-with-contrastive-divergence}

Grathwohl et al. (\citeproc{ref-grathwohl2020your}{2020}) observe that
any classifier can be re-interpreted as a joint energy-based model that
learns to discriminate output classes conditional on the observed
(training) samples from \(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). The authors show that JEMs can be trained to
perform well at both tasks by directly maximizing the joint
log-likelihood:
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\),
where the first term can be optimized using cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\), they
minimize the contrastive divergence between the observed samples from
\(p(\mathbf{x})\) and samples generated from \(p_\theta(\mathbf{x})\).

To generate samples, Grathwohl et al.
(\citeproc{ref-grathwohl2020your}{2020}) use Stochastic Gradient
Langevin Dynamics (SGLD) with an uninformative prior for initialization
but we depart from their methodology: we propose to leverage
counterfactual explainers to generate counterfactuals of observed
training samples. Specifically, we have:
\begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation} where \(\mathcal{E}_\theta(\cdot)\) denotes the energy
function defined as
\(\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]\),
with \(y^+\) denoting the index of the randomly drawn target class,
\(y^+ \sim p(y)\). Conditional on the target class \(y^+\),
\(\mathbf{x}_T^\prime\) denotes a mature counterfactual for a randomly
sampled factual from a non-target class generated with a gradient-based
CE generator for up to \(T\) iterations. Intuitively, the gradient of
Equation~\ref{eq-div} decreases the energy of observed training samples
(positive samples) while increasing the energy of counterfactuals
(negative samples) (\citeproc{ref-du2019implicit}{Du and Mordatch
2020}). As the counterfactuals get more plausible (Def.
\ref{def-explainability}) during training, these opposing effects
gradually balance each other out (\citeproc{ref-lippe2024uvadlc}{Lippe
2024}).

Since maturity of counterfactuals in terms of a probability threshold is
often reached before \(T\), this form of sampling is not only more
closely aligned with Def. \ref{def-explainability}., but can also speed
up training times compared to SGLD. The departure from SGLD also allows
us to tap into the vast repertoire of explainers that have been proposed
in the literature to meet different desiderata. For example, many
methods support domain and mutability constraints. In principle, any
existing approach for generating CEs is viable, so long as it does not
violate the faithfulness condition. Like JEMs
(\citeproc{ref-murphy2022probabilistic}{Murphy 2022}), counterfactual
training can be considered a form of contrastive representation
learning.

\subsection{Indirectly Inducing Explainability with Adversarial
Robustness}\label{indirectly-inducing-explainability-with-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
(\citeproc{ref-balashankar2023improving}{Balashankar et al. 2023};
\citeproc{ref-luu2023counterfactual}{Luu and Inoue 2023}) or adversarial
examples (\citeproc{ref-freiesleben2022intriguing}{Freiesleben 2022};
\citeproc{ref-pawelczyk2022exploring}{Pawelczyk et al. 2022}). This
leaves some flexibility with regards to the choice for the
\(\text{advloss}(\cdot)\) term in Equation~\ref{eq-obj}. An intuitive
functional form, but likely not the only sensible choice, is inspired by
adversarial training: \begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation} Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as AEs as long as the magnitude of the
perturbation to any single feature is at most \(\varepsilon\). This is
closely aligned with Szegedy et al.
(\citeproc{ref-szegedy2013intriguing}{2014}) who define an adversarial
attack as an ``imperceptible non-random perturbation''. Thus, we work
with a different distinction between CE and AE than Freiesleben
(\citeproc{ref-freiesleben2022intriguing}{2022}) who considers
misclassification as the distinguishing feature of adversarial examples.
One of the key observations of this work is that we can leverage CEs
during training and get AEs essentially for free to reap the
aforementioned benefits of adversarial training.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints. In fact, both types of constraints can be implemented for
any explainer that relies on gradient descent in the feature space for
optimization (\citeproc{ref-altmeyer2023explaining}{Altmeyer, Deursen,
and Liem 2023}). In this context, domain constraints can be imposed by
simply projecting counterfactuals back to the specified domain, if the
previous gradient step resulted in updated feature values that were
out-of-domain. Similarly, mutability constraints can be enforced by
setting partial derivatives to zero to ensure that features are only
perturbed in the allowed direction, if at all.

Since actionability constraints are binding at test time, we also impose
them when generating \(\mathbf{x}^\prime\) during each training
iteration to inform model representations. Through their effect on
\(\mathbf{x}^\prime\), both types of constraints influence model
outcomes via Equation~\ref{eq-div}. Here it is crucial that we avoid
penalizing implausibility that arises due to mutability constraints. For
any mutability-constrained feature \(d\) this can be achieved by
enforcing \(\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0\) whenever
perturbing \(\mathbf{x}^\prime[d]\) in the direction of
\(\mathbf{x}^+[d]\) would violate mutability constraints. Specifically,
we set \(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  \(\mathbf{x}^+[d]>\mathbf{x}^\prime[d]\), but \(d\) can only be
  decreased in practice.
\item
  \(\mathbf{x}^+[d]<\mathbf{x}^\prime[d]\), but \(d\) can only be
  increased in practice.
\end{enumerate}

\noindent From a Bayesian perspective, setting
\(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) can be understood as
assuming a point mass prior for \(p(\mathbf{x}^+)\) wrt. feature \(d\).
Intuitively, we think of this as ignoring implausibility costs of
immutable features, which effectively forces the model to instead seek
plausibility through the remaining features. This can be expected to
result in relatively lower sensitivity to immutable features; and higher
relative sensitivity to mutable features should make
mutability-constrained recourse less costly
(Section~\ref{sec-experiments}). Under certain conditions, this result
holds theoretically; for the proof, see the supplementary appendix:

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. Assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}

\section{Experiments}\label{sec-experiments}

We seek to answer the following four research questions:

\begin{enumerate}[label={(\makebox[2em][c]{RQ\arabic*})}, leftmargin=3.5em]
    \item To what extent does the CT objective in Equation 1 induce models to learn plausible explanations?
    \item To what extent does CT result in more favorable algorithmic recourse outcomes in the presence of actionability constraints?
    \item To what extent does CT influence the adversarial robustness of trained models?
    \item What are the effects of hyperparameter selection on counterfactual training?
\end{enumerate}

\subsection{Experimental Setup}\label{experimental-setup}

Our focus is the improvement in explainability (Def.
\ref{def-explainability}). Thus, we primarily look at the plausibility
and cost of faithfully generated counterfactuals at test time. Other
metrics, such as validity and redundancy, are reported in the
supplementary appendix. To measure the cost, we follow the standard
proxy of distances (\(\ell_1\)-norm) between factuals and
counterfactuals. For plausibility, we assess how similar CEs are to
observed samples in the target domain,
\(\mathbf{X}^+\subset\mathcal{X}^+\). We rely on the metric used by
Altmeyer et al. (\citeproc{ref-altmeyer2024faithful}{2024}),
\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{IP}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation} and introduce a novel divergence-based adaptation,
\begin{equation}\phantomsection\label{eq-impl-div}{
\text{IP}^*(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation} where \(\mathbf{X}^\prime\) denotes a collection of
counterfactuals and \(\text{MMD}(\cdot)\) is the unbiased estimate of
the squared population maximum mean discrepancy, proposed by Gretton et
al. (\citeproc{ref-gretton2012kernel}{2012}). The metric in
Equation~\ref{eq-impl-div} is equal to zero if and only if the two
distributions are exactly the same, \(\mathbf{X}^\prime=\mathbf{X}^+\).

To assess outcomes with respect to actionability for non-linear models,
we look at the costs of (just) valid counterfactuals in terms of their
distances from factual starting points with \(\tau=0.5\). While this an
imperfect proxy of sensitivity, we hypothesize that CT can reduce these
costs by teaching models to seek plausibility with respect to mutable
features, much like we observe in Figure~\ref{fig-poc} in panel (d)
compared to (c). We supplement this analysis with estimates for
integrated gradients (\citeproc{ref-sundararajan2017ig}{Sundararajan,
Taly, and Yan 2017}). Finally, for predictive performance, we use
standard metrics, such as robust accuracy estimated on adversarially
perturbed data using FGSM
(\citeproc{ref-goodfellow2014explaining}{Goodfellow, Shlens, and Szegedy
2015}).

We run experiments with three gradient-based generators: \emph{Generic}
of Wachter, Mittelstadt, and Russell
(\citeproc{ref-wachter2017counterfactual}{2017}) as a simple baseline
approach, \emph{REVISE} (\citeproc{ref-joshi2019realistic}{Joshi et al.
2019}) that aims to generate plausible counterfactuals using a surrogate
Variational Autoencoder (VAE), and \emph{ECCCo}
(\citeproc{ref-altmeyer2024faithful}{Altmeyer et al. 2024}), which
targets faithfulness.

We make use of nine classification datasets common in the CE/AR
literature. Four of them are synthetic with two classes and different
characteristics: linearly separable clusters (\emph{LS}), overlapping
clusters (\emph{OL}), concentric circles (\emph{Circ}), and interlocking
moons (\emph{Moon}). Next, we have four real-world binary tabular
datasets: \emph{Adult} (Census data) of Becker and Kohavi
(\citeproc{ref-becker1996adult2}{1996}), California housing (\emph{CH})
of Pace and Barry (\citeproc{ref-pace1997sparse}{1997}), Default of
Credit Card Clients (\emph{Cred}) of Yeh
(\citeproc{ref-yeh2016default}{2016}), and Give Me Some Credit
(\emph{GMSC}) from Kaggle (\citeproc{ref-kaggle2011give}{2011}).
Finally, for the convenience of illustration, we use the 10-class
\emph{MNIST} (\citeproc{ref-lecun1998mnist}{LeCun 1998}).

To assess CT, we investigate the improvements in performance metrics
when using it on top of a weak baseline (BL): a multilayer perceptron
(\emph{MLP}). This is the best way to get a clear picture of the
effectiveness of CT, and it is consistent with evaluation practices in
the related literature
(\citeproc{ref-goodfellow2014explaining}{Goodfellow, Shlens, and Szegedy
2015}; \citeproc{ref-ross2021learning}{Ross, Lakkaraju, and Bastani
2024}; \citeproc{ref-teney2020learning}{Teney, Abbasnedjad, and Hengel
2020}).

\subsection{Experimental Results}\label{experimental-results}

Our main results for plausibility and actionability for \emph{MLP}
models are summarised in Table~\ref{tbl-main} that presents
counterfactual outcomes grouped by dataset along with standard errors
averaged across bootstrap samples. Asterisks (\(^*\)) are used when the
bootstrapped 99\%-confidence interval of differences in mean outcomes
does \emph{not} include zero, so the observed effects are statistically
significant at the 0.01 level.

The first two columns (\(\text{IP}\) and \(\text{IP}^*\)) show the
percentage reduction in implausibility for our two metrics when using CT
on top of the weak baseline. As an example, consider the first row for
\emph{LS} data: the observed positive values indicate that faithful
counterfactuals are around 30-55\% more plausible for models trained
with CT, in line with our observations in panel (b) of
Figure~\ref{fig-poc} compared to panel (a).

The third column shows the results for a scenario when mutability
constraints are imposed on the selected features. Again, we are
comparing CT to the baseline, so reductions in the positive direction
imply that valid counterfactuals are ``cheaper'' (more actionable) when
using CT with feature protection. Relating this back to
Figure~\ref{fig-poc}, the third column represents the reduction in
distances travelled by counterfactuals in panel (d) compared to panel
(c). In the following paragraphs, we summarize the results for all
datasets.

\begin{table}

\caption{\label{tbl-main}Key evaluation metrics for valid counterfactual
along with bootstrapped standard errors for all datasets.
\textbf{Plausibility} (columns 1-2): percentage reduction in
implausibility for \(\text{IP}\) and \(\text{IP}^*\), respectively;
\textbf{Cost} / \textbf{Actionability} (column 3): percentage reduction
in costs when selected features are protected. Outcomes are aggregated
across bootstrap samples (100 rounds) and varying degrees of the energy
penalty \(\lambda_{\text{egy}}\) used for \emph{ECCCo} at test time.
Asterisks (\(^*\)) indicate that the bootstrapped 99\%-confidence
interval of differences in mean outcomes does \emph{not} include zero.}

\centering{

\small
\centering
\begin{tabular}{
  l
  S[table-format=2.2(1.2)]
  S[table-format=3.2(3.2)]
  S[table-format=3.2(1.2)]
}
  \toprule
  \textbf{Data} & \textbf{$ \text{IP} $ $(-\%)$} & \textbf{$ \text{IP}^* $ $(-\%)$} & \textbf{Cost $(-\%)$} \\\midrule
  LS & 29.05\pm0.67 $^{*}$ & 55.33\pm2.03 $^{*}$ & 14.07\pm0.6 $^{*}$ \\
  Circ & 56.29\pm0.44 $^{*}$ & 89.38\pm9.3 $^{*}$ & 45.55\pm0.76 $^{*}$ \\
  Moon & 20.62\pm0.69 $^{*}$ & 19.26\pm8.12 $^{*}$ & 2.86\pm1.03 $^{*}$ \\
  OL & -1.13\pm0.88 $^{}$ & -24.52\pm14.52 $^{}$ & 38.39\pm2.21 $^{*}$ \\\midrule
  Adult & 0.77\pm1.34 $^{}$ & 32.29\pm6.87 $^{*}$ & -2.82\pm4.88 $^{}$ \\
  CH & 12.05\pm1.41 $^{*}$ & 70.27\pm3.72 $^{*}$ & 40.71\pm1.55 $^{*}$ \\
  Cred & 12.31\pm1.84 $^{*}$ & 54.89\pm11.21 $^{*}$ & -17.43\pm5.17 $^{*}$ \\
  GMSC & 23.44\pm1.99 $^{*}$ & 73.31\pm4.83 $^{*}$ & 62.64\pm2.04 $^{*}$ \\
  MNIST & 7.05\pm1.8 $^{*}$ & -25.09\pm109.05 $^{}$ & -12.34\pm6.52 $^{}$ \\\midrule
  Avg. & 17.83 & 38.35 & 19.07 \\\bottomrule
\end{tabular}

}

\end{table}%

\subsubsection{Plausibility (RQ1).}\label{sec-plaus}

\emph{CT generally produces substantial and statistically significant
improvements in plausibility.}

Average reductions in \(\text{IP}\) range from around 7\% for
\emph{MNIST} to almost 60\% for \emph{Circ}. For the real-world tabular
datasets they are around 12\% for \emph{CH} and \emph{Cred} and almost
25\% for \emph{GMSC}; for \emph{Adult} and \emph{OL} we find no
significant impact of CT on \(\text{IP}\). Reductions in \(\text{IP}^*\)
are even more substantial and generally statistically significant,
although the average degree of uncertainty is higher than for
\(\text{IP}\): reductions range from around 20\% (\emph{Moons}) to
almost 90\% (\emph{Circ}). The only negative findings are for \emph{OL}
and \emph{MNIST}, but they are insignificant. A qualitative inspection
of the counterfactuals in Figure~\ref{fig-mnist-ce} suggests
recognizable digits for the model trained with CT (bottom row), unlike
the baseline (top row).

\begin{figure}

\begin{minipage}{\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_ce.png}}

}

\subcaption{\label{fig-mnist-ce}\emph{Plausibility}: \emph{ECCCo}
counterfactuals for factual `0' (blue).}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_ig.png}}

}

\subcaption{\label{fig-mnist-ig}\emph{Actionability}: class-cond.
integrated gradients.}

\end{minipage}%

\caption{\label{fig-mnist}Visual explanations for \emph{MNIST}. Top and
bottom rows (of images) show the results for BL and CT, respectively.
Mutability constraints are imposed on the five top and bottom rows (of
pixels). CT produces plausible counterfactuals (a) and is less sensitive
to protected features (b).}

\end{figure}%

\subsubsection{Actionability (RQ2).}\label{sec-act}

\emph{CT tends to improve actionability in the presence of immutable
features, but this is not guaranteed if the assumptions in Proposition
\ref{prp-mtblty} are violated.}

For synthetic datasets, we always protect the first feature; for all
real-world tabular datasets we could identify and protect an \emph{age}
variable; for \emph{MNIST}, we protect the five top and bottom pixel
rows of the full image. Statistically significant reductions in costs
overwhelmingly point in the positive direction reaching up to around
60\% for \emph{GMSC}. Only in the case of \emph{Cred}, average costs
increase, likely because any benefits from protecting \emph{age} are
outweighed by an increase in costs required for greater plausibility.
The findings for \emph{Adult} and \emph{MNIST} are insignificant. A
qualitative inspection of the class-conditional integrated gradients in
Figure~\ref{fig-mnist-ig} suggests that CT still has the expected
effect: the model (bottom) is less sensitive (blue) to the protected
rows of pixels; details are reported in the appendix.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/acc.png}}

}

\caption{\label{fig-acc}Test accuracies on adversarially perturbed data
with varying perturbation sizes for all non-synthetic datasets.}

\end{figure*}%

\subsubsection{Predictive Performance (RQ3).}\label{sec-pred}

\emph{Models trained with CT are substantially more robust to
gradient-based adversarial attacks than conventionally-trained
baselines.}

Test accuracies on adversarially perturbed data are shown in
Figure~\ref{fig-acc}. The perturbations size, \(\varepsilon\in[0,0.1]\),
increases along the horizontal axis and includes zero, corresponding to
standard test accuracy for non-perturbed data. For all synthetic
datasets, predictive performance of CT is virtually identical to the
baseline and unaffected by perturbations. For all real-world datasets,
we find that CT substantially improves robustness: while in some cases
baseline accuracies drop to essentially zero for large enough
perturbation sizes, accuracies of CT models remain remarkably robust.

\subsubsection{Hyperparameter settings
(RQ4).}\label{sec-hyperparameters}

\emph{CT is highly sensitive to the choice of a CE generator and its
hyperparameters but (1) we observe manageable patterns, and (2) we can
usually identify settings that improve either plausibility or
actionability, and typically both of them at the same time.}

We evaluate the impacts of three types of hyperparameters on CT. In this
section we focus on the highlights and make the full results available
in the supplementary appendix.

Firstly, we find that optimal results are generally obtained when using
\emph{ECCCo} to generate counterfactuals. Conversely, using a generator
that may inhibit faithfulness (\emph{REVISE}), tends to yield poor
results. Concerning hyperparameters that guide the gradient-based
counterfactual search, we find that increasing \(T\), the maximum number
of steps, generally yields better outcomes because more CEs can mature.
Relatedly, we also find that the effectiveness and stability of CT is
positively associated with the total number of counterfactuals generated
during each training epoch. The impact of \(\tau\), the decision
threshold, is more difficult to predict. On ``harder'' datasets it may
be difficult to satisfy high \(\tau\) for any given sample (i.e., also
factuals) and so increasing this threshold does not seem to correlate
with better outcomes. In fact, \(\tau=0.5\) generally leads to optimal
results as it is associated with high proportions of mature
counterfactuals.

Secondly, the strength of the energy regularization,
\(\lambda_{\text{reg}}\) is highly impactful and should be set
sufficiently high to avoid common problems associated with exploding
gradients. The sensitivity with respect to \(\lambda_{\text{div}}\) and
\(\lambda_{\text{adv}}\) is much less evident. While high values of
\(\lambda_{\text{reg}}\) may increase the variability in outcomes when
combined with high values of \(\lambda_{\text{div}}\) or
\(\lambda_{\text{adv}}\), this effect is not particularly pronounced.

Finally, we also observe desired improvements when CT was combined with
conventional training and applied only for the final 50\% of epochs of
the complete training process. Put differently, CT can improve the
explainability of models in a post-hoc, fine-tuning manner.

\section{Discussion}\label{sec-discussion}

As our results indicate, counterfactual training produces models that
are more explainable. Nonetheless, these advantages come with certain
limitations.

\emph{Interventions on features have implications for fairness.} We
provide a method to modify the sensitivity of a model to certain
features, which can be misused by enforcing explanations based on
features that are more difficult to modify by a (group of) decision
subjects. Such abuse could result in an unfairly assigned burden of
recourse (\citeproc{ref-sharma2020certifai}{Sharma, Henderson, and Ghosh
2020}), threatening the equality of opportunity
(\citeproc{ref-bell2024fairness}{Bell et al. 2024}). Also, even if all
immutable features are protected, there may exist proxies that are
theoretically mutable, but preserve sufficient information about the
principals to hinder these protections. Indeed, deciding on the
actionability of features remains a major open challenge in the AR
literature
(\citeproc{ref-venkatasubramanian2020philosophical}{Venkatasubramanian
and Alfano 2020}).

\emph{Plausibility is costly.} As noted by Altmeyer et al.
(\citeproc{ref-altmeyer2024faithful}{2024}), more plausible
counterfactuals are inevitably more costly. CT improves plausibility and
robustness, but it can impact average costs and validity when cheap,
implausible and adversarial explanations are removed from the solution
space.

\emph{CT increases the training times.} Just like contrastive and robust
learning, CT is more resource-intensive than conventional regimes. Three
factors mitigate this effect: (1) CT yields itself to parallel
execution; (2) it amortizes the cost of CEs for the training samples;
and (3) it can be used to fine-tune conventionally-trained models.

We also highlight three key directions for future research. Firstly, it
is an interesting challenge to extend CT beyond classification settings.
Our formulation relies on the distinction between non-target class(es)
and target class(es), requiring the output space to be discrete. Thus,
it does not apply to ML tasks where the change in outcome cannot be
readily discretized. Focus on classification is a common choice in
research on CEs and AR; other settings have attracted some interest,
e.g., regression (\citeproc{ref-spooner2021counterfactual}{Spooner et
al. 2021}), but there is little consensus how to robustly extend the
notion of CEs.

Secondly, our analysis covers CE generators with different
characteristics, but it is interesting to extend it to more algorithms,
including ones that do not rely on computationally costly gradient-based
optimization. This should reduce training costs while possibly
preserving the benefits of CT.

Finally, we believe that it is possible to considerably improve
hyperparameter selection procedures, and thus performance. We have
relied exclusively on grid searches, but future work could benefit from
more sophisticated approaches.

\section{Conclusion}\label{sec-conclusion}

State-of-the-art machine learning models are prone to learning complex
representations that cannot be interpreted by humans. Existing work on
counterfactual explanations has largely focused on designing tools to
generate plausible and actionable for any model. In this work, we
instead hold models accountable for delivering such explanations. We
introduce counterfactual training: a novel training regime that
integrates recent advances in contrastive learning, adversarial
robustness, and CE to incentivize highly-explainable models. Through
theoretical results and extensive experiments, we demonstrate that CT
satisfies this goal while promoting adversarial robustness of models.
Explanations generated from CT-based models are both more plausible
(compliant with the underlying data-generating process) and more
actionable (compliant with user-specified mutability constraints), and
thus meaningful to their recipients. In turn, our work highlights the
value of simultaneously improving models and their explanations.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abbasnejad2020counterfactual}
Abbasnejad, Ehsan, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van
den Hengel. 2020. {``{Counterfactual Vision and Language Learning}.''}
In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 10041--51.
\url{https://doi.org/10.1109/CVPR42600.2020.01006}.

\bibitem[\citeproctext]{ref-altmeyer2023explaining}
Altmeyer, Patrick, Arie van Deursen, and Cynthia C. S. Liem. 2023.
{``{Explaining Black-Box Models through Counterfactuals}.''} In
\emph{Proceedings of the JuliaCon Conferences}, 1:130.

\bibitem[\citeproctext]{ref-altmeyer2024faithful}
Altmeyer, Patrick, Mojtaba Farmanbar, Arie van Deursen, and Cynthia C.
S. Liem. 2024. {``{Faithful Model Explanations through
Energy-Constrained Conformal Counterfactuals}.''} In \emph{Proceedings
of the Thirty-Eighth AAAI Conference on Artificial Intelligence},
38:10829--37. 10. \url{https://doi.org/10.1609/aaai.v38i10.28956}.

\bibitem[\citeproctext]{ref-augustin2020adversarial}
Augustin, Maximilian, Alexander Meinke, and Matthias Hein. 2020.
{``{Adversarial Robustness on In- and Out-Distribution Improves
Explainability}.''} In \emph{Computer Vision -- ECCV 2020}, edited by
Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
228--45. Cham: Springer.

\bibitem[\citeproctext]{ref-balashankar2023improving}
Balashankar, Ananth, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed
Chi, Jilin Chen, and Alex Beutel. 2023. {``{Improving Classifier
Robustness through Active Generative Counterfactual Data
Augmentation}.''} In \emph{Findings of the Association for Computational
Linguistics: EMNLP 2023}, 127--39. ACL.
\url{https://doi.org/10.18653/v1/2023.findings-emnlp.10}.

\bibitem[\citeproctext]{ref-becker1996adult2}
Becker, Barry, and Ronny Kohavi. 1996. {``{Adult}.''} UCI Machine
Learning Repository.

\bibitem[\citeproctext]{ref-bell2024fairness}
Bell, Andrew, Joao Fonseca, Carlo Abrate, Francesco Bonchi, and Julia
Stoyanovich. 2024. {``{Fairness in Algorithmic Recourse Through the Lens
of Substantive Equality of Opportunity}.''}
\url{https://arxiv.org/abs/2401.16088}.

\bibitem[\citeproctext]{ref-du2019implicit}
Du, Yilun, and Igor Mordatch. 2020. {``{Implicit Generation and
Generalization in Energy-Based Models}.''}
\url{https://arxiv.org/abs/1903.08689}.

\bibitem[\citeproctext]{ref-freiesleben2022intriguing}
Freiesleben, Timo. 2022. {``{The Intriguing Relation Between
Counterfactual Explanations and Adversarial Examples}.''} \emph{Minds
and Machines} 32 (1): 77--109.

\bibitem[\citeproctext]{ref-goodfellow2016deep}
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \emph{Deep
{Learning}}. {MIT Press}.

\bibitem[\citeproctext]{ref-goodfellow2014explaining}
Goodfellow, Ian, Jonathon Shlens, and Christian Szegedy. 2015.
{``{Explaining and Harnessing Adversarial Examples}.''}
\url{https://arxiv.org/abs/1412.6572}.

\bibitem[\citeproctext]{ref-grathwohl2020your}
Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
Mohammad Norouzi, and Kevin Swersky. 2020. {``Your Classifier Is
Secretly an Energy Based Model and You Should Treat It Like One.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-gretton2012kernel}
Gretton, Arthur, Karsten M Borgwardt, Malte J Rasch, Bernhard Schlkopf,
and Alexander Smola. 2012. {``A Kernel Two-Sample Test.''} \emph{The
Journal of Machine Learning Research} 13 (1): 723--73.

\bibitem[\citeproctext]{ref-guo2023counternet}
Guo, Hangzhi, Thanh H. Nguyen, and Amulya Yadav. 2023. {``{CounterNet:
End-to-End Training of Prediction Aware Counterfactual Explanations}.''}
In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining}, 577-\/-589. KDD '23. New York, NY, USA:
Association for Computing Machinery.
\url{https://doi.org/10.1145/3580305.3599290}.

\bibitem[\citeproctext]{ref-joshi2019realistic}
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and
Joydeep Ghosh. 2019. {``{Towards Realistic Individual Recourse and
Actionable Explanations in Black-Box Decision Making Systems}.''}
\url{https://arxiv.org/abs/1907.09615}.

\bibitem[\citeproctext]{ref-kaggle2011give}
Kaggle. 2011. {``Give Me Some Credit, {Improve} on the State of the Art
in Credit Scoring by Predicting the Probability That Somebody Will
Experience Financial Distress in the Next Two Years.''}
https://www.kaggle.com/c/GiveMeSomeCredit; {Kaggle}.
\url{https://www.kaggle.com/c/GiveMeSomeCredit}.

\bibitem[\citeproctext]{ref-karimi2020survey}
Karimi, Amir-Hossein, Gilles Barthe, Bernhard Schlkopf, and Isabel
Valera. 2021. {``A Survey of Algorithmic Recourse: Definitions,
Formulations, Solutions, and Prospects.''}
\url{https://arxiv.org/abs/2010.04050}.

\bibitem[\citeproctext]{ref-lakshminarayanan2016simple}
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2017.
{``Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.''} In \emph{Proceedings of the 31st International Conference
on Neural Information Processing Systems}, 6405--16. NIPS'17. Red Hook,
NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-lecun1998mnist}
LeCun, Yann. 1998. {``{The MNIST database of handwritten digits}.''}
http://yann.lecun.com/exdb/mnist/.

\bibitem[\citeproctext]{ref-lippe2024uvadlc}
Lippe, Phillip. 2024. {``{UvA Deep Learning Tutorials}.''}
\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}.

\bibitem[\citeproctext]{ref-luu2023counterfactual}
Luu, Hoai Linh, and Naoya Inoue. 2023. {``{Counterfactual Adversarial
Training for Improving Robustness of Pre-trained Language Models}.''} In
\emph{Proceedings of the 37th Pacific Asia Conference on Language,
Information and Computation}, 881--88. ACL.
\url{https://aclanthology.org/2023.paclic-1.88/}.

\bibitem[\citeproctext]{ref-molnar2022interpretable}
Molnar, Christoph. 2022. \emph{Interpretable Machine Learning: A Guide
for Making Black Box Models Explainable}. 2nd ed.
\url{https://christophm.github.io/interpretable-ml-book}.

\bibitem[\citeproctext]{ref-murphy2022probabilistic}
Murphy, Kevin P. 2022. \emph{Probabilistic {Machine Learning}: {An}
Introduction}. {MIT Press}.

\bibitem[\citeproctext]{ref-pace1997sparse}
Pace, R Kelley, and Ronald Barry. 1997. {``Sparse Spatial
Autoregressions.''} \emph{Statistics \& Probability Letters} 33 (3):
291--97. \url{https://doi.org/10.1016/s0167-7152(96)00140-x}.

\bibitem[\citeproctext]{ref-pawelczyk2022exploring}
Pawelczyk, Martin, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and
Himabindu Lakkaraju. 2022. {``Exploring Counterfactual Explanations
Through the Lens of Adversarial Examples: A Theoretical and Empirical
Analysis.''} In \emph{Proceedings of the 25th International Conference
on Artificial Intelligence and Statistics}, edited by Gustau
Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, 151:4574--94.
Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v151/pawelczyk22a.html}.

\bibitem[\citeproctext]{ref-ross2021learning}
Ross, Alexis, Himabindu Lakkaraju, and Osbert Bastani. 2024.
{``{Learning Models for Actionable Recourse}.''} In \emph{Proceedings of
the 35th International Conference on Neural Information Processing
Systems}. NIPS '21. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-sauer2021counterfactual}
Sauer, Axel, and Andreas Geiger. 2021. {``{Counterfactual Generative
Networks}.''} \url{https://arxiv.org/abs/2101.06046}.

\bibitem[\citeproctext]{ref-schut2021generating}
Schut, Lisa, Oscar Key, Rory McGrath, Luca Costabello, Bogdan Sacaleanu,
Yarin Gal, et al. 2021. {``Generating {Interpretable Counterfactual
Explanations By Implicit Minimisation} of {Epistemic} and {Aleatoric
Uncertainties}.''} In \emph{International {Conference} on {Artificial
Intelligence} and {Statistics}}, 1756--64. {PMLR}.

\bibitem[\citeproctext]{ref-sharma2020certifai}
Sharma, Shubham, Jette Henderson, and Joydeep Ghosh. 2020. {``{CERTIFAI:
A Common Framework to Provide Explanations and Analyse the Fairness and
Robustness of Black-box Models}.''} In \emph{Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society}, 166--72. AIES '20. New York, NY,
USA: Association for Computing Machinery.
\url{https://doi.org/10.1145/3375627.3375812}.

\bibitem[\citeproctext]{ref-spooner2021counterfactual}
Spooner, Thomas, Danial Dervovic, Jason Long, Jon Shepard, Jiahao Chen,
and Daniele Magazzeni. 2021. {``{Counterfactual Explanations for
Arbitrary Regression Models}.''} \url{https://arxiv.org/abs/2106.15212}.

\bibitem[\citeproctext]{ref-sundararajan2017ig}
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. {``Axiomatic
Attribution for Deep Networks.''}
\url{https://arxiv.org/abs/1703.01365}.

\bibitem[\citeproctext]{ref-szegedy2013intriguing}
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. {``Intriguing
Properties of Neural Networks.''} \url{https://arxiv.org/abs/1312.6199}.

\bibitem[\citeproctext]{ref-teh2003energy}
Teh, Yee Whye, Max Welling, Simon Osindero, and Geoffrey E. Hinton.
2003. {``Energy-Based Models for Sparse Overcomplete Representations.''}
\emph{J. Mach. Learn. Res.} 4 (null): 1235--60.

\bibitem[\citeproctext]{ref-teney2020learning}
Teney, Damien, Ehsan Abbasnedjad, and Anton van den Hengel. 2020.
{``Learning What Makes a Difference from Counterfactual Examples and
Gradient Supervision.''} In \emph{Computer Vision - ECCV 2020}, 580--99.
Berlin, Heidelberg: Springer-Verlag.
\url{https://doi.org/10.1007/978-3-030-58607-2_34}.

\bibitem[\citeproctext]{ref-ustun2019actionable}
Ustun, Berk, Alexander Spangher, and Yang Liu. 2019. {``Actionable
Recourse in Linear Classification.''} In \emph{Proceedings of the
{Conference} on {Fairness}, {Accountability}, and {Transparency}},
10--19. \url{https://doi.org/10.1145/3287560.3287566}.

\bibitem[\citeproctext]{ref-venkatasubramanian2020philosophical}
Venkatasubramanian, Suresh, and Mark Alfano. 2020. {``The Philosophical
Basis of Algorithmic Recourse.''} In \emph{Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency}, 284--93. FAT*
'20. New York, NY, USA: Association for Computing Machinery.
\url{https://doi.org/10.1145/3351095.3372876}.

\bibitem[\citeproctext]{ref-verma2020counterfactual}
Verma, Sahil, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P.
Dickerson, and Chirag Shah. 2022. {``Counterfactual Explanations and
Algorithmic Recourses for Machine Learning: A Review.''}
\url{https://arxiv.org/abs/2010.10596}.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box:
{Automated} Decisions and the {GDPR}.''} \emph{Harv. JL \& Tech.} 31:
841. \url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wilson2020case}
Wilson, Andrew Gordon. 2020. {``{The Case for Bayesian Deep
Learning}.''} \url{https://arxiv.org/abs/2001.10995}.

\bibitem[\citeproctext]{ref-wu2021polyjuice}
Wu, Tongshuang, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld.
2021. {``Polyjuice: Generating Counterfactuals for Explaining,
Evaluating, and Improving Models.''} In \emph{Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers)}, edited by Chengqing Zong, Fei Xia, Wenjie Li,
and Roberto Navigli, 6707--23. Online: ACL.
\url{https://doi.org/10.18653/v1/2021.acl-long.523}.

\bibitem[\citeproctext]{ref-yeh2016default}
Yeh, I-Cheng. 2016. {``{Default of Credit Card Clients}.''} UCI Machine
Learning Repository.

\end{CSLReferences}




\end{document}
