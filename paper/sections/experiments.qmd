# Experiments {#sec-experiments}

In this section, we present experiments that we have conducted in order to answer the following research questions:

::: {#cor-plaus}

## Plausibility

Does our proposed counterfactual training objective (@eq-obj) induce models to learn plausible explanations?

:::

::: {#cor-action}

## Actionability

Does our proposed counterfactual training objective (@eq-obj) yield more favorable algorithmic recourse outcomes in the presence of actionability constraints?

:::

Beyond this, we are also interested in understanding how robust our answers to @cor-plaus and @cor-action are:

::: {#cor-hyper}

## Hyperparameters 

What are the effects of different hyperparameter choices with respect to @eq-obj?

:::


## Experimental Setup

### Evaluation 

Our key outcome of interest is how well models perform with respect to explainability (@def-explainability): to this end, we focus primarily on the plausibility and cost of faithfully generated counterfactuals at test time. To measure the cost of counterfactuals, we follow the standard convention of using distances ($\ell_1$-norm) between factuals and counterfactuals as a proxy. For plausibility, we assess how similar counterfactuals are to observed samples in the target domain. We rely on previously used distance-based metrics, in particular the one used by @altmeyer2024faithful, and introduce a novel divergence metric. 


We also assess their predictive performance using conventional performance metrics including accuracy and the F1-score. 

## Experimental Results