<!-- # Conclusion {#sec-conclusion} -->
To conclude, state-of-the-art machine learning models are prone to learning complex representations that cannot be interpreted by humans and existing post-hoc explainability approaches cannot guarantee that the explanations agree with the model's learned representation of data. As a step towards addressing this challenge, we introduced counterfactual training, a novel training regime that incentivizes highly-explainable models. Our approach leads to explanations that are both plausible---compliant with the underlying data-generating process---and actionable---compliant with user-specified mutability constraints---and thus meaningful to their recipients. Through extensive experiments we demonstrate that CT satisfies its objective while promoting robustness and preserving the predictive performance of the models. It can also be used to fine-tune conventionally-trained models and achieve similar gains. Finally, this work showcases that it is practical to improve models *and* their explanations at the same time.