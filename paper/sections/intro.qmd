---
bibliography: bibliography.bib
---

# Introduction

Counterfactual explanations (CE) have become an integral part of the broader explainable AI (XAI) toolkit [@molnar2022interpretable]. Originally proposed by @wachter2017counterfactual, CEs prescribe minimal changes for factual inputs that, if implemented, would prompt some fitted model to produce an alternative, more desirable output. This is useful and necessary to not only understand how opaque AI models make predictions, but also to provide recourse to individuals subjected to them: a retail bank, for example, could use CE to provide meaningful feedback to unsuccessful loan applicants that were rejected based on an opaque AI decision-making system (ADM).

For feedback to be meaningful, counterfactual explanations need to fulfill certain desiderata [@verma2020counterfactual; @karimi2020survey]: they should be faithful to the model [@altmeyer2024faithful], plausible [@joshi2019realistic] and actionable [@ustun2019actionable]. Plausibility is typically understood as counterfactuals being *in-domain*: unsuccessful loan applicants that implement recourse should end up with a credit profile that is genuinely similar to that of individuals who have successfully repaid their loans in the past. Actionable explanations comply with practical constraints: a young, unsuccessful loan applicant cannot increase their age in an instance.

Existing state-of-the-art (SOTA) approaches in the field have largely focused on designing model-agnostic CE methods that identify subsets of counterfactuals, which comply with specific desiderata. This is problematic, because a narrow focus on any specific desideratum can adversely affect others: it is possible, for example, to generate plausible counterfactuals for models that are also highly vulnerable to implausible, possibly adversarial counterfactuals [@altmeyer2024faithful]. In this work, we therefore follow the paradigm that models---as opposed to explanation methods---should be held accountable for plausible and actionable explanations. While previous work has shown that at least plausibility can be indirectly achieved through existing techniques aimed at models' generative capacity, generalization and robustness [@altmeyer2024faithful; @augustin2020adversarial; @schut2021generating], we directly incorporate both plausibility and actionability in the training objective of models. 

Specifically, **we propose Counterfactual Training (CT)**: a novel training regime for differentiable models that leverage counterfactual explanations on-the-fly to ensure that models learn plausible and actionable explanations for the data. In essence, we achieve this by contrasting faithful explanations with ground-truth data, while at the same time protecting features that are subject to actionability constraints. To the best of our knowledge, CT represents the first attempt in this direction.

In our experiments, we show that thanks to counterfactual training, (1) implausibility decreases with 16% on average; (2) the cost of reaching counterfactuals with protected features decreases with 15%; and (3) robust accuracy increases for all datasets with up to .42.

__________________________________________________________

The remainder of this manuscript is structured as follows. @sec-lit presents related work, focusing on the link between AEs and CEs. Then follow our two principal contributions. In @sec-method, we introduce our methodological framework and show theoretically that it can be employed to enforce global actionability constraints. In @sec-experiments, through extensive experiments, we empirically demonstrate that CT substantially improves explainability and positively contributes to the robustness of trained models without sacrificing predictive performance. Finally, in @sec-discussion, we discuss open challenges and conclude that CT is a promising approach towards making opaque models more trustworthy.

