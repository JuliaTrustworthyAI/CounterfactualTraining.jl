\makeatletter
\@ifpackageloaded{hyperref}{}{%
  \newcommand{\texorpdfstring}[2]{#1}%
}
\newcommand{\tightlist}
\makeatother
\providecommand\phantomsection{}

%File: anonymous-submission-latex-2026.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
% \title{AAAI Press Anonymous Submission\\Instructions for Authors Using \LaTeX{}}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{placeins}
\usepackage{siunitx}
\sisetup{uncertainty-mode = separate}
\DeclareMathSizes{10}{9}{7}{6.5}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


\title{Counterfactual Training: Teaching Models Plausible and Actionable
Explanations}
\author{Patrick Altmeyer \and Aleksander Buszydlik \and Arie van
Deursen \and Cynthia C. S. Liem}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory
capacity of models. Counterfactual explanations have emerged as a
popular post-hoc explanation method for opaque machine learning models:
they inform how factual inputs would need to change in order for a model
to produce some desired output. To be useful in real-word
decision-making systems, counterfactuals should be (1) plausible with
respect to the underlying data and (2) actionable with respect to the
user-defined mutability constraints. Much existing research has
therefore focused on developing post-hoc methods to generate
counterfactuals that meet these desiderata. In this work, we instead
hold models directly accountable for the desired end goal:
counterfactual training employs counterfactuals ad-hoc during the
training phase to minimize the divergence between learned
representations and plausible, actionable explanations. We demonstrate
empirically and theoretically that our proposed method facilitates
training models that deliver inherently desirable explanations while
promoting robustness and preserving high predictive performance.
\end{abstract}
\section{Introduction}\label{introduction}

Today's prominence of artificial intelligence (AI) has largely been
driven by \textbf{representation learning}: instead of relying on
features and rules hand-crafted by humans, modern machine learning (ML)
models are tasked with learning representations directly from the data,
guided by narrow objectives such as predictive accuracy
\citep{goodfellow2016deep}. Advances in computing have made it possible
to provide these models with ever-growing degrees of freedom to achieve
this task, which often allows them to outperform traditionally
parsimonious models. Unfortunately, in doing so, models learn
increasingly complex, sensitive representations that humans can no
longer easily interpret.

The trend towards complexity for the sake of performance has come under
scrutiny in recent years. At the very cusp of the deep learning (DL)
revolution, \citet{szegedy2013intriguing} showed that artificial neural
networks (ANN) are susceptible to adversarial examples (AEs): perturbed
versions of data instances that yield vastly different model predictions
despite being semantically indistinguishable from their factual
counterparts. Some partial mitigation strategies have been
proposed---most notably \textbf{adversarial training}
\citep{goodfellow2014explaining}---but truly robust deep learning
remains unattainable even for models that are considered ``shallow'' by
today's standards \citep{kolter2023keynote}.

Part of the problem is that the high degrees of freedom---high number of
parameters estimated from data---provide room for many solutions that
are locally optimal with respect to narrow objectives
\citep{wilson2020case}. As one example, research on the ``lottery ticket
hypothesis'' suggests that modern neural networks can be pruned by up to
90\% without losing predictive performance \citep{frankle2018lottery}.
Thus, looking at the predictive performance alone, found solutions may
seem to provide compelling explanations for the data, when in fact they
are based on purely associative and semantically meaningless patterns.
This poses two related challenges. Firstly, there is no dependable way
to verify if learned representations correspond to meaningful, plausible
explanations. Secondly, even if we resolve this challenge, it remains
undecided how to ensure that machine learning models can \emph{only}
learn valuable explanations.

The first challenge has attracted an abundance of work on
\textbf{explainable AI} (XAI), a paradigm that focuses on the
development of tools to derive (post-hoc) explanations from complex
model representations, aiming to mitigate scenarios in which
practitioners deploy opaque models and have to blindly rely on their
predictions. On many occasions, this has happened in practice, causing
harms to people who were adversely and unfairly affected by automated
decision-making (ADM) systems involving opaque models; see, e.g.,
\citet{oneil2016weapons}. Effective XAI tools can also aid in monitoring
models and providing recourse, empowering people to turn negative
outcomes (e.g., ``loan application rejected'') into positive ones (e.g.,
``loan application accepted''). In line with this, our work builds upon
\textbf{counterfactual explanations} (CE) proposed by
\citet{wachter2017counterfactual}; CEs prescribe minimal changes for
factual inputs that, if implemented, would prompt some fitted model to
produce an alternative, more desirable output.

To our surprise, the second challenge has not yet attracted major
research interest. In particular, there has been no concerted effort
towards improving the degree to which learned representations promote
explanations that are both \textbf{interpretable} to and deemed
\textbf{plausible} by humans. Instead, the typical choice has been to
improve the ability of XAI tools to identify the subset of explanations
that are plausible and valid for any given model, independent of whether
these explanations are compatible with the learned representations
\citep{altmeyer2024faithful}. Fortunately, recent findings indicate that
improved ``explanatory capacity'' of a model can arise as a consequence
of regularization techniques aimed at other training objectives such as
generative capacity, generalization, or robustness
\citep{altmeyer2024faithful, augustin2020adversarial, schut2021generating}.
Our contribution consolidates these findings within a unified framework.

Specifically, \textbf{we propose Counterfactual Training (CT)}: a novel
training regime explicitly geared towards improving the explanatory
capacity of models that, in high-level terms, we define as the extent to
which valid explanations derived for a model can be deemed plausible
with respect to the underlying data and global actionability constraints
(we refine this notion in Def. \ref{def-explainability}). For
simplicity, we refer to models with high explanatory capacity as
\emph{explainable}. To the best of our knowledge, Counterfactual
Training represents the first attempt to achieve more explainable models
by employing counterfactual explanations already in the training phase.

The remainder of this manuscript is structured as follows.
Section~\ref{sec-lit} presents related work, focusing on the link
between AEs and CEs. Then follow our two principal contributions. In
Section~\ref{sec-method}, we introduce our methodological framework and
show theoretically that it can be employed to enforce global
actionability constraints. In Section~\ref{sec-experiments}, through
extensive experiments, we empirically demonstrate that CT substantially
improves explainability and positively contributes to the robustness of
trained models without sacrificing predictive performance. In
Section~\ref{sec-discussion}, we discuss open challenges and, in
\textbf{?@sec-conclusion}, conclude that CT is a promising approach
towards making opaque models more trustworthy.

\section{Related Literature}\label{sec-lit}

To make the desiderata for our framework more concrete, we follow
\citet{augustin2020adversarial} in tying explainability to the quality
of CEs that can be generated for a given model. The authors show that
CEs (understood as minimal input perturbations that yield some desired
model prediction) tend to be more meaningful if the underlying model is
more robust to adversarial examples. We can make intuitive sense of this
finding if we look at adversarial training (AT) through the lens of
representation learning with high degrees of freedom. As argued before,
learned representations may be sensitive to producing implausible
explanations and mispredicting for worst-case counterfactuals (i.e.,
AEs). Thus, by inducing models to ``unlearn'' susceptiblity to such
examples, adversarial training can effectively remove implausible
explanations from the solution space.

\subsection{Adversarial Examples are
Counterfactuals}\label{adversarial-examples-are-counterfactuals}

The interpretation of the link between explainability through
counterfactuals on the one side, and robustness to adversarial examples
on the other is backed by empirical evidence.
\citet{sauer2021counterfactual} demonstrate that using counterfactual
images during classifier training improves model robustness. Similarly,
\citet{abbasnejad2020counterfactual} argue that counterfactuals
represent potentially useful training data in machine learning,
especially in supervised settings where inputs may be reasonably mapped
to multiple outputs. They, too, show that augmenting the training data
of (image) classifiers can improve generalization performance. Finally,
\citet{teney2020learning} argue that counterfactual pairs tend to exist
in training data. Hence, their approach aims to identify similar input
samples with different annotations and ensure that the gradient of the
classifier aligns with the vector between such pairs of counterfactual
inputs using a cosine distance loss function.

CEs have also been used to improve models in the natural language
processing domain. For example, \citet{wu2021polyjuice2} propose
\emph{Polyjuice}, a general-purpose CE generator for language models and
demonstrate that the augmentation of training data with \emph{Polyjuice}
improves robustness in a number of tasks, while
\citet{luu2023counterfactual} introduce the \emph{Counterfactual
Adversarial Training} (CAT) framework that aims to improve
generalization and robustness of language models by generating
counterfactuals for training samples that are subject to high predictive
uncertainty.

There have also been several attempts at formalizing the relationship
between counterfactual explanations and adversarial examples. Pointing
to clear similarities in how CEs and AEs are generated,
\citet{freiesleben2022intriguing} makes the case for jointly studying
the opaqueness and robustness problems in representation learning.
Formally, AEs can be seen as the subset of CEs for which
misclassification is achieved \citep{freiesleben2022intriguing}.
Similarly, \citet{pawelczyk2022exploring} show that CEs and AEs are
equivalent under certain conditions.

Two other works are closely related to ours in that they use
counterfactuals during training with the explicit goal of affecting
certain properties of the post-hoc counterfactual explanations. Firstly,
\citet{ross2021learning} propose a way to train models that guarantee
recourse to a positive target class with high probability. Their
approach builds on adversarial training by explicitly inducing
susceptibility to targeted AEs for the positive class. Additionally, the
method allows for imposing a set of actionability constraints ex-ante.
For example, users can specify that certain features are immutable.
Secondly, \citet{guo2023counternet} are the first to propose an
end-to-end training pipeline that includes CEs as part of the training
procedure. Their \emph{CounterNet} network architecture includes a
predictor and a CE generator, where the parameters of the CE generator
are learnable. Counterfactuals are generated during each training
iteration and fed back to the predictor. In contrast, we impose no
restrictions on the ANN architecture at all.

\subsection{Aligning Representations with
Explanations}\label{aligning-representations-with-explanations}

Improving the adversarial robustness of models is not the only path
towards aligning representations with plausible explanations. In a
closely related work, \citet{altmeyer2024faithful} show that
explainability can be improved through model averaging and refined model
objectives. They propose a way to generate counterfactuals that are
maximally faithful to the model in that they are consistent with what
the model has learned about the underlying data. Formally, they rely on
tools from energy-based modelling \citep{teh2003energy} to minimize the
divergence between the distribution of counterfactuals and the
conditional posterior over inputs learned by the model. Their
counterfactual explainer, \emph{ECCCo}, yields plausible explanations if
and only if the underlying model has learned representations that align
with them. The authors find that both deep ensembles
\citep{lakshminarayanan2016simple} and joint energy-based models (JEMs)
\citep{grathwohl2020your} tend to do well in this regard.

Once again it helps to look at these findings through the lens of
representation learning with high degrees of freedom. Deep ensembles are
approximate Bayesian model averages, which are particularly effective
when models are underspecified by the available data
\citep{wilson2020case}. Averaging across solutions mitigates the
aforementioned risk of overrelying on a single locally optimal
representation that corresponds to semantically meaningless explanations
for the data. Likewise, previous work of \citet{schut2021generating}
found that generating plausible (``interpretable'') CEs is almost
trivial for deep ensembles that have undergone adversarial training. The
case for JEMs is even clearer: they optimize a hybrid objective that
induces both high predictive performance and strong generative capacity
\citep{grathwohl2020your}, which bears resemblance to the idea of
aligning models with plausible explanations and has inspired our CT
objective.

\section{Counterfactual Training}\label{sec-method}

This section introduces the Counterfactual Training framework. CT
combines ideas from adversarial training, counterfactual explanations,
and energy-based modelling with the explicit goal of producing models
whose learned representations align with plausible explanations that
further comply with user-defined actionability constraints.

In the context of counterfactual explanations, plausibility has broadly
been defined as the degree to which generated CEs comply with the
underlying data-generating process
\citep{altmeyer2024faithful, guidotti2022counterfactual, poyiadzi2020face}.
Plausibility is a necessary but insufficient condition for using CEs to
provide algorithmic recourse (AR) to individuals (negatively) affected
by opaque models. An AR recommendations must also be actionable, i.e.,
possible to attain by the recipient. A plausible CE for a rejected
20-year-old loan applicant, for example, might reveal that their
application would have been accepted, if only they had been 20 years
older. Ignoring all other features, this would comply with the
definition of plausibility if 40-year-old individuals were in fact more
credit-worthy on average than young adults. But of course this CE does
not qualify for providing actionable recourse to the applicant since
\emph{age} is not a (directly) mutable feature. Counterfactual training
aims to improve model explainability by aligning models with
counterfactuals that meet both desiderata: plausibility and
actionability. Formally, we define explainability as follows:

\begin{definition}[Model Explainability]
\label{def-explainability}
Let $\mathbf{M}_\theta: \mathcal{X} \mapsto \mathcal{Y}$ denote a supervised classification model that maps from the $D$-dimensional input space $\mathcal{X}$ to representations $\phi(\mathbf{x};\theta)$ and finally to the $K$-dimensional output space $\mathcal{Y}$. Assume that for any given input-output pair $\{\mathbf{x},\mathbf{y}\}_i$ there exists a counterfactual $\mathbf{x}^{\prime} = \mathbf{x} + \Delta: \mathbf{M}_\theta(\mathbf{x}^{\prime}) = \mathbf{y}^{+} \neq \mathbf{y} = \mathbf{M}_\theta(\mathbf{x})$, where $\arg\max_y{\mathbf{y}^{+}}=y^+$ is the index of the target class. 

We say that $\mathbf{M}_\theta$ has an \textbf{explanatory capacity} to the extent that faithfully generated counterfactuals are also plausible and actionable. We define these properties as follows:

\begin{enumerate}
    \item (Faithfulness) $\int^{A} p_\theta(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$, where $A$ is some arbitrarily small region around $\mathbf{x}^{\prime}$.
    \item (Plausibility) $\int^{A} p(\mathbf{x}^\prime|\mathbf{y}^{+})d\mathbf{x} \rightarrow 1$; $A$ as specified above.
    \item (Actionability) Perturbations $\Delta$ are subject to some actionability constraints.
\end{enumerate}
and $p_\theta(\mathbf{x}|\mathbf{y}^{+})$ denotes the conditional posterior distribution over inputs. For simplicity, we refer to a model with high explanatory capacity as \textbf{explainable} in this manuscript. 
\end{definition}

\noindent The characterization of faithfulness and plausibility in Def.
\ref{def-explainability} is the same as in \citet{altmeyer2024faithful},
with adapted notation. Intuitively, plausible counterfactuals are
consistent with the data and faithful counterfactuals are consistent
with what the model has learned about the input data. Actionability
constraints in Def. \ref{def-explainability} vary and depend on the
context in which \(\mathbf{M}_\theta\) is deployed. In this work, we
choose to only consider domain and mutability constraints for individual
features \(x_d\) for \(d=1,...,D\). We also limit ourselves to
classification tasks for reasons discussed in
Section~\ref{sec-discussion}.

\subsection{Proposed Objective}\label{proposed-objective}

Let \(\mathbf{x}_t^\prime\) for \(t=0,...,T\) denote a counterfactual
generated through gradient descent over \(T\) iterations as originally
proposed by \citet{wachter2017counterfactual}. In broad terms, searching
for CEs using gradient descent entails optimizing some form of an
objective that balances (1) the classification loss
\(\text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y})\), and (2) one
or more penalty terms \(\lambda_{i}\text{cost}_{i}(\cdot)\). The exact
specification of these penalties induces various properties in the
counterfactual outcomes, and tends to be the key feature that
distinguishes various gradient-based ``generators'' or ``explainers'' in
the literature \citep{altmeyer2023explaining}, including all generators
used in our experiments. We refer the reader to the supplementary
appendix for details.

CT adopts gradient-based CE search during training to generate
on-the-fly model explanations \(\mathbf{x}^\prime\) for training
samples. We use the term \emph{nascent} to denote counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) that are not yet valid where \(t\)
indicates the last iteration before the label is flipped. We store and
use these interim counterfactuals as adversarial examples. Conversely,
we consider counterfactuals \(\mathbf{x}_T^\prime\) as \emph{mature}
explanations if they have either exhausted all \(T\) iterations or
converged in terms reaching a pre-specified threshold, \(\tau\), for the
predicted probability of the target class:
\(\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}^\prime))[y^+] \geq \tau\),
where \(\mathcal{S}\) is the softmax function.

Formally, we propose the following counterfactual training objective to
train explainable (as in Def. \ref{def-explainability}) models:
\begin{equation}\phantomsection\label{eq-obj}{
\begin{aligned}
&\min_\theta \text{yloss}(\mathbf{M}_\theta(\mathbf{x}),\mathbf{y}) + \lambda_{\text{div}} \text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) \\+ &\lambda_{\text{adv}} \text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y}) + \lambda_{\text{reg}}\text{ridge}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta)
\end{aligned}
}\end{equation} where \(\text{yloss}(\cdot)\) is any classification loss
that induces discriminative performance (e.g., cross-entropy). The
second and third terms are explained in detail below. For now, they can
be summarized as inducing explainability directly and indirectly by
penalizing the contrastive divergence, \(\text{div}(\cdot)\), between
mature counterfactuals \(\mathbf{x}_T^\prime\) and observed samples
\(\mathbf{x}^+\in\mathcal{X}^+=\{\mathbf{x}:y=y^+\}\) in the target
class \(y^+\), and the adversarial loss, \(\text{advloss}(.)\), wrt.
nascent counterfactuals \(\mathbf{x}_{t\leq T}^\prime\). Finally,
\(\text{ridge}(\cdot)\) denotes a Ridge penalty (\(\ell_2\)-norm) that
regularizes the magnitude of the energy terms involved in
\(\text{div}(\cdot)\) \citep{du2019implicit}. The trade-off between the
components are governed through \(\lambda_{\text{div}}\),
\(\lambda_{\text{adv}}\) and \(\lambda_{\text{reg}}\).

\subsection{Directly Inducing Explainability with Contrastive
Divergence}\label{directly-inducing-explainability-with-contrastive-divergence}

\citet{grathwohl2020your} observe that any classifier can be
re-interpreted as a joint energy-based model that learns to discriminate
output classes conditional on the observed (training) samples from
\(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\). The authors show that JEMs can be trained to
perform well at both tasks by directly maximizing the joint
log-likelihood:
\(\log p_\theta(\mathbf{x},\mathbf{y})=\log p_\theta(\mathbf{y}|\mathbf{x}) + \log p_\theta(\mathbf{x})\),
where the first term can be optimized using cross-entropy as in
Equation~\ref{eq-obj}. To optimize \(\log p_\theta(\mathbf{x})\), they
minimize the contrastive divergence between the observed samples from
\(p(\mathbf{x})\) and the generated samples from
\(p_\theta(\mathbf{x})\).

A key empirical finding of \citet{altmeyer2024faithful} was that JEMs
perform well on the plausibility objective in Def.
\ref{def-explainability}. This follows directly if we consider samples
drawn from \(p_\theta(\mathbf{x})\) as counterfactuals --- the JEM
objective effectively minimizes the divergence between the conditional
posterior and \(p(\mathbf{x}|\mathbf{y}^{+})\). To generate samples,
\citet{grathwohl2020your} use Stochastic Gradient Langevin Dynamics
(SGLD) with an uninformative prior for initialization but we depart from
their methodology. Instead we propose to leverage counterfactual
explainers to generate counterfactuals of observed training samples.
Specifically, we have: \begin{equation}\phantomsection\label{eq-div}{
\text{div}(\mathbf{x}^+,\mathbf{x}_T^\prime,y;\theta) = \mathcal{E}_\theta(\mathbf{x}^+,y) - \mathcal{E}_\theta(\mathbf{x}_T^\prime,y)
}\end{equation} where \(\mathcal{E}_\theta(\cdot)\) denotes the energy
function defined as
\(\mathcal{E}_\theta(\mathbf{x},y)=-\mathbf{M}_\theta(\mathbf{x})[y^+]\),
with \(y^+\) denoting the index of the randomly drawn target class,
\(y^+ \sim p(y)\). Conditional on the target class \(y^+\),
\(\mathbf{x}_T^\prime\) denotes a mature counterfactual for a randomly
sampled factual from a non-target class generated with a gradient-based
CE generator for up to \(T\) iterations. Mature counterfactuals are ones
that have either reached convergence wrt. the decision threshold
\(\tau\) or exhausted \(T\).

Intuitively, the gradient of Equation~\ref{eq-div} decreases the energy
of observed training samples (positive samples) while increasing the
energy of counterfactuals (negative samples) \citep{du2019implicit}. As
the counterfactuals get more plausible (Def. \ref{def-explainability})
during training, these opposing effects gradually balance each other out
\citep{lippe2024uvadlc}.

The departure from SGLD allows us to tap into the vast repertoire of
explainers that have been proposed in the literature to meet different
desiderata. For example, many methods support domain and mutability
constraints. In principle, any existing approach for generating CEs is
viable, so long as it does not violate the faithfulness condition. Like
JEMs \citep{murphy2022probabilistic}, Counterfactual Training can be
considered a form of contrastive representation learning.

\subsection{Indirectly Inducing Explainability with Adversarial
Robustness}\label{indirectly-inducing-explainability-with-adversarial-robustness}

Based on our analysis in Section~\ref{sec-lit}, counterfactuals
\(\mathbf{x}^\prime\) can be repurposed as additional training samples
\citep{balashankar2023improving, luu2023counterfactual} or adversarial
examples \citep{freiesleben2022intriguing, pawelczyk2022exploring}. This
leaves some flexibility with regards to the choice for the
\(\text{advloss}(\cdot)\) term in Equation~\ref{eq-obj}. An intuitive
functional form, but likely not the only sensible choice, is inspired by
adversarial training: \begin{equation}\phantomsection\label{eq-adv}{
\begin{aligned}
\text{advloss}(\mathbf{M}_\theta(\mathbf{x}_{t\leq T}^\prime),\mathbf{y};\varepsilon)&=\text{yloss}(\mathbf{M}_\theta(\mathbf{x}_{t_\varepsilon}^\prime),\mathbf{y}) \\
t_\varepsilon &= \max_t \{t: ||\Delta_t||_\infty < \varepsilon\}
\end{aligned}
}\end{equation} Under this choice, we consider nascent counterfactuals
\(\mathbf{x}_{t\leq T}^\prime\) as AEs as long as the magnitude of the
perturbation to any single feature is at most \(\varepsilon\). This is
closely aligned with \citet{szegedy2013intriguing} who define an
adversarial attack as an ``imperceptible non-random perturbation''.
Thus, we work with a different distinction between CE and AE than
\citet{freiesleben2022intriguing} who considers misclassification as the
distinguishing feature of adversarial examples. One of the key
observations of this work is that we can leverage CEs during training
and get AEs essentially for free to reap the aforementioned benefits of
adversarial training.

\subsection{Encoding Actionability Constraints}\label{sec-constraints}

Many existing counterfactual explainers support domain and mutability
constraints out-of-the-box. In fact, both types of constraints can be
implemented for any explainer that relies on gradient descent in the
feature space for optimization \citep{altmeyer2023explaining}. In this
context, domain constraints can be imposed by simply projecting
counterfactuals back to the specified domain, if the previous gradient
step resulted in updated feature values that were out-of-domain.
Similarly, mutability constraints can be enforced by setting partial
derivatives to zero to ensure that features are only perturbed in the
allowed direction, if at all.

Since actionability constraints are binding at test time, we should also
impose them when generating \(\mathbf{x}^\prime\) during each training
iteration to inform model representations. Through their effect on
\(\mathbf{x}^\prime\), both types of constraints influence model
outcomes via Equation~\ref{eq-div}. Here it is crucial that we avoid
penalizing implausibility that arises due to mutability constraints. For
any mutability-constrained feature \(d\) this can be achieved by
enforcing \(\mathbf{x}^+[d] - \mathbf{x}^\prime[d]:=0\) whenever
perturbing \(\mathbf{x}^\prime[d]\) in the direction of
\(\mathbf{x}^+[d]\) would violate mutability constraints. Specifically,
we set \(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Feature \(d\) is strictly immutable in practice.
\item
  \(\mathbf{x}^+[d]>\mathbf{x}^\prime[d]\), but \(d\) can only be
  decreased in practice.
\item
  \(\mathbf{x}^+[d]<\mathbf{x}^\prime[d]\), but \(d\) can only be
  increased in practice.
\end{enumerate}

\noindent From a Bayesian perspective, setting
\(\mathbf{x}^+[d] := \mathbf{x}^\prime[d]\) can be understood as
assuming a point mass prior for \(p(\mathbf{x}^+)\) wrt. feature \(d\).
Intuitively, we think of this as ignoring implausibility costs of
immutable features, which effectively forces the model to instead seek
plausibility through the remaining features. This can be expected to
result in lower overall sensitivity to immutable features, which we
investigate empirically in Section~\ref{sec-experiments}. Under certain
conditions, this result holds theoretically; for the proof, see the
supplementary appendix:

\begin{proposition}[Protecting Immutable Features]
\label{prp-mtblty}
Let $f_\theta(\mathbf{x})=\mathcal{S}(\mathbf{M}_\theta(\mathbf{x}))=\mathcal{S}(\Theta\mathbf{x})$ denote a linear classifier with softmax activation $\mathcal{S}$ where $y\in\{1,...,K\}=\mathcal{K}$ and $\mathbf{x} \in \mathbb{R}^D$. Assume multivariate Gaussian class densities with common diagonal covariance matrix $\Sigma_k=\Sigma$ for all $k \in \mathcal{K}$, then protecting an immutable feature from the contrastive divergence penalty will result in lower classifier sensitivity to that feature relative to the remaining features, provided that at least one of those is discriminative and mutable.
\end{proposition}

\section{Experiments}\label{sec-experiments}

We seek to answer the following three research questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To what extent does the CT objective in Equation 1 induce models to
  learn plausible explanations?
\item
  To what extent does CT lead to more favorable AR outcomes in the
  presence of actionability constraints?
\item
  What are the effects of hyperparameter selection on CT?
\end{enumerate}

\subsection{Experimental Setup}\label{experimental-setup}

Our focus is the improvement in explainability (Def.
\ref{def-explainability}). Thus, we primarily look at the plausibility
and cost of faithfully generated counterfactuals at test time. Other
metrics, such as validity and redundancy, are reported in the
supplementary appendix. To measure the cost, we follow the standard
proxy of distances (\(\ell_1\)-norm) between factuals and
counterfactuals. For plausibility, we assess how similar CEs are to the
observed samples in the target domain,
\(\mathbf{X}^+\subset\mathcal{X}^+\). We rely on the metric used by
\citet{altmeyer2024faithful},
\begin{equation}\phantomsection\label{eq-impl-dist}{
\text{IP}(\mathbf{x}^\prime,\mathbf{X}^+) = \frac{1}{\lvert\mathbf{X}^+\rvert}\sum_{\mathbf{x} \in \mathbf{X}^+} \text{dist}(\mathbf{x}^{\prime},\mathbf{x})
}\end{equation} and introduce a novel divergence metric,
\begin{equation}\phantomsection\label{eq-impl-div}{
\text{IP}^*(\mathbf{X}^\prime,\mathbf{X}^+) = \text{MMD}(\mathbf{X}^\prime,\mathbf{X}^+)
}\end{equation} where \(\mathbf{X}^\prime\) denotes a collection of
counterfactuals and \(\text{MMD}(\cdot)\) is the unbiased estimate of
the squared population maximum mean discrepancy, proposed by
\citet{gretton2012kernel}. The metric in Equation~\ref{eq-impl-div} is
equal to zero if and only if the two distributions are exactly the same,
\(\mathbf{X}^\prime=\mathbf{X}^+\).

For predictive performance, we use standard metrics, such as robust
accuracy estimated on adversarially perturbed data using FGSM
\citep{goodfellow2014explaining}.

We run experiments with three gradient-based generators: \emph{Generic}
of \citet{wachter2017counterfactual} as a simple baseline approach,
\emph{REVISE} \citep{joshi2019realistic} that aims to generate plausible
counterfactuals using a surrogate Variational Autoencoder (VAE), and
\emph{ECCo}---the generator of \citet{altmeyer2024faithful} without the
conformal prediction component---as a method that directly targets both
faithfulness and plausibility of the counterfactuals.

We make use of nine classification datasets common in the CE/AR
literature. Four of them are synthetic with two classes and different
characteristics: linearly separable clusters (\emph{LS}), overlapping
clusters (\emph{OL}), concentric circles (\emph{Circ}), and interlocking
moons (\emph{Moon}). They are generated using the library of
\citet{altmeyer2023explaining} and we present them in the supplementary
appendix. Next, we have four real-world binary tabular datasets:
\emph{Adult} (Census data) of \citet{becker1996adult2}, California
housing (\emph{CH}) of \citet{pace1997sparse}, Default of Credit Card
Clients (\emph{Cred}) of \citet{yeh2016default}, and Give Me Some Credit
(\emph{GMSC}) from \citet{kaggle2011give2}. Finally, for the convenience
of illustration, we use the 10-class \emph{MNIST}
\citep{lecun1998mnist}.

To assess CT, we investigate the improvements in performance metrics
when using it on top of a weak baseline (BL): a multilayer perceptron
(\emph{MLP}). This is the best way to get a clear picture of the
effectiveness of CT, and it is consistent with evaluation practices in
the related literature
\citep{goodfellow2014explaining, ross2021learning, teney2020learning}.

\subsection{Experimental Results}\label{experimental-results}

Our main quantitative results for \emph{MLP} models are summarised in
Table~\ref{tbl-main}, which presents average outcomes along with
bootstrapped two standard errors. The following example motivates CT and
illustrates how to read Table~\ref{tbl-main}.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{paper_files/mediabag/../../paper/figures/poc.pdf}}

}

\caption{\label{fig-poc}Illustration of how CT improves model
explainability: (a) conventional training, all mutable; (b) CT, all
mutable; (c) conventional, \emph{age} immutable; (d) CT, \emph{age}
immutable. The linear decision boundary is shown in green along with
training data colored according to ground-truth labels: \(y^-=1\) (blue)
and \(y^+=2\) (orange). Stars indicate counterfactuals in the target
class.}

\end{figure*}%

\subsubsection{Synthetic Example: Prediction of Credit Card
Defaults.}\label{synthetic-example-prediction-of-credit-card-defaults.}

Figure~\ref{fig-poc} presents results for a linear classifier fitted to
\emph{LS} that complies with the data assumptions in Proposition
\ref{prp-mtblty}. The four panels show the outcomes for different
training procedures: in panels (a) and (c) we have trained the models
conventionally, while in panels (b) and (d) we have applied CT. For
illustrative purposes, suppose the first feature represents \emph{debt}
(mutable) and the second feature represents \emph{age} (immutable) of
loan applicants seeking counterfactual explanations for moving to the
target class: loan provided (orange).

In all four cases, it is possible to generate valid counterfactuals
(stars) for unsuccessful applicants (blue). They cross the decision
boundary (green) into the target class, but their quality differs. In
panel (a), they are not plausible: they do not comply with the
distribution of the factuals in \(y^+\) to the point where they form a
clearly discernible cluster. In panel (b), they are highly plausible,
meeting the first objective of Def. \ref{def-explainability}. This
difference in outcomes is quantified for the non-linear MLP in the first
two columns of Table~\ref{tbl-main} as the \(\%\)-reduction in
implausibility: it is substantial and statistically significant for
\emph{LS} across both metrics, the distance-based \(IP\) (29\%) and
divergence-based \(IP^{*}\) (55\%).

In panel (c) of Figure~\ref{fig-poc}, the CEs involve substantial
reductions in \emph{debt} for younger applicants. By comparison,
counterfactual paths are shorter on average in panel (d) where we have
protected the immutable \emph{age} as described in
Section~\ref{sec-constraints}. Due to the classifier's lower sensitivity
to \emph{age}, recommendations with respect to \emph{debt} are much more
homogenous and do not unfairly punish younger individuals. These CEs are
also plausible with respect to the mutable feature, despite requiring
smaller debt reductions on average, resulting in smaller costs to
individuals. This result is quantified for the non-linear case in column
three of Table~\ref{tbl-main}, which shows the \(\%\)-reduction in costs
averaged across valid counterfactuals. Once again, the impact of CT is
statistically significant and substantial (14\%). Thus, we consider the
model in panel (d) as the most explainable according to Def.
\ref{def-explainability}. Next, we present the results for all remaining
datasets.

\subsubsection{Plausibility.}\label{sec-plaus}

We find that CT generally leads to substantial and statistically
significant improvements in plausibility: average reductions in \(IP\)
range from around 7\% for \emph{MNIST} to almost 60\% for \emph{Circ};
for the real-world tabular datasets they are around 12\% for both
\emph{CH} and \emph{Cred} and almost 25\% for \emph{GMSC}; for
\emph{Adult} and \emph{OL} we find no significant impact of CT on
\(IP\). Reductions in \(IP^{*}\) are even more substantial and generally
statistically significant, although the average degree of uncertainty is
higher than for \(IP\): average reductions range from around 20\%
(\emph{Moons}) to almost 90\% (\emph{Circ}). The only negative findings
for \emph{OL} and \emph{MNIST} are statistically insignificant and, for
MNIST, do not align with qualitative findings, which are much more
plausible for CT (Figure~\ref{fig-mnist}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{../../paper/figures/mnist_body.png}}

}

\caption{\label{fig-mnist}Sample explanations for \emph{MNIST} for BL
(top) and CT (bottom). First column is a random factual 0 (blue).
Columns 2 to 5 are corresponding \emph{ECCo} counterfactuals in target
classes 1 to 4. Columns 6 to 10 show integrated gradients averaged over
all test images in classes 5 to 9.}

\end{figure}%

\subsubsection{Actionability.}\label{sec-act}

We also find that CT can reduce sensitivity to immutable, protected
features and thus lead to less costly counterfactual outcomes as shown
in Figure~\ref{fig-poc}. In column three of Table~\ref{tbl-main}, we
impose mutability constraints on selected features and compute the
reduction in average costs of CEs associated with CT compared to the
baseline: for synthetic datasets, we always protect the first feature;
for all real-world tabular datasets we could identify and protect an
\emph{age} variable; for \emph{MNIST}, we protect the five top and
bottom rows of digits. Reductions in costs are overwhelmingly positive
and significant of up to nearly 60\% for \emph{GMSC}. While the
estimated cost reductions for \emph{Adult} and \emph{MNIST} are not
significant, Figure~\ref{fig-mnist} (columns 6-10) demonstrates that CT
does have the expected effect: sensitivity to protected features as per
integrated gradients is drastically reduced; details of this experiment
are reported in the supplementary appendix. In the case of \emph{Cred},
average costs increase, likely because any potential benefits from
protecting the \emph{age} are outweighed by the increase in costs
required for greater plausibility.

\subsubsection{Predictive Performance.}\label{sec-pred}

Test accuracy for CT is virtually identical to the baseline for
\emph{Adult}, \emph{Circ}, \emph{LS}, \emph{Moon}, and \emph{OL}, and
even slightly improved for \emph{Cred}. Exceptions to this general
pattern are \emph{MNIST}, \emph{CH}, and \emph{GMSC}, for which we
observe a reduction in test accuracy of 2, 5, and 15 percentage points
respectively. When looking at robust test accuracies (Acc.\(^*\)) for
these datasets in particular, we find that CT strongly outperforms the
baseline. In fact, we observe that CT improves adversarial robustness on
all datasets.

\begin{table*}

\caption{\label{tbl-main}Key performance metrics and bootstrapped
\emph{two} standard errors for all datasets. \textbf{Plausibility}
(columns 1-2): percentage reduction in implausibility for \(IP\) and
\(IP^*\), respectively; \textbf{Actionability} (column 3): percentage
reduction in costs with protected features. \textbf{Accuracy} (columns
4-7): test accuracies and robust accuracies (\(\text{Acc}^*\)) for CT
and the baseline (BL). Counterfactual outcomes in columns 1-3 are
aggregated across bootstrap samples and varying degrees of the energy
penalty \(\lambda_{\text{egy}}\) used for \emph{ECCo} at test time.
Standard errors for accuracy are bootstrapped from the test set.}

\centering{

\input{tables/main.tex}

}

\end{table*}%

\subsubsection{Hyperparameter settings.}\label{sec-hyperparameters}

We test the impact of three types of hyperparameters. Here we focus on
the highlights; full results are available in the supplementary
appendix.\\
\indent First, we note that CT is highly sensitive to the choice of a CE
generator and its hyperparameters but (1) there are manageable patterns,
and (2) we can usually identify settings that improve either
plausibility or cost, and often both of them at the same time. For
example, \emph{REVISE} tends to perform the worst, most likely because
it uses a surrogate VAE to generate counterfactuals which impedes
faithfulness \citep{altmeyer2024faithful}. Increasing \(T\), the maximum
number of steps, generally yields better outcomes because more CEs can
mature in each training epoch. The impact of \(\tau\), the required
decision threshold is more difficult to predict. On ``harder'' datasets
it may be difficult to satisfy high \(\tau\) for any given sample (i.e.,
also factuals) and so increasing this threshold does not seem to
correlate with better outcomes. In fact, \(\tau=0.5\) generally leads to
optimal results as it is associated with high proportions of mature
counterfactuals.\\
\indent Second, the strength of the energy regularization,
\(\lambda_{\text{reg}}\) is highly impactful and leads to poor
performance in terms of decreased plausibility and increased costs if
insufficiently high. The sensitivity with respect to
\(\lambda_{\text{div}}\) and \(\lambda_{\text{adv}}\) is much less
evident. While high values of \(\lambda_{\text{reg}}\) may increase the
variability in outcomes when combined with high values of
\(\lambda_{\text{div}}\) or \(\lambda_{\text{adv}}\), this effect is not
very pronounced.\\
\indent Third, the effectiveness and stability of CT is positively
associated with the number of counterfactuals generated during each
training epoch. A higher number of training epochs is also beneficial.
Interestingly, we observed desired improvements when CT was combined
with conventional training and applied only for the final 50\% of epochs
of the complete training process. Put differently, CT can improve the
explainability of models in a fine-tuning manner.

\section{Discussion}\label{sec-discussion}

As our results indicate, counterfactual training produces models that
are more explainable. Nonetheless, these advantages come at the cost of
two important limitations.

\emph{Interventions on features have implications for fairness.} We
provide a method to modify the sensitivity of a model to certain
features, which can be misused by enforcing explanations based on
features that are more difficult to modify by a (group of) decision
subjects. Such abuse could result in an unfairly assigned burden of
recourse \citep{sharma2020certifai}, threatening the equality of
opportunity \citep{bell2024fairness}. Also, even if all immutable
features are protected, there may exist proxies that are theoretically
mutable, but preserve sufficient information about the principals to
hinder these protections. Indeed, deciding on the actionability of
features remains a major open challenge in the AR literature
\citep{venkatasubramanian2020philosophical}.

\emph{CT increases the training times.} Like adversarial training, CT is
more resource-intensive than conventional regimes. Higher numbers of CEs
improve the quality of the learned representations but they also
increase the number of computations. As our codebase is not performance
optimized, grids of around 300 settings for the largest datasets in our
experiments took up to four hours using 34 2GB CPUs (see supplementary
appendix). Other than optimization, three factors mitigate this effect:
(1) CT yields itself to parallel execution; (2) it amortizes the cost of
CEs for the training samples; and (3) it can be used to fine-tune
conventionally-trained models.

We also highlight three key directions for future research. Firstly, it
is an interesting challenge to extend CT beyond classification settings.
Our formulation relies on the distinction between non-target class(es)
and target class(es), requiring the output space to be discrete. Thus,
it does not apply to ML tasks where the change in outcome cannot be
readily discretized. Focus on classification is a common choice in
research on CEs and AR; other settings have attracted some interest,
e.g., regression \citep{spooner2021counterfactual}, but there is little
consensus how to robustly extend the notion of CEs.\\
Secondly, CT is susceptible to training instabilities, as also
recognized for the related JEMs \citep{grathwohl2020your}. CT is exposed
to two sources of instabilities: (1) the energy-based contrastive
divergence term, \(\text{div}(\cdot)\), in Equation~\ref{eq-div}, and
(2) the underlying explainers. We find several promising ways to
mitigate this problem: regularizing energy (\(\lambda_{\text{reg}}\)),
generating sufficiently many counterfactuals during each epoch, and
including only mature counterfactuals in \(\text{div}(\cdot)\).\\
Finally, we believe that it is possible to considerably improve
hyperparameter selection procedures, and thus performance. We have
relied exclusively on grid searches, but future work could benefit from
more sophisticated approaches.

\section{Conclusions}\label{sec-conclusions}

State-of-the-art machine learning models are prone to learning complex
representations that cannot be interpreted by humans. Existing
explainability solutions cannot guarantee that explanations agree with
these learned representation. As a step towards addressing this
challenge, we introduce counterfactual training, a novel training regime
that incentivizes highly-explainable models. Through extensive
experiments, we demonstrate that CT satisfies this objective while
promoting robustness and preserving the predictive performance of
models. Explanations generated from CT-based models are both more
plausible (compliant with the underlying data-generating process) and
more actionable (compliant with user-specified mutability constraints),
and thus meaningful to their recipients. We also show that CT can be
used to fine-tune conventionally-trained models and achieve similar
gains. Lastly, our work highlights the value of simultaneously improving
models and their explanations.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{../quarto_ecml/bibliography.bib}
\end{document}
