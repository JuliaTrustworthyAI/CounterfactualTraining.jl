
```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)

using CTExperiments
using CTExperiments.AlgebraOfGraphics
using CTExperiments.Makie
using CTExperiments.CSV
using CTExperiments.DataFrames
using CTExperiments.Plots
using CTExperiments.PrettyTables
using CTExperiments.StatsBase
using Random
using Serialization

using DotEnv
DotEnv.load!()
```

```{julia}
res_dir = joinpath(ENV["FINAL_MAIN_RESULTS"],"single")
mtbl_dir = joinpath(ENV["FINAL_MAIN_RESULTS"],"mutability")
```

\FloatBarrier

# Details on Main Experiments {#sec-app-main .appendix}

## Final Hyperparameters

As discussed the main paper, CT is sensitive to certain hyperparameter choices. We study the effect of many hyperparameters extensively in @sec-app-grid of this appendix. For the main results, we tune a small set of key hyperparameters (@sec-app-tune). The final choices for the main results are presented for each data set in @tbl-final-params along with training, test and batch sizes.

::: {#tbl-final-params}

::: {.content-hidden unless-format="pdf"}

```{julia}
#| output: asis

df = final_params(res_dir)
float_cols = ["decision_threshold", "lambda_energy_reg"]
get_table_inputs(df, nothing; backend=Val(:latex)) |>
    inputs -> tabulate_results(inputs; wrap_table=false, table_type=:tabular, formatters=PrettyTables.ft_round(2, findall([x in float_cols for x in names(df)])))
```

:::

Final hyperparameters used for the main results presented in the main paper. Any hyperparameter not shown here is set to its default value (@nte-train-default).

:::

<!-- NOTE: Computing final results below -->

```{julia}
#| eval: false

df_mtbl = aggregate_ce_evaluation(
    mtbl_dir; 
    ratio=true, 
    y="distance", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = final_table(res_dir; tbl_mtbl=df_mtbl)
Serialization.serialize("paper/experiments/output/final_table.jls", df)
```

```{julia}
#| eval: false

df = Serialization.deserialize("paper/experiments/output/final_table.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
for fname in ["paper/tables/main.tex","paper/preprint/tables/main.tex"]
    tabulate_results(
        inputs; 
        table_type=:tabular, 
        save_name=fname, 
        wrap_table=false, 
    )
end
```

<!-- NOTE: Generating robust performance plot below -->

```{julia}
#| eval: false

plt = CTExperiments.plot_performance(res_dir, eps=range(0.0, 0.1, length=10) |> collect, adversarial=true, byvars=["objective", "eps"], drop_synthetic=false)
plt_out = draw(plt, figure=(size=(1000, 150),), axis=(yticks=[0.0,0.5,1.0], limits=(nothing, (0,1)), xticklabelsvisible=false, xticksvisible=false))
save("paper/figures/acc.png", plt_out; px_per_unit=3)
```

### Confidence Intervals

@tbl-ci present the exact confidence intervals (99%) for the difference in mean outcomes on which we base our assessment of statistical significance in the main paper. Grouped by evaluation metrics (Variable) and dataset (Data), the table presents the mean outcomes for CT and BT and finally the lower bound (LB) and upper bound (UB) of the confidence interval. To compute the intervals, we used the percentile method for bootstrapped confidence intervals: the lower and upper bound represent the $\alpha/2$- and $(1-\alpha / 2)-$quantile of the bootstrap distribution, respectively, for $\alpha=0.01$.

```{julia}
#| eval: false

lvls = [0.90, 0.95, 0.99]

# Mutability:
y = "distance"
df_mtbl = aggregate_ce_evaluation(
    mtbl_dir; 
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
    drop_models=["linear"]
)
df_mtbl.measure .= y

# Distance-based:
y = "plausibility_distance_from_target"
df_ip = aggregate_ce_evaluation(
    res_dir;
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
) 
df_ip.measure .= y 

# Divergence-based:
y = "mmd"
df_mmd = aggregate_ce_evaluation(
    res_dir;
    y,
    return_sig_level=false, 
    conf_int=lvls,
    agg_further_vars=["run", "lambda_energy_eval"],
)
df_mmd.measure .= y
df = vcat(df_mtbl, df_ip, df_mmd)
Serialization.serialize("paper/experiments/output/ci.jls", df)
```

::: {#tbl-ci}

```{julia}
#| output: asis

df = Serialization.deserialize("paper/experiments/output/ci.jls")
df = select(df, :variable, :measure, :dataset, :full, :vanilla, "99.0%")
df.variable = (x -> LatexCell(replace(x.data, " \$(-%)\$" => ""))).(df.variable)
sort!(df, [:measure, :dataset])
df = select(df, Not(:measure))
df = transform(df, "99.0%" => AsTable) |>
    df -> select(df, Not("99.0%"))
rename!(df, :dataset => :data, :x1 => :LB, :x2 => :UB, :full => :CT, :vanilla => :BL)
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
    sort_by_data = false,
    hlines = :none,
)

```

Mean outcomes for CT and BL along with bootstrapped confidence intervals (99%) for difference in mean outcomes grouped by dataset and evaluation metric. Column LB and UB show the lower and upper bound of the intervals, respectively, and computed using the percentile method (for significance, interval should not include zero). The underlying counterfactual evaluations are the same as the ones used to produce the main table in the paper. 

:::

### Qualitative Findings for Image Data

```{julia}
#| eval: false

Random.seed!(42)    # change seed for different outcome
overwrite = false   # set to `true` if you want to overwrite the file
fname = joinpath(res_dir,"mlp/mnist/grid_config.toml")
output_dir = mkpath("paper/experiments/output/extra/")
gen = CTExperiments.GeneratorParams(lambda_energy=25.0)     # instead of a high penalty, you can also use e.g. 1.0 and many iterations e.g 1000
conv = "max_iter"
cfg = EvaluationConfig(
    grid_file=fname, 
    counterfactual_params=(conv=conv, generator_params=gen), 
    save_dir=output_dir,
    test_time=true
)
generate_factual_target_pairs(cfg; overwrite, nce=10)
plts = plot_ce(MNIST(), cfg; byvars="objective")
savefig(plts["objective"]["full"]["ecco"], "paper/figures/mnist_mlp.png")
savefig(plts["objective"]["vanilla"]["ecco"], "paper/figures/mnist_mlp_vanilla.png")
```

@fig-mnist shows much more plausible (faithful) counterfactuals for a model with CT than the model with conventional training (@fig-mnist-vanilla).

::: {layout="[10,-2,10]" layout-valign="top"}
![Counterfactual images for *MLP* with counterfactual training. Factual images are shown on the diagonal, with the corresponding counterfactual for each target class (columns) in that same row. The underlying generator, *ECCCo*, aims to generate counterfactuals that are faithful to the model [@altmeyer2024faithful].](/paper/figures/mnist_mlp.png){#fig-mnist}

![The same setup, factuals, model architecture and generator as in @fig-mnist, but the model was trained conventionally.](/paper/figures/mnist_mlp_vanilla.png){#fig-mnist-vanilla}
:::

### Integrated Gradients

We make use of integrated gradients (IG) proposed by @sundararajan2017ig to empirically evaluate the feature protection mechanism in CT. We choose this approach because it produces theoretically sound results, works well for non-linear models, and remains relatively inexpensive.

IG calculates the contribution of each input feature towards a specific prediction by approximating the integral of the model output with respect to its input, using a set of samples that linearly interpolate between a test instance and some baseline instance [@sundararajan2017ig]. This process produces a vector of real numbers, one per input feature, which informs about the contribution of each feature to the prediction. For example:

- a large positive value indicates that a feature has strong positive influence on the classification (i.e., increases the score for a class);
- a small negative value indicates that a feature has weak negative influence on the classification (i.e., decreases the score for a class).

To calculate the contributions, IG compares the output to a baseline. The selection of an appropriate baseline is an important design decision — it should produce a "neutral" prediction to avoid capturing effects that cannot be directly attributed to the model [@sundararajan2017ig; @sturmfels2020visualizing]. To remain consistent in our evaluations, we use a baseline drawn at random from a uniform distribution, $\mathcal{U}(-1,1)$, for all datasets. This aligns with standard evaluation practices for IG.

We run IG on models trained on all datasets to compare their sensitivity to features that were protected using CT:

- for synthetic datasets, this is always the first feature
- for real-world tabular datasets, this is always *age*
- for MNIST, this is first five and last five rows of pixels

As IG outputs are not bounded (i.e., they are arbitrary real numbers), it becomes a challenge to meaningfully compare IG outputs of different models — ones that are trained conventionally, and ones that underwent counterfactual training. For our purposes, we observe with reference to our Proposition, that we are interested estimating changes in the relative contribution of protected features compared to mutable ones. Thus, to meaningfully compare integrated gradients for different models and to accommodate for variable ranges of outputs in absolute terms, we standardize the integrated gradients across features. 

Let $\mathbf{g}_d$ denote the estimated IG for feature $d$. Then in the case of 2D synthetic datasets we find that taking the absolute value of the outputs, $|\mathbf{g}_d|$, and then dividing them by a $\max(\mathbf{g}) - \min(\mathbf{g})$ term allows us to make the most meaningful comparison. In the case of real-world datasets we choose to normalize the values to a $[0,1]$ range instead. We compare the (average) sensitivity to the features that were protected for CT models. Once again we use bootstrapping (100 rounds, 2500 samples per round) to establish the significance of our results. 


```{julia}
#| output: asis
#| eval: false

df = Serialization.deserialize(joinpath(mtbl_dir, "ig.jls"))
CTExperiments.bootstrap_ci_table(df, "ig.tex")
```

### Costs and Validity

In @tbl-panel, we present additional outcomes for common evaluation metrics: @tbl-costs presents the average reduction in costs of counterfactuals for CT vs. BL with no mutability constraints, i.e. corresponding to the first two columns in the main table of the paper; @tbl-val shows the corresponding average validities; finally, @tbl-val-mtbl shows average validities for the case with mutability constraints, i.e. corresponding to the third columns in the main table of the paper.

As noted in the discussion section of the main paper, we observe mixed results results here. Average costs in terms of distances from factual values decrease for most datasets, which is positively surprising since improved plausibility requires counterfactuals to travel further into the target domain than minimum distance counterfactuals. It appears that in these cases faithful counterfactuals for the baseline model still end up far away from their initial starting points, but not close enough for samples in the target domain to be plausible. In that sense, CT can be seen to improve both plausibility and costs for faithful CE. In some cases though (*LS*, *CH*, *MNIST*), we do seem to observe the tradeoff between plausibility and costs play out, as we would expect (compare panels (a) and (b) of Figure 1 in the main paper for reference).

Concerning validity, we find that can lead to substantial reductions and only increases average validity compared to the baseline in one case (*Circ*). As noted in the discussion section of the main paper, this result does not surprise us: by design, CT shrinks the solution space for valid counterfactual explanations, thus making it "harder" to reach validity compared to the baseline model. Note that for a number of reasons this should not be seen as problematic:

1. Validity of gradient-based CE is a function on the number of steps and the step size which we both kept fixed during evaluation: simply adjusting $T=50$ to higher values or choosing a larger step size will lead to higher rates of validity.
2. Even though reaching validity is sometimes "harder" in terms of the necessary number of steps for a given step size, we have already shown that the average distances that counterfactuals need to travel decrease for most datasets. Users care about costs in terms of feature distances, not search iteration steps.
3. From a philosophical perspective on algorithmic recourse, validity in and off itself is not a sufficient desideratum for counterfactuals. In fact, @venkatasubramanian2020philosophical propose introducing an upper bound on costs of the flipset (i.e. the set of valid CE), arguing that valid but highly costly counterfactuals are not useful to individuals in practice. In a similar fashion, it could be argued that there should be an upper bound on the implausibility of counterfactuals in the flipset.

::: {#tbl-panel layout="[[40,-20,40], [100]]"}

```{julia}
#| eval: false

df = final_table(
    res_dir;
    ce_var=["distance"], 
    agg_further_vars=[["run", "lambda_energy_eval"]], 
    total_uncertainty=false,
)
Serialization.serialize("paper/experiments/output/costs.jls", df)
```

::: {#tbl-costs}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/costs.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
al = [
    "l", 
    "S[table-format=2.2(1.2)]", 
]
tabulate_results(
    inputs, al; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Reduction in average costs for CT vs. the baseline. Results correspond to the case with no mutability constraints in the main table of the paper.

:::

```{julia}
#| eval: false

# Unconstrained:
df = aggregate_ce_evaluation(
    res_dir; 
    ratio=false, 
    y="validity", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = select(df, Not(:se, :variable)) |> df -> unstack(df, :objective, :mean)
rename!(df, :dataset => :data, :Full => :CT, :Vanilla => :BL)
Serialization.serialize("paper/experiments/output/validity.jls", df)

# Mutability constrained:
df = aggregate_ce_evaluation(
    mtbl_dir; 
    ratio=false, 
    y="validity", 
    agg_further_vars=["run", "lambda_energy_eval"], 
    total_uncertainty=false, 
    drop_models=["linear"]
)
df = select(df, Not(:se, :variable)) |> df -> unstack(df, :objective, :mean)
rename!(df, :dataset => :data, :Full => :CT, :Vanilla => :BL)
Serialization.serialize("paper/experiments/output/validity_mtbl.jls", df)
```

::: {#tbl-val}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/validity.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Average validities of counterfactuals for CT and BL. Unconstrained case.

:::

::: {#tbl-val-mtbl}

```{julia}
#| output: asis
#| eval: true

df = Serialization.deserialize("paper/experiments/output/validity_mtbl.jls")
inputs = get_table_inputs(df, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df)-1)...],
    wrap_table = false,
)

```

Average validities of counterfactuals for CT and BL. Mutability constrained case.

:::

Costs and validity.

:::

```{julia}
#| output: asis
#| eval: false

df1 = Serialization.deserialize("paper/experiments/output/validity.jls")
df2 = Serialization.deserialize("paper/experiments/output/validity_mtbl.jls")
# Rename columns in df1
df1_renamed = rename(df1, :CT => Symbol("CT mut."), :BL => Symbol("BL mut."))
# Rename columns in df2  
df2_renamed = rename(df2, :CT => Symbol("CT constr."), :BL => Symbol("BL constr."))
# Remove the duplicate 'data' column from df2_renamed and horizontally concatenate
df_combined = hcat(df1_renamed, select(df2_renamed, Not(:data)))
# Calculate column averages (excluding the data column)
avg_row = DataFrame(
    :data => "Avg.",
    Symbol("CT mut.") => mean(skipmissing(df_combined[:, Symbol("CT mut.")])),
    Symbol("BL mut.") => mean(skipmissing(df_combined[:, Symbol("BL mut.")])),
    Symbol("CT constr.") => mean(skipmissing(df_combined[:, Symbol("CT constr.")])),
    Symbol("BL constr.") => mean(skipmissing(df_combined[:, Symbol("BL constr.")]))
)
# Append the average row
df_combined = vcat(df_combined, avg_row)
inputs = get_table_inputs(df_combined, nothing; backend=Val(:latex))
tabulate_results(
    inputs; 
    table_type=:tabular,
    alignment = [:l, fill(:c, ncol(df_combined)-1)...],
    wrap_table = false,
)

```

