```{julia}
projectdir = splitpath(pwd()) |>
    ss -> joinpath(ss[1:findall([s == "CounterfactualTraining.jl" for s in ss])[1]]...) 
cd(projectdir)

using CTExperiments
using CTExperiments.CSV
using CTExperiments.DataFrames
using CTExperiments.StatsBase
using Logging

using DotEnv
DotEnv.load!()
```

## Training Hyperparameters {#sec-app-training}

@nte-train-default presents the default hyperparameters used during training.

::: {#nte-train-default .callout-note}

## Training Phase

```{julia}
#| output: asis

dict = Logging.with_logger(Logging.NullLogger()) do 
    CTExperiments.to_dict(Experiment()) 
end
println(CTExperiments.dict_to_quarto_markdown(dict, filter_empty=false))
```

:::

## Evaluation Details {#sec-app-eval}

### Counterfactual Outcomes

For all of our counterfactual evaluations, we proceed as follows: for each dataset we run $J$ bootstrap rounds ("No. Runs") to account for stochasticity (@nte-eval-default); for each bootstrap round, we randomly draw factual and target pairs; then, for each model, we draw samples from the test set (with replacement) for which the model predicts the randomly chosen factual class; finally, we generate multiple counterfactuals ("No. Counterfactuals") and evaluate the outcomes (@nte-eval-default). This is in line with standard practice in the related literature on CE (see e.g. @schut2021generating). For our final results presented in the main paper, we rely on held-out test sets for evaluation. For tuning purposes we rely on training and/or validation sets.

@nte-eval-default presents the default hyperparameters used during evaluation for tuning purposes. For the main results presented in the paper, we use larger evaluations, specifically:

- "No. Runs": We set the number of bootstrap rounds to $J=100$ for all datasets. 
- "No. Individuals": In each round we draw 1,250, 500 and 125 samples for synthetic datasets, real-world tabular datasets and *MNIST*, respectively, across five different values for the strength of the energy penalty of *ECCCo* at test time, $\lambda_{\text{egy}}\in\{0.1, 0.5, 1.0, 5.0, 10.0\}$.


::: {#nte-eval-default .callout-note}

## Evaluation Phase

```{julia}
#| output: asis

dict = Logging.with_logger(Logging.NullLogger()) do  
    CTExperiments.to_dict(EvaluationGrid(ExperimentGrid()))
end
println(CTExperiments.dict_to_quarto_markdown(dict["counterfactual_params"], filter_empty=false))
```

:::

### Predictive Performance

To assess (robust) predictive performance, we evaluate model accuracy on (adversarially perturbed) test data. To generate adversarial examples we use the Fast Gradient Sign Method (FGSM) [@goodfellow2014explaining]. For the main results in the paper, we choose a range of values $\epsilon=[0.0,0.1]$. In some places of this appendix, you will also find predictive performance evaluations in terms of the F1-score.
