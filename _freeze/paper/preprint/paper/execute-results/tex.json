{
  "hash": "3d54862fa667f83628c75bc6be012b8d",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\njulia: \n  exeflags: [\"--project=../experiments/\"]\nformat:\n  arxiv-pdf:\n    keep-tex: true  \n    linenumbers: true\n    doublespacing: false\n    runninghead: \"Counterfactual Training (A Preprint)\"\n    authorcols: true\n---\n\n\n# Abstract\n\nCounterfactual Explanations (CE) have emerged as a popular method to explain the predictions made by opaque machine learning models in a post-hoc fashion. We propose a novel approach that leverages counterfactuals during the training phase of models. \n\n\n\n# Introduction\n\n\n\n# Related Literature {#sec-lit}\n\n## Background on Counterfactual Explanations\n\n[@wachter2017counterfactual;@joshi2019realistic;@altmeyer2024faithful]\n\n## Learning Representations\n\n> For example, joint-energy models\n\n## Generalization and Robustness\n\n@sauer2021counterfactual generate counterfactual images for MNIST and ImageNet through independent mechanisms (IM): each IM learns class-conditional input distributions over a specific lower-dimensional, semantically meaningful factor, such as *texture*, *shape* and *background*. The demonstrate that using these generated counterfactuals during classifier training improves model robustness. Similarly, @abbasnejad2020counterfactual argue that counterfactuals represent potentially useful training data in machine learning, especially in supervised settings where inputs may be reasonably mapped to multiple outputs. They, too, demonstrate the augmenting the training data of image classifiers can improve generalization. \n\n@teney2020learning propose an approach using counterfactuals in training that does not rely on data augmentation: they argue that counterfactual pairs typically already exist in training datasets. Specifically, their approach relies on, firstly, identifying similar input samples with different annotations and, secondly, ensuring that the gradient of the classifier aligns with the vector between pairs of counterfactual inputs using the cosine distance as a loss function (referred to as *gradient supervision*) (***this might be useful for our task as well***). In the natural language processing (NLP) domain, counterfactuals have similarly been used to improve models through data augmentation: @wu2021polyjuice, propose POLYJUICE, a general-purpose counterfactual generator for language models. They demonstrate empirically that augmenting training data through POLYJUICE counterfactuals improves robustness in a number of NLP tasks. \n\n## Link to Adversarial Training\n\n@freiesleben2022intriguing propose two definitional differences between Adversarial Examples (AE) and Counterfactual Explanations (CE): firstly, and more importantly according to the authors, the term AE implies missclassification, which is not the case for CE (***this might be a useful notion for use to distinguish between adversarials and explanations during training***); secondly, they argue that closeness plays a more critical role in the context of CE but confess that even counterfactuals that are not close might be relevant explanations. @pawelczyk2022exploring show that CE and AE are equivalent under certain conditions and derive upper bounds on the distances between them. \n\n## Closely Related\n\n@guo2023counternet are the first to propose end-to-end training pipeline that includes counterfactual explanations as part of the training prodeduce. In particular, they propose a specific network architecture that includes a predictor and CE generator network (***akin a GAN?***), where the parameters of the CE generator network are learnable. Counterfactuals are generated during each training iteration and fed back to the predictor network (***here we are aligned***). In contrast, we impose no restrictions on the neural network architecture at all. (***to ensure the one-hot encoding of categorical features is maintained, they simple use softmax (might be interesting for CE.jl)***) Interestingly, the authors find that their approach is sensitive to the choice of the loss function: only MSE seems to lead to good performance. They also demonstrate theoretically, that the objective function is difficult to optimize due to divergent gradients and suffers from poor adversarial robustness. (***because partial gradients with respect to the classification loss component and the counterfactual validity component point in opposite directions***). To mitigate these issues, the authors use block-wise gradient descent: they first update with respect to classification loss and then use a second update with respect to the other loss components (***this might be useful for our task as well***). @ross2021learning propose a way to train models that are guaranteed to provide recourse for individuals with high probability. The approach builds on adversarial training (***here we are aligned***), where in this context adversarial examples are actively encouraged to exist, but only target attacks with respect to the positive class. The proposed method allows for imposing a set of actionable recourse ex-ante: for example, users can impose mutability constraints for features (***here we are aligned***). (***To solve their objective function more efficiently, they use a first-order Taylor approximation to approximate the recourse loss component (might be applicable in our case)***)\n\n@luu2023counterfactual introduce Counterfactual Adversarial Training (CAT) with intention of improving generalization and robustness of language models. Specifically, they propose to proceed as follows: firstly, identify training samples that are subject to high predictive uncertainty (entropy); secondly, generate counterfactual explanations for those samples; and, finally, finetune the model on the augmented dataset that includes the generated counterfactuals.\n\n\n\n# Counterfactual Training\n\n\n\n# Experiments {#sec-experiments}\n\n## Experimental Setup\n\n## Experimental Results\n\n\n\n# Discussion {#sec-discussion}\n\n\n\n# Conclusion {#sec-conclusion}\n\n\n\n\n\n\n\n\n\n\n\n\n# Appendix\n\n## Training Details\n\n## Initial Grid Search\n\n\n\n\n\n\nFor the initial round of experiments we \n\n### Generator Parameters\n\nThe hyperparameter choices are shown in @exr-gen-params-first-run:\n\n::: {#exr-gen-params-first-run}\n\n## Parameters\n\n\n- **generator_params**:\n    - **lambda_cost**: 0.0, 0.001, 0.1\n    - **lambda_energy**: 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 15.0\n    - **lr**: 1.0\n    - **maxiter**: 20, 50, 100\n    - **opt**: sgd\n- **generator_type**: ecco, generic, omni, revise\n- **training_params**:\n    - **objective**: full, vanilla\n\n\n\n\n:::\n\n\n\n\n\n\n#### Linearly Separable \n\n- **Energy Penalty** (@tbl-lin_sep-lambda_energy_exper): *ECCo* generally does yield better results than *Vanilla* for higher choices of the energy penalty (10,15) during training. *Generic* performs poorly accross the board. *Omni* seems to have an anchoring effect, in that it never performs terribly but also never as good as the best *ECCo* results. *REVISE* performs poorly across the board.\n- **Cost (distance penalty)**: Results for all generators (except *Omni*) are quite bad, which can likely be attributed to extremely bad results for some choices of the **Energy Penalty** (results here are averaged). For *ECCo* and *Generic*, higher cost values generally lead to worse results.\n- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n- **Validity**: *ECCo* almost always valid except for very low values during training and high values at evaluation time. *Generic* often has poor validity.\n- **Accuracy**: Seems largely unaffected.\n\n\n\n\n\n\n::: {#tbl-lin_sep-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  full & 0.01 & \\textit{ECCo} & $-9.91 \\cdot 10^{11}$ & $2.25 \\cdot 10^{12}$ \\\\\n  full & 0.01 & \\textit{Generic} & $-5.71 \\cdot 10^{17}$ & $1.3 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.54}} & \\color{blue}{\\textbf{0.116}} \\\\\n  full & 0.01 & \\textit{REVISE} & -15.6 & 13.2 \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -4.28 & 3.52 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -4.45 & 3.47 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -4.91 & 4.24 \\\\\n  full & 0.05 & \\textit{ECCo} & $-5.63 \\cdot 10^{5}$ & $1.28 \\cdot 10^{6}$ \\\\\n  full & 0.05 & \\textit{Generic} & $-8.35 \\cdot 10^{17}$ & $1.9 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.53}} & \\color{blue}{\\textbf{0.114}} \\\\\n  full & 0.05 & \\textit{REVISE} & -15 & 12.6 \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -4.4 & 3.66 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -5.25 & 4.62 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -4.94 & 4.22 \\\\\n  full & 0.1 & \\textit{ECCo} & $-6.74 \\cdot 10^{5}$ & $1.53 \\cdot 10^{6}$ \\\\\n  full & 0.1 & \\textit{Generic} & $-1.72 \\cdot 10^{11}$ & $3.9 \\cdot 10^{11}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.56}} & \\color{blue}{\\textbf{0.124}} \\\\\n  full & 0.1 & \\textit{REVISE} & -15.6 & 13.2 \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -4.28 & 3.52 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -4.45 & 3.48 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -4.91 & 4.25 \\\\\n  full & 0.5 & \\textit{ECCo} & -11.8 & 9.83 \\\\\n  full & 0.5 & \\textit{Generic} & $-1.06 \\cdot 10^{18}$ & $2.42 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.54}} & \\color{blue}{\\textbf{0.123}} \\\\\n  full & 0.5 & \\textit{REVISE} & -15 & 12.6 \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -5.25 & 4.61 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -4.95 & 4.22 \\\\\n  full & 1 & \\textit{ECCo} & -11.5 & 11.1 \\\\\n  full & 1 & \\textit{Generic} & $-1.71 \\cdot 10^{11}$ & $3.88 \\cdot 10^{11}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.59}} & \\color{blue}{\\textbf{0.117}} \\\\\n  full & 1 & \\textit{REVISE} & -15.7 & 13.3 \\\\\n  vanilla & 1 & \\textit{ECCo} & -4.28 & 3.51 \\\\\n  vanilla & 1 & \\textit{Generic} & -4.44 & 3.47 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -5.11 & 4.46 \\\\\n  vanilla & 1 & \\textit{REVISE} & -4.91 & 4.25 \\\\\n  full & 5 & \\textit{ECCo} & -3.99 & 3.12 \\\\\n  full & 5 & \\textit{Generic} & $-4.88 \\cdot 10^{17}$ & $1.11 \\cdot 10^{18}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-2.53}} & \\color{blue}{\\textbf{0.117}} \\\\\n  full & 5 & \\textit{REVISE} & -14.6 & 12.1 \\\\\n  vanilla & 5 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 5 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -5.25 & 4.61 \\\\\n  vanilla & 5 & \\textit{REVISE} & -4.95 & 4.22 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.31}} & \\color{blue}{\\textbf{0.735}} \\\\\n  full & 10 & \\textit{Generic} & $-1.7 \\cdot 10^{11}$ & $3.86 \\cdot 10^{11}$ \\\\\n  full & 10 & \\textit{Omniscient} & -2.53 & 0.117 \\\\\n  full & 10 & \\textit{REVISE} & -15.5 & 13 \\\\\n  vanilla & 10 & \\textit{ECCo} & -4.28 & 3.51 \\\\\n  vanilla & 10 & \\textit{Generic} & -4.44 & 3.47 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -5.12 & 4.46 \\\\\n  vanilla & 10 & \\textit{REVISE} & -4.91 & 4.24 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.01}} & \\color{blue}{\\textbf{0.488}} \\\\\n  full & 15 & \\textit{Generic} & $-4.91 \\cdot 10^{17}$ & $1.12 \\cdot 10^{18}$ \\\\\n  full & 15 & \\textit{Omniscient} & -2.53 & 0.116 \\\\\n  full & 15 & \\textit{REVISE} & -14.4 & 11.7 \\\\\n  vanilla & 15 & \\textit{ECCo} & -4.4 & 3.65 \\\\\n  vanilla & 15 & \\textit{Generic} & -4.38 & 3.48 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -5.25 & 4.6 \\\\\n  vanilla & 15 & \\textit{REVISE} & -4.95 & 4.23 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Linearly Separable data by energy penalty.\n\n:::\n\n#### Moons\n\n- **Energy Penalty** (@tbl-moons-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, except for very low choices of the energy penalty during training for which it performs abismal. *Generic* performs quite badly across the board for high enough choices of the energy penalty at evaluation time. *Omni* has small positive effect. *REVISE* performs poorly across the board.\n- **Cost (distance penalty)**: *Generic* generally does better for higher values, while *ECCo* does better for lower values.\n- **Maximum Iterations**: No clear patterns recognizable, so it seems that smaller choices are ok. \n- **Validity**: *ECCo* generally achieves full validity except for very low choices the energy penalty during training and high choices at evaluation time. *Generic* performs poorly for high choices of the energy penalty during evaluation.\n- **Accuracy**: Largely unaffected although *ECCo* suffers a bit for very low choices the energy penalty during training. *REVISE* suffers a lot in general (around 10 percentage points).\n\n\n\n\n\n\n::: {#tbl-moons-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  full & 0.01 & \\textit{ECCo} & $-2.8 \\cdot 10^{22}$ & $6.39 \\cdot 10^{22}$ \\\\\n  full & 0.01 & \\textit{Generic} & $-4.89 \\cdot 10^{30}$ & $1.11 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.74}} & \\color{blue}{\\textbf{5.08}} \\\\\n  full & 0.01 & \\textit{REVISE} & -572 & $1.25 \\cdot 10^{3}$ \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -11.2 & 13 \\\\\n  full & 0.05 & \\textit{ECCo} & $-1.55 \\cdot 10^{16}$ & $3.52 \\cdot 10^{16}$ \\\\\n  full & 0.05 & \\textit{Generic} & $-2.22 \\cdot 10^{20}$ & $5 \\cdot 10^{20}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.41}} & \\color{blue}{\\textbf{4.48}} \\\\\n  full & 0.05 & \\textit{REVISE} & $-1.04 \\cdot 10^{3}$ & $2.3 \\cdot 10^{3}$ \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 0.1 & \\textit{ECCo} & $-3.41 \\cdot 10^{3}$ & $7.73 \\cdot 10^{3}$ \\\\\n  full & 0.1 & \\textit{Generic} & $-5.22 \\cdot 10^{30}$ & $1.19 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.78}} & \\color{blue}{\\textbf{5.12}} \\\\\n  full & 0.1 & \\textit{REVISE} & -288 & 594 \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 0.5 & \\textit{ECCo} & -7.09 & 7.51 \\\\\n  full & 0.5 & \\textit{Generic} & $-1.11 \\cdot 10^{31}$ & $2.53 \\cdot 10^{31}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.58}} & \\color{blue}{\\textbf{4.83}} \\\\\n  full & 0.5 & \\textit{REVISE} & $-1.19 \\cdot 10^{3}$ & $2.64 \\cdot 10^{3}$ \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  full & 1 & \\textit{ECCo} & -6.06 & 6.33 \\\\\n  full & 1 & \\textit{Generic} & $-1.58 \\cdot 10^{33}$ & $3.59 \\cdot 10^{33}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{Omniscient}} & \\color{blue}{\\textbf{-4.66}} & \\color{blue}{\\textbf{4.89}} \\\\\n  full & 1 & \\textit{REVISE} & $-1.16 \\cdot 10^{3}$ & $2.59 \\cdot 10^{3}$ \\\\\n  vanilla & 1 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 1 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 1 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-2.57}} & \\color{blue}{\\textbf{2.07}} \\\\\n  full & 5 & \\textit{Generic} & $-1.17 \\cdot 10^{28}$ & $2.66 \\cdot 10^{28}$ \\\\\n  full & 5 & \\textit{Omniscient} & -4.29 & 4.31 \\\\\n  full & 5 & \\textit{REVISE} & -530 & $1.16 \\cdot 10^{3}$ \\\\\n  vanilla & 5 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 5 & \\textit{Generic} & -11.7 & 12.7 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 5 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.76}} & \\color{blue}{\\textbf{0.974}} \\\\\n  full & 10 & \\textit{Generic} & $-1.54 \\cdot 10^{33}$ & $3.51 \\cdot 10^{33}$ \\\\\n  full & 10 & \\textit{Omniscient} & -4.44 & 4.56 \\\\\n  full & 10 & \\textit{REVISE} & $-1.52 \\cdot 10^{3}$ & $3.4 \\cdot 10^{3}$ \\\\\n  vanilla & 10 & \\textit{ECCo} & -15.5 & 17.3 \\\\\n  vanilla & 10 & \\textit{Generic} & -10.9 & 11.9 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -12.7 & 14.4 \\\\\n  vanilla & 10 & \\textit{REVISE} & -11.3 & 13.1 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.37}} & \\color{blue}{\\textbf{0.365}} \\\\\n  full & 15 & \\textit{Generic} & $-5.32 \\cdot 10^{28}$ & $1.21 \\cdot 10^{29}$ \\\\\n  full & 15 & \\textit{Omniscient} & -4.34 & 4.38 \\\\\n  full & 15 & \\textit{REVISE} & -473 & $1.03 \\cdot 10^{3}$ \\\\\n  vanilla & 15 & \\textit{ECCo} & -15.5 & 17.2 \\\\\n  vanilla & 15 & \\textit{Generic} & -11.7 & 12.8 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -12.4 & 14.1 \\\\\n  vanilla & 15 & \\textit{REVISE} & -11.3 & 13.1 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Moons data by energy penalty.\n\n:::\n\n#### Circles\n\n- **Energy Penalty** (@tbl-circles-lambda_energy_exper): *ECCo* consistently yields better results than *Vanilla*, though primarily for low to medium choices of the energy penalty (<=5) during training. The same goes for *Generic*, which sometimes outperforms *ECCo* (for small energy penalty at evaluation time). *Omni* does alright for lower energy penalty at evaluation time, but loses out for higher choices. *REVISE* performs poorly across the board (except very low choices at evaluation time).\n- **Cost (distance penalty)**: *ECCo* and *Generic* generally achieve the best results when no cost penalty is used during training. Both *Omni* and *REVISE* are largely unaffected.\n- **Maximum Iterations**: *ECCo* consistently yields better results for higher numbers of iterations. *Generic* generally does best for a medium number (50). *Omni* is sometimes invalid (**???**).\n- **Validity**: *ECCo* tends to outperform its *Vanilla* counterpart, though primarily for low to medium choices of the energy penalty (<=5) during training and evaluation. *Vanilla* typically worse across the board.\n- **Accuracy**: Mostly unaffected, but *REVISE* again consistently some deterioration and *ECCo* deteriorates for high choices of energy penalty during training, reflecting other outcomes above.\n\n\n\n\n\n\n::: {#tbl-circles-lambda_energy_exper}\n\n::: {.content-hidden unless-format=\"pdf\"}\n\n\n\\begin{longtable}{ccccc}\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endfirsthead\n  \\toprule\n  \\textbf{Objective} & \\textbf{$\\lambda_{\\text{div}} (\\text{train})$} & \\textbf{Generator} & \\textbf{Value} & \\textbf{Std} \\\\\\midrule\n  \\endhead\n  \\bottomrule\n  \\multicolumn{5}{r}{Continuing table below.}\\\\\n  \\bottomrule\n  \\endfoot\n  \\endlastfoot\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.01}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.26}} & \\color{blue}{\\textbf{0.423}} \\\\\n  full & 0.01 & \\textit{Generic} & -1.49 & 0.71 \\\\\n  full & 0.01 & \\textit{Omniscient} & -5.21 & 5.25 \\\\\n  full & 0.01 & \\textit{REVISE} & $-2.71 \\cdot 10^{26}$ & $6.37 \\cdot 10^{26}$ \\\\\n  vanilla & 0.01 & \\textit{ECCo} & -9.33 & 7.34 \\\\\n  vanilla & 0.01 & \\textit{Generic} & -8.89 & 6.88 \\\\\n  vanilla & 0.01 & \\textit{Omniscient} & -8.67 & 6.87 \\\\\n  vanilla & 0.01 & \\textit{REVISE} & -8.65 & 6.8 \\\\\n  full & 0.05 & \\textit{ECCo} & -1.29 & 0.397 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.05}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.21}} & \\color{blue}{\\textbf{0.356}} \\\\\n  full & 0.05 & \\textit{Omniscient} & -5.08 & 5.09 \\\\\n  full & 0.05 & \\textit{REVISE} & $-5.91 \\cdot 10^{27}$ & $1.36 \\cdot 10^{28}$ \\\\\n  vanilla & 0.05 & \\textit{ECCo} & -9.35 & 7.32 \\\\\n  vanilla & 0.05 & \\textit{Generic} & -8.85 & 6.87 \\\\\n  vanilla & 0.05 & \\textit{Omniscient} & -8.7 & 6.96 \\\\\n  vanilla & 0.05 & \\textit{REVISE} & -8.52 & 6.76 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.1}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.2}} & \\color{blue}{\\textbf{0.383}} \\\\\n  full & 0.1 & \\textit{Generic} & -1.5 & 0.735 \\\\\n  full & 0.1 & \\textit{Omniscient} & -5.17 & 5.23 \\\\\n  full & 0.1 & \\textit{REVISE} & $-3.06 \\cdot 10^{26}$ & $7.7 \\cdot 10^{26}$ \\\\\n  vanilla & 0.1 & \\textit{ECCo} & -9.33 & 7.32 \\\\\n  vanilla & 0.1 & \\textit{Generic} & -8.88 & 6.86 \\\\\n  vanilla & 0.1 & \\textit{Omniscient} & -8.69 & 6.9 \\\\\n  vanilla & 0.1 & \\textit{REVISE} & -8.68 & 6.81 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{0.5}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.12}} & \\color{blue}{\\textbf{0.217}} \\\\\n  full & 0.5 & \\textit{Generic} & -1.21 & 0.352 \\\\\n  full & 0.5 & \\textit{Omniscient} & -5.09 & 5.12 \\\\\n  full & 0.5 & \\textit{REVISE} & $-5.97 \\cdot 10^{27}$ & $1.37 \\cdot 10^{28}$ \\\\\n  vanilla & 0.5 & \\textit{ECCo} & -9.35 & 7.3 \\\\\n  vanilla & 0.5 & \\textit{Generic} & -8.89 & 6.92 \\\\\n  vanilla & 0.5 & \\textit{Omniscient} & -8.68 & 6.93 \\\\\n  vanilla & 0.5 & \\textit{REVISE} & -8.53 & 6.75 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{1}} & \\color{blue}{\\textbf{ECCo}} & \\color{blue}{\\textbf{-1.1}} & \\color{blue}{\\textbf{0.163}} \\\\\n  full & 1 & \\textit{Generic} & -1.49 & 0.726 \\\\\n  full & 1 & \\textit{Omniscient} & -5.16 & 5.2 \\\\\n  full & 1 & \\textit{REVISE} & $-3.09 \\cdot 10^{26}$ & $7.22 \\cdot 10^{26}$ \\\\\n  vanilla & 1 & \\textit{ECCo} & -9.34 & 7.36 \\\\\n  vanilla & 1 & \\textit{Generic} & -8.86 & 6.85 \\\\\n  vanilla & 1 & \\textit{Omniscient} & -8.7 & 6.9 \\\\\n  vanilla & 1 & \\textit{REVISE} & -8.69 & 6.85 \\\\\n  full & 5 & \\textit{ECCo} & -1.75 & 0.154 \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{5}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.21}} & \\color{blue}{\\textbf{0.363}} \\\\\n  full & 5 & \\textit{Omniscient} & -5.14 & 5.16 \\\\\n  full & 5 & \\textit{REVISE} & $-1.1 \\cdot 10^{28}$ & $2.5 \\cdot 10^{28}$ \\\\\n  vanilla & 5 & \\textit{ECCo} & -9.36 & 7.32 \\\\\n  vanilla & 5 & \\textit{Generic} & -8.88 & 6.91 \\\\\n  vanilla & 5 & \\textit{Omniscient} & -8.7 & 6.93 \\\\\n  vanilla & 5 & \\textit{REVISE} & -8.52 & 6.73 \\\\\n  full & 10 & \\textit{ECCo} & $-1.02 \\cdot 10^{6}$ & $2.32 \\cdot 10^{6}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{10}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.49}} & \\color{blue}{\\textbf{0.702}} \\\\\n  full & 10 & \\textit{Omniscient} & -5.13 & 5.16 \\\\\n  full & 10 & \\textit{REVISE} & $-3.74 \\cdot 10^{26}$ & $9.09 \\cdot 10^{26}$ \\\\\n  vanilla & 10 & \\textit{ECCo} & -9.31 & 7.33 \\\\\n  vanilla & 10 & \\textit{Generic} & -8.87 & 6.86 \\\\\n  vanilla & 10 & \\textit{Omniscient} & -8.7 & 6.89 \\\\\n  vanilla & 10 & \\textit{REVISE} & -8.69 & 6.83 \\\\\n  full & 15 & \\textit{ECCo} & $-3.31 \\cdot 10^{13}$ & $7.54 \\cdot 10^{13}$ \\\\\n  \\color{blue}{\\textbf{full}} & \\color{blue}{\\textbf{15}} & \\color{blue}{\\textbf{Generic}} & \\color{blue}{\\textbf{-1.22}} & \\color{blue}{\\textbf{0.37}} \\\\\n  full & 15 & \\textit{Omniscient} & -5.2 & 5.23 \\\\\n  full & 15 & \\textit{REVISE} & $-9.01 \\cdot 10^{27}$ & $2.06 \\cdot 10^{28}$ \\\\\n  vanilla & 15 & \\textit{ECCo} & -9.38 & 7.34 \\\\\n  vanilla & 15 & \\textit{Generic} & -8.86 & 6.87 \\\\\n  vanilla & 15 & \\textit{Omniscient} & -8.69 & 6.96 \\\\\n  vanilla & 15 & \\textit{REVISE} & -8.51 & 6.73 \\\\\\bottomrule\n\\end{longtable}\n\n\n\n\n:::\n\nResults for Circles data by energy penalty.\n\n:::\n\n\n\n# References\n\n",
    "supporting": [
      "paper_files"
    ],
    "filters": []
  }
}